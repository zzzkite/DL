{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLAStdsMib7m"
      },
      "source": [
        "# Assignment 1\n",
        "\n",
        "This code baseline is inspired by and modified from [this great tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html).\n",
        "\n",
        "This code can achieve an accuracy of approximately 86.50% on CIFAR-10. Please set up the environment and run your experiments starting from this baseline. You are expected to achieve an accuracy higher than this baseline."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task1**——5 points\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZPPixQdVvum0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhZxMrK1jO_T",
        "outputId": "9325bab1-0aa5-41be-95c2-51f1bd3461d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"Device Name: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRnMMP8ojUY5",
        "outputId": "d610a1c8-3992-4e02-a662-346f8ae9acb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version: 2.8.0+cu126\n",
            "CUDA Available: True\n",
            "Device Name: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5pGHzg4ib7m"
      },
      "outputs": [],
      "source": [
        "# import some necessary packages\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision.datasets as tv_datasets\n",
        "import torchvision.transforms as tv_transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rJhJtZ3ib7n"
      },
      "outputs": [],
      "source": [
        "# some experimental setup\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "num_epochs = 128\n",
        "batch_size = 64\n",
        "num_workers = 2\n",
        "print_every = 200\n",
        "\n",
        "optim_name = \"Adam\"\n",
        "optim_kwargs = dict(\n",
        "    lr=3e-4,\n",
        "    weight_decay=1e-6,\n",
        ")\n",
        "\n",
        "# preprocessing pipeline for input images\n",
        "transformation = dict()\n",
        "for data_type in (\"train\", \"test\"):\n",
        "    is_train = data_type==\"train\"\n",
        "    transformation[data_type] = tv_transforms.Compose(([\n",
        "        tv_transforms.RandomRotation(degrees=15),\n",
        "        tv_transforms.RandomHorizontalFlip(),\n",
        "        tv_transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "    ] if is_train else []) +\n",
        "    [\n",
        "        tv_transforms.ToTensor(),\n",
        "        tv_transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tojyOA0ib7n",
        "outputId": "6647b7d4-86f9-4a78-844e-2745561006c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:14<00:00, 12.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "# prepare datasets\n",
        "dataset, loader = {}, {}\n",
        "for data_type in (\"train\", \"test\"):\n",
        "    is_train = data_type==\"train\"\n",
        "    dataset[data_type] = tv_datasets.CIFAR10(\n",
        "        root=\"./data\", train=is_train, download=True, transform=transformation[data_type],\n",
        "    )\n",
        "    loader[data_type] = torch.utils.data.DataLoader(\n",
        "        dataset[data_type], batch_size=batch_size, shuffle=is_train, num_workers=num_workers,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lU4GhD6Oib7n",
        "outputId": "8ff51dc5-6fb3-4cf9-87fb-bc2e21d2073f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 7.28M\n"
          ]
        }
      ],
      "source": [
        "# our network architecture\n",
        "net = nn.Sequential(\n",
        "    nn.Conv2d(3, 128, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),\n",
        "    nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),\n",
        "    nn.Conv2d(256, 512, 3, padding=1), nn.ReLU(inplace=True),\n",
        "    nn.Conv2d(512, 512, 3, padding=1), nn.ReLU(inplace=True),\n",
        "    nn.Conv2d(512, 256, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(256 * 4 * 4, 512), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
        "    nn.Linear(512, 256), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
        "    nn.Linear(256, 128), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
        "    nn.Linear(128, 10),\n",
        ")\n",
        "\n",
        "# move to device\n",
        "net.to(device)\n",
        "\n",
        "# print the number of parameters\n",
        "print(f\"number of parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad) / 1_000_000:.2f}M\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Xuc_nHDib7n"
      },
      "source": [
        "## Start Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbKzara7ib7n",
        "outputId": "b3a3db5f-02dd-4879-f351-d8c844f456be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch=  1, iter=  200] loss: 2.197\n",
            "[epoch=  1, iter=  400] loss: 1.977\n",
            "[epoch=  1, iter=  600] loss: 1.856\n",
            "[epoch=  2, iter=  200] loss: 1.698\n",
            "[epoch=  2, iter=  400] loss: 1.598\n",
            "[epoch=  2, iter=  600] loss: 1.543\n",
            "[epoch=  3, iter=  200] loss: 1.457\n",
            "[epoch=  3, iter=  400] loss: 1.402\n",
            "[epoch=  3, iter=  600] loss: 1.379\n",
            "[epoch=  4, iter=  200] loss: 1.302\n",
            "[epoch=  4, iter=  400] loss: 1.286\n",
            "[epoch=  4, iter=  600] loss: 1.220\n",
            "[epoch=  5, iter=  200] loss: 1.179\n",
            "[epoch=  5, iter=  400] loss: 1.175\n",
            "[epoch=  5, iter=  600] loss: 1.160\n",
            "[epoch=  6, iter=  200] loss: 1.088\n",
            "[epoch=  6, iter=  400] loss: 1.088\n",
            "[epoch=  6, iter=  600] loss: 1.071\n",
            "[epoch=  7, iter=  200] loss: 1.047\n",
            "[epoch=  7, iter=  400] loss: 1.017\n",
            "[epoch=  7, iter=  600] loss: 0.990\n",
            "[epoch=  8, iter=  200] loss: 0.960\n",
            "[epoch=  8, iter=  400] loss: 0.965\n",
            "[epoch=  8, iter=  600] loss: 0.948\n",
            "[epoch=  9, iter=  200] loss: 0.911\n",
            "[epoch=  9, iter=  400] loss: 0.903\n",
            "[epoch=  9, iter=  600] loss: 0.902\n",
            "[epoch= 10, iter=  200] loss: 0.875\n",
            "[epoch= 10, iter=  400] loss: 0.875\n",
            "[epoch= 10, iter=  600] loss: 0.858\n",
            "[epoch= 11, iter=  200] loss: 0.838\n",
            "[epoch= 11, iter=  400] loss: 0.835\n",
            "[epoch= 11, iter=  600] loss: 0.811\n",
            "[epoch= 12, iter=  200] loss: 0.815\n",
            "[epoch= 12, iter=  400] loss: 0.780\n",
            "[epoch= 12, iter=  600] loss: 0.799\n",
            "[epoch= 13, iter=  200] loss: 0.765\n",
            "[epoch= 13, iter=  400] loss: 0.780\n",
            "[epoch= 13, iter=  600] loss: 0.771\n",
            "[epoch= 14, iter=  200] loss: 0.749\n",
            "[epoch= 14, iter=  400] loss: 0.733\n",
            "[epoch= 14, iter=  600] loss: 0.731\n",
            "[epoch= 15, iter=  200] loss: 0.717\n",
            "[epoch= 15, iter=  400] loss: 0.708\n",
            "[epoch= 15, iter=  600] loss: 0.720\n",
            "[epoch= 16, iter=  200] loss: 0.693\n",
            "[epoch= 16, iter=  400] loss: 0.691\n",
            "[epoch= 16, iter=  600] loss: 0.702\n",
            "[epoch= 17, iter=  200] loss: 0.668\n",
            "[epoch= 17, iter=  400] loss: 0.673\n",
            "[epoch= 17, iter=  600] loss: 0.670\n",
            "[epoch= 18, iter=  200] loss: 0.652\n",
            "[epoch= 18, iter=  400] loss: 0.654\n",
            "[epoch= 18, iter=  600] loss: 0.646\n",
            "[epoch= 19, iter=  200] loss: 0.623\n",
            "[epoch= 19, iter=  400] loss: 0.635\n",
            "[epoch= 19, iter=  600] loss: 0.642\n",
            "[epoch= 20, iter=  200] loss: 0.599\n",
            "[epoch= 20, iter=  400] loss: 0.621\n",
            "[epoch= 20, iter=  600] loss: 0.623\n",
            "[epoch= 21, iter=  200] loss: 0.599\n",
            "[epoch= 21, iter=  400] loss: 0.620\n",
            "[epoch= 21, iter=  600] loss: 0.605\n",
            "[epoch= 22, iter=  200] loss: 0.590\n",
            "[epoch= 22, iter=  400] loss: 0.585\n",
            "[epoch= 22, iter=  600] loss: 0.581\n",
            "[epoch= 23, iter=  200] loss: 0.567\n",
            "[epoch= 23, iter=  400] loss: 0.575\n",
            "[epoch= 23, iter=  600] loss: 0.584\n",
            "[epoch= 24, iter=  200] loss: 0.550\n",
            "[epoch= 24, iter=  400] loss: 0.564\n",
            "[epoch= 24, iter=  600] loss: 0.570\n",
            "[epoch= 25, iter=  200] loss: 0.552\n",
            "[epoch= 25, iter=  400] loss: 0.543\n",
            "[epoch= 25, iter=  600] loss: 0.563\n",
            "[epoch= 26, iter=  200] loss: 0.530\n",
            "[epoch= 26, iter=  400] loss: 0.538\n",
            "[epoch= 26, iter=  600] loss: 0.557\n",
            "[epoch= 27, iter=  200] loss: 0.538\n",
            "[epoch= 27, iter=  400] loss: 0.524\n",
            "[epoch= 27, iter=  600] loss: 0.523\n",
            "[epoch= 28, iter=  200] loss: 0.493\n",
            "[epoch= 28, iter=  400] loss: 0.518\n",
            "[epoch= 28, iter=  600] loss: 0.537\n",
            "[epoch= 29, iter=  200] loss: 0.501\n",
            "[epoch= 29, iter=  400] loss: 0.518\n",
            "[epoch= 29, iter=  600] loss: 0.519\n",
            "[epoch= 30, iter=  200] loss: 0.491\n",
            "[epoch= 30, iter=  400] loss: 0.492\n",
            "[epoch= 30, iter=  600] loss: 0.499\n",
            "[epoch= 31, iter=  200] loss: 0.489\n",
            "[epoch= 31, iter=  400] loss: 0.487\n",
            "[epoch= 31, iter=  600] loss: 0.489\n",
            "[epoch= 32, iter=  200] loss: 0.479\n",
            "[epoch= 32, iter=  400] loss: 0.488\n",
            "[epoch= 32, iter=  600] loss: 0.482\n",
            "[epoch= 33, iter=  200] loss: 0.466\n",
            "[epoch= 33, iter=  400] loss: 0.472\n",
            "[epoch= 33, iter=  600] loss: 0.470\n",
            "[epoch= 34, iter=  200] loss: 0.457\n",
            "[epoch= 34, iter=  400] loss: 0.472\n",
            "[epoch= 34, iter=  600] loss: 0.461\n",
            "[epoch= 35, iter=  200] loss: 0.444\n",
            "[epoch= 35, iter=  400] loss: 0.467\n",
            "[epoch= 35, iter=  600] loss: 0.468\n",
            "[epoch= 36, iter=  200] loss: 0.454\n",
            "[epoch= 36, iter=  400] loss: 0.447\n",
            "[epoch= 36, iter=  600] loss: 0.475\n",
            "[epoch= 37, iter=  200] loss: 0.438\n",
            "[epoch= 37, iter=  400] loss: 0.432\n",
            "[epoch= 37, iter=  600] loss: 0.451\n",
            "[epoch= 38, iter=  200] loss: 0.433\n",
            "[epoch= 38, iter=  400] loss: 0.441\n",
            "[epoch= 38, iter=  600] loss: 0.421\n",
            "[epoch= 39, iter=  200] loss: 0.425\n",
            "[epoch= 39, iter=  400] loss: 0.427\n",
            "[epoch= 39, iter=  600] loss: 0.435\n",
            "[epoch= 40, iter=  200] loss: 0.431\n",
            "[epoch= 40, iter=  400] loss: 0.428\n",
            "[epoch= 40, iter=  600] loss: 0.423\n",
            "[epoch= 41, iter=  200] loss: 0.403\n",
            "[epoch= 41, iter=  400] loss: 0.417\n",
            "[epoch= 41, iter=  600] loss: 0.428\n",
            "[epoch= 42, iter=  200] loss: 0.408\n",
            "[epoch= 42, iter=  400] loss: 0.403\n",
            "[epoch= 42, iter=  600] loss: 0.420\n",
            "[epoch= 43, iter=  200] loss: 0.397\n",
            "[epoch= 43, iter=  400] loss: 0.400\n",
            "[epoch= 43, iter=  600] loss: 0.423\n",
            "[epoch= 44, iter=  200] loss: 0.415\n",
            "[epoch= 44, iter=  400] loss: 0.408\n",
            "[epoch= 44, iter=  600] loss: 0.402\n",
            "[epoch= 45, iter=  200] loss: 0.380\n",
            "[epoch= 45, iter=  400] loss: 0.404\n",
            "[epoch= 45, iter=  600] loss: 0.401\n",
            "[epoch= 46, iter=  200] loss: 0.374\n",
            "[epoch= 46, iter=  400] loss: 0.385\n",
            "[epoch= 46, iter=  600] loss: 0.399\n",
            "[epoch= 47, iter=  200] loss: 0.379\n",
            "[epoch= 47, iter=  400] loss: 0.393\n",
            "[epoch= 47, iter=  600] loss: 0.396\n",
            "[epoch= 48, iter=  200] loss: 0.383\n",
            "[epoch= 48, iter=  400] loss: 0.372\n",
            "[epoch= 48, iter=  600] loss: 0.393\n",
            "[epoch= 49, iter=  200] loss: 0.392\n",
            "[epoch= 49, iter=  400] loss: 0.376\n",
            "[epoch= 49, iter=  600] loss: 0.365\n",
            "[epoch= 50, iter=  200] loss: 0.360\n",
            "[epoch= 50, iter=  400] loss: 0.369\n",
            "[epoch= 50, iter=  600] loss: 0.374\n",
            "[epoch= 51, iter=  200] loss: 0.365\n",
            "[epoch= 51, iter=  400] loss: 0.351\n",
            "[epoch= 51, iter=  600] loss: 0.363\n",
            "[epoch= 52, iter=  200] loss: 0.366\n",
            "[epoch= 52, iter=  400] loss: 0.365\n",
            "[epoch= 52, iter=  600] loss: 0.377\n",
            "[epoch= 53, iter=  200] loss: 0.354\n",
            "[epoch= 53, iter=  400] loss: 0.365\n",
            "[epoch= 53, iter=  600] loss: 0.372\n",
            "[epoch= 54, iter=  200] loss: 0.337\n",
            "[epoch= 54, iter=  400] loss: 0.355\n",
            "[epoch= 54, iter=  600] loss: 0.367\n",
            "[epoch= 55, iter=  200] loss: 0.353\n",
            "[epoch= 55, iter=  400] loss: 0.362\n",
            "[epoch= 55, iter=  600] loss: 0.374\n",
            "[epoch= 56, iter=  200] loss: 0.363\n",
            "[epoch= 56, iter=  400] loss: 0.356\n",
            "[epoch= 56, iter=  600] loss: 0.339\n",
            "[epoch= 57, iter=  200] loss: 0.348\n",
            "[epoch= 57, iter=  400] loss: 0.345\n",
            "[epoch= 57, iter=  600] loss: 0.358\n",
            "[epoch= 58, iter=  200] loss: 0.325\n",
            "[epoch= 58, iter=  400] loss: 0.357\n",
            "[epoch= 58, iter=  600] loss: 0.344\n",
            "[epoch= 59, iter=  200] loss: 0.338\n",
            "[epoch= 59, iter=  400] loss: 0.354\n",
            "[epoch= 59, iter=  600] loss: 0.332\n",
            "[epoch= 60, iter=  200] loss: 0.325\n",
            "[epoch= 60, iter=  400] loss: 0.335\n",
            "[epoch= 60, iter=  600] loss: 0.327\n",
            "[epoch= 61, iter=  200] loss: 0.328\n",
            "[epoch= 61, iter=  400] loss: 0.328\n",
            "[epoch= 61, iter=  600] loss: 0.342\n",
            "[epoch= 62, iter=  200] loss: 0.332\n",
            "[epoch= 62, iter=  400] loss: 0.334\n",
            "[epoch= 62, iter=  600] loss: 0.353\n",
            "[epoch= 63, iter=  200] loss: 0.308\n",
            "[epoch= 63, iter=  400] loss: 0.338\n",
            "[epoch= 63, iter=  600] loss: 0.334\n",
            "[epoch= 64, iter=  200] loss: 0.327\n",
            "[epoch= 64, iter=  400] loss: 0.338\n",
            "[epoch= 64, iter=  600] loss: 0.332\n",
            "[epoch= 65, iter=  200] loss: 0.315\n",
            "[epoch= 65, iter=  400] loss: 0.322\n",
            "[epoch= 65, iter=  600] loss: 0.327\n",
            "[epoch= 66, iter=  200] loss: 0.308\n",
            "[epoch= 66, iter=  400] loss: 0.316\n",
            "[epoch= 66, iter=  600] loss: 0.327\n",
            "[epoch= 67, iter=  200] loss: 0.292\n",
            "[epoch= 67, iter=  400] loss: 0.313\n",
            "[epoch= 67, iter=  600] loss: 0.311\n",
            "[epoch= 68, iter=  200] loss: 0.294\n",
            "[epoch= 68, iter=  400] loss: 0.317\n",
            "[epoch= 68, iter=  600] loss: 0.320\n",
            "[epoch= 69, iter=  200] loss: 0.307\n",
            "[epoch= 69, iter=  400] loss: 0.293\n",
            "[epoch= 69, iter=  600] loss: 0.323\n",
            "[epoch= 70, iter=  200] loss: 0.310\n",
            "[epoch= 70, iter=  400] loss: 0.304\n",
            "[epoch= 70, iter=  600] loss: 0.319\n",
            "[epoch= 71, iter=  200] loss: 0.276\n",
            "[epoch= 71, iter=  400] loss: 0.297\n",
            "[epoch= 71, iter=  600] loss: 0.305\n",
            "[epoch= 72, iter=  200] loss: 0.304\n",
            "[epoch= 72, iter=  400] loss: 0.307\n",
            "[epoch= 72, iter=  600] loss: 0.299\n",
            "[epoch= 73, iter=  200] loss: 0.295\n",
            "[epoch= 73, iter=  400] loss: 0.301\n",
            "[epoch= 73, iter=  600] loss: 0.294\n",
            "[epoch= 74, iter=  200] loss: 0.281\n",
            "[epoch= 74, iter=  400] loss: 0.297\n",
            "[epoch= 74, iter=  600] loss: 0.291\n",
            "[epoch= 75, iter=  200] loss: 0.296\n",
            "[epoch= 75, iter=  400] loss: 0.294\n",
            "[epoch= 75, iter=  600] loss: 0.297\n",
            "[epoch= 76, iter=  200] loss: 0.298\n",
            "[epoch= 76, iter=  400] loss: 0.280\n",
            "[epoch= 76, iter=  600] loss: 0.297\n",
            "[epoch= 77, iter=  200] loss: 0.286\n",
            "[epoch= 77, iter=  400] loss: 0.297\n",
            "[epoch= 77, iter=  600] loss: 0.281\n",
            "[epoch= 78, iter=  200] loss: 0.270\n",
            "[epoch= 78, iter=  400] loss: 0.302\n",
            "[epoch= 78, iter=  600] loss: 0.281\n",
            "[epoch= 79, iter=  200] loss: 0.280\n",
            "[epoch= 79, iter=  400] loss: 0.276\n",
            "[epoch= 79, iter=  600] loss: 0.276\n",
            "[epoch= 80, iter=  200] loss: 0.276\n",
            "[epoch= 80, iter=  400] loss: 0.282\n",
            "[epoch= 80, iter=  600] loss: 0.278\n",
            "[epoch= 81, iter=  200] loss: 0.274\n",
            "[epoch= 81, iter=  400] loss: 0.260\n",
            "[epoch= 81, iter=  600] loss: 0.287\n",
            "[epoch= 82, iter=  200] loss: 0.278\n",
            "[epoch= 82, iter=  400] loss: 0.289\n",
            "[epoch= 82, iter=  600] loss: 0.273\n",
            "[epoch= 83, iter=  200] loss: 0.278\n",
            "[epoch= 83, iter=  400] loss: 0.269\n",
            "[epoch= 83, iter=  600] loss: 0.257\n",
            "[epoch= 84, iter=  200] loss: 0.264\n",
            "[epoch= 84, iter=  400] loss: 0.273\n",
            "[epoch= 84, iter=  600] loss: 0.277\n",
            "[epoch= 85, iter=  200] loss: 0.269\n",
            "[epoch= 85, iter=  400] loss: 0.270\n",
            "[epoch= 85, iter=  600] loss: 0.275\n",
            "[epoch= 86, iter=  200] loss: 0.266\n",
            "[epoch= 86, iter=  400] loss: 0.275\n",
            "[epoch= 86, iter=  600] loss: 0.261\n",
            "[epoch= 87, iter=  200] loss: 0.260\n",
            "[epoch= 87, iter=  400] loss: 0.273\n",
            "[epoch= 87, iter=  600] loss: 0.270\n",
            "[epoch= 88, iter=  200] loss: 0.267\n",
            "[epoch= 88, iter=  400] loss: 0.270\n",
            "[epoch= 88, iter=  600] loss: 0.258\n",
            "[epoch= 89, iter=  200] loss: 0.249\n",
            "[epoch= 89, iter=  400] loss: 0.241\n",
            "[epoch= 89, iter=  600] loss: 0.283\n",
            "[epoch= 90, iter=  200] loss: 0.252\n",
            "[epoch= 90, iter=  400] loss: 0.255\n",
            "[epoch= 90, iter=  600] loss: 0.268\n",
            "[epoch= 91, iter=  200] loss: 0.256\n",
            "[epoch= 91, iter=  400] loss: 0.252\n",
            "[epoch= 91, iter=  600] loss: 0.267\n",
            "[epoch= 92, iter=  200] loss: 0.256\n",
            "[epoch= 92, iter=  400] loss: 0.261\n",
            "[epoch= 92, iter=  600] loss: 0.262\n",
            "[epoch= 93, iter=  200] loss: 0.247\n",
            "[epoch= 93, iter=  400] loss: 0.246\n",
            "[epoch= 93, iter=  600] loss: 0.267\n",
            "[epoch= 94, iter=  200] loss: 0.244\n",
            "[epoch= 94, iter=  400] loss: 0.252\n",
            "[epoch= 94, iter=  600] loss: 0.257\n",
            "[epoch= 95, iter=  200] loss: 0.243\n",
            "[epoch= 95, iter=  400] loss: 0.253\n",
            "[epoch= 95, iter=  600] loss: 0.254\n",
            "[epoch= 96, iter=  200] loss: 0.251\n",
            "[epoch= 96, iter=  400] loss: 0.251\n",
            "[epoch= 96, iter=  600] loss: 0.249\n",
            "[epoch= 97, iter=  200] loss: 0.243\n",
            "[epoch= 97, iter=  400] loss: 0.265\n",
            "[epoch= 97, iter=  600] loss: 0.245\n",
            "[epoch= 98, iter=  200] loss: 0.241\n",
            "[epoch= 98, iter=  400] loss: 0.246\n",
            "[epoch= 98, iter=  600] loss: 0.248\n",
            "[epoch= 99, iter=  200] loss: 0.230\n",
            "[epoch= 99, iter=  400] loss: 0.236\n",
            "[epoch= 99, iter=  600] loss: 0.242\n",
            "[epoch=100, iter=  200] loss: 0.255\n",
            "[epoch=100, iter=  400] loss: 0.234\n",
            "[epoch=100, iter=  600] loss: 0.246\n",
            "[epoch=101, iter=  200] loss: 0.243\n",
            "[epoch=101, iter=  400] loss: 0.240\n",
            "[epoch=101, iter=  600] loss: 0.228\n",
            "[epoch=102, iter=  200] loss: 0.245\n",
            "[epoch=102, iter=  400] loss: 0.236\n",
            "[epoch=102, iter=  600] loss: 0.240\n",
            "[epoch=103, iter=  200] loss: 0.241\n",
            "[epoch=103, iter=  400] loss: 0.243\n",
            "[epoch=103, iter=  600] loss: 0.227\n",
            "[epoch=104, iter=  200] loss: 0.242\n",
            "[epoch=104, iter=  400] loss: 0.232\n",
            "[epoch=104, iter=  600] loss: 0.239\n",
            "[epoch=105, iter=  200] loss: 0.242\n",
            "[epoch=105, iter=  400] loss: 0.238\n",
            "[epoch=105, iter=  600] loss: 0.228\n",
            "[epoch=106, iter=  200] loss: 0.217\n",
            "[epoch=106, iter=  400] loss: 0.229\n",
            "[epoch=106, iter=  600] loss: 0.239\n",
            "[epoch=107, iter=  200] loss: 0.228\n",
            "[epoch=107, iter=  400] loss: 0.225\n",
            "[epoch=107, iter=  600] loss: 0.224\n",
            "[epoch=108, iter=  200] loss: 0.232\n",
            "[epoch=108, iter=  400] loss: 0.230\n",
            "[epoch=108, iter=  600] loss: 0.249\n",
            "[epoch=109, iter=  200] loss: 0.235\n",
            "[epoch=109, iter=  400] loss: 0.221\n",
            "[epoch=109, iter=  600] loss: 0.237\n",
            "[epoch=110, iter=  200] loss: 0.231\n",
            "[epoch=110, iter=  400] loss: 0.225\n",
            "[epoch=110, iter=  600] loss: 0.230\n",
            "[epoch=111, iter=  200] loss: 0.228\n",
            "[epoch=111, iter=  400] loss: 0.219\n",
            "[epoch=111, iter=  600] loss: 0.220\n",
            "[epoch=112, iter=  200] loss: 0.223\n",
            "[epoch=112, iter=  400] loss: 0.224\n",
            "[epoch=112, iter=  600] loss: 0.234\n",
            "[epoch=113, iter=  200] loss: 0.214\n",
            "[epoch=113, iter=  400] loss: 0.225\n",
            "[epoch=113, iter=  600] loss: 0.221\n",
            "[epoch=114, iter=  200] loss: 0.236\n",
            "[epoch=114, iter=  400] loss: 0.215\n",
            "[epoch=114, iter=  600] loss: 0.220\n",
            "[epoch=115, iter=  200] loss: 0.242\n",
            "[epoch=115, iter=  400] loss: 0.219\n",
            "[epoch=115, iter=  600] loss: 0.224\n",
            "[epoch=116, iter=  200] loss: 0.208\n",
            "[epoch=116, iter=  400] loss: 0.221\n",
            "[epoch=116, iter=  600] loss: 0.231\n",
            "[epoch=117, iter=  200] loss: 0.215\n",
            "[epoch=117, iter=  400] loss: 0.204\n",
            "[epoch=117, iter=  600] loss: 0.220\n",
            "[epoch=118, iter=  200] loss: 0.205\n",
            "[epoch=118, iter=  400] loss: 0.214\n",
            "[epoch=118, iter=  600] loss: 0.207\n",
            "[epoch=119, iter=  200] loss: 0.224\n",
            "[epoch=119, iter=  400] loss: 0.226\n",
            "[epoch=119, iter=  600] loss: 0.217\n",
            "[epoch=120, iter=  200] loss: 0.221\n",
            "[epoch=120, iter=  400] loss: 0.202\n",
            "[epoch=120, iter=  600] loss: 0.227\n",
            "[epoch=121, iter=  200] loss: 0.214\n",
            "[epoch=121, iter=  400] loss: 0.210\n",
            "[epoch=121, iter=  600] loss: 0.215\n",
            "[epoch=122, iter=  200] loss: 0.218\n",
            "[epoch=122, iter=  400] loss: 0.224\n",
            "[epoch=122, iter=  600] loss: 0.224\n",
            "[epoch=123, iter=  200] loss: 0.205\n",
            "[epoch=123, iter=  400] loss: 0.198\n",
            "[epoch=123, iter=  600] loss: 0.209\n",
            "[epoch=124, iter=  200] loss: 0.205\n",
            "[epoch=124, iter=  400] loss: 0.217\n",
            "[epoch=124, iter=  600] loss: 0.222\n",
            "[epoch=125, iter=  200] loss: 0.205\n",
            "[epoch=125, iter=  400] loss: 0.206\n",
            "[epoch=125, iter=  600] loss: 0.218\n",
            "[epoch=126, iter=  200] loss: 0.221\n",
            "[epoch=126, iter=  400] loss: 0.208\n",
            "[epoch=126, iter=  600] loss: 0.201\n",
            "[epoch=127, iter=  200] loss: 0.193\n",
            "[epoch=127, iter=  400] loss: 0.209\n",
            "[epoch=127, iter=  600] loss: 0.218\n",
            "[epoch=128, iter=  200] loss: 0.203\n",
            "[epoch=128, iter=  400] loss: 0.209\n",
            "[epoch=128, iter=  600] loss: 0.209\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "# # the network optimizer\n",
        "# optimizer = getattr(optim, optim_name)(net.parameters(), **optim_kwargs)\n",
        "\n",
        "# # loss function\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# # training loop\n",
        "# net.train()\n",
        "# for epoch in range(num_epochs):\n",
        "\n",
        "#     running_loss = 0.0\n",
        "#     for i, (img, target) in enumerate(loader[\"train\"]):\n",
        "#         img, target = img.to(device), target.to(device)\n",
        "\n",
        "#         pred = net(img)\n",
        "#         loss = criterion(pred, target)\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         # print statistics\n",
        "#         running_loss += loss.item()\n",
        "#         if i % print_every == print_every - 1:\n",
        "#             print(f\"[epoch={epoch + 1:3d}, iter={i + 1:5d}] loss: {running_loss / print_every:.3f}\")\n",
        "#             running_loss = 0.0\n",
        "\n",
        "# print(\"Finished Training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALuzJ8ITib7n"
      },
      "source": [
        "## Evaluating its accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVdCsKYAib7n",
        "outputId": "a33f1abe-37d6-4004-e3c9-57d30ed76061"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 86.72%\n"
          ]
        }
      ],
      "source": [
        "# net.eval()\n",
        "# correct, total = 0, 0\n",
        "# with torch.no_grad():\n",
        "#     for img, target in loader[\"test\"]:\n",
        "#         img, target = img.to(device), target.to(device)\n",
        "\n",
        "#         # make prediction\n",
        "#         pred = net(img)\n",
        "\n",
        "#         # accumulate\n",
        "#         total += len(target)\n",
        "#         correct += (torch.argmax(pred, dim=1) == target).sum().item()\n",
        "\n",
        "# print(f\"Accuracy of the network on the {total} test images: {100 * correct / total:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task2**——25 points"
      ],
      "metadata": {
        "id": "HD6f6kwpwAAr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Due to computational resource limitations, we reduced the number of epochs for performance comparison. We found that the performance was already quite good at 40 epochs, so we directly used a smaller number of epochs for comparison.\n"
      ],
      "metadata": {
        "id": "yYbjPO00yHdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # 实验 0: 基线 (40 Epochs)\n",
        "# # 改动: num_epochs 从 128 改为 40\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# import torchvision.datasets as tv_datasets\n",
        "# import torchvision.transforms as tv_transforms\n",
        "\n",
        "# # --- 实验配置 ---\n",
        "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# num_epochs = 40\n",
        "# batch_size = 64\n",
        "# num_workers = 2\n",
        "# print_every = 200\n",
        "# optim_name = \"Adam\"\n",
        "# optim_kwargs = dict(lr=3e-4, weight_decay=1e-6)\n",
        "\n",
        "# # --- 数据预处理 ---\n",
        "# transformation = dict()\n",
        "# for data_type in (\"train\", \"test\"):\n",
        "#     is_train = data_type==\"train\"\n",
        "#     transformation[data_type] = tv_transforms.Compose(([\n",
        "#         tv_transforms.RandomRotation(degrees=15),\n",
        "#         tv_transforms.RandomHorizontalFlip(),\n",
        "#         tv_transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "#     ] if is_train else []) +\n",
        "#     [\n",
        "#         tv_transforms.ToTensor(),\n",
        "#         tv_transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "#     ])\n",
        "\n",
        "# # --- 数据加载器 ---\n",
        "# dataset, loader = {}, {}\n",
        "# for data_type in (\"train\", \"test\"):\n",
        "#     is_train = data_type==\"train\"\n",
        "#     dataset[data_type] = tv_datasets.CIFAR10(root=\"./data\", train=is_train, download=True, transform=transformation[data_type])\n",
        "#     loader[data_type] = torch.utils.data.DataLoader(dataset[data_type], batch_size=batch_size, shuffle=is_train, num_workers=num_workers)\n",
        "\n",
        "# # --- 网络架构 ---\n",
        "# net = nn.Sequential(\n",
        "#     nn.Conv2d(3, 128, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),\n",
        "#     nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),\n",
        "#     nn.Conv2d(256, 512, 3, padding=1), nn.ReLU(inplace=True),\n",
        "#     nn.Conv2d(512, 512, 3, padding=1), nn.ReLU(inplace=True),\n",
        "#     nn.Conv2d(512, 256, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),\n",
        "#     nn.Flatten(),\n",
        "#     nn.Linear(256 * 4 * 4, 512), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
        "#     nn.Linear(512, 256), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
        "#     nn.Linear(256, 128), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
        "#     nn.Linear(128, 10),\n",
        "# ).to(device)\n",
        "\n",
        "# # --- 优化器与损失函数 ---\n",
        "# optimizer = getattr(optim, optim_name)(net.parameters(), **optim_kwargs)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# # --- 训练循环 ---\n",
        "# print(\"--- 开始训练: 实验 0 (基线) ---\")\n",
        "# net.train()\n",
        "# for epoch in range(num_epochs):\n",
        "#     running_loss = 0.0\n",
        "#     for i, (img, target) in enumerate(loader[\"train\"]):\n",
        "#         img, target = img.to(device), target.to(device)\n",
        "#         pred = net(img)\n",
        "#         loss = criterion(pred, target)\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         running_loss += loss.item()\n",
        "#         if i % print_every == print_every - 1:\n",
        "#             print(f\"[epoch={epoch + 1:3d}, iter={i + 1:5d}] loss: {running_loss / print_every:.3f}\")\n",
        "#             running_loss = 0.0\n",
        "# print(\"--- 训练结束 ---\")\n",
        "\n",
        "# # --- 评估 ---\n",
        "# net.eval()\n",
        "# correct, total = 0, 0\n",
        "# with torch.no_grad():\n",
        "#     for img, target in loader[\"test\"]:\n",
        "#         img, target = img.to(device), target.to(device)\n",
        "#         pred = net(img)\n",
        "#         total += len(target)\n",
        "#         correct += (torch.argmax(pred, dim=1) == target).sum().item()\n",
        "# print(f\"实验 0 (基线) 准确率 @ 40 epochs: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Lxe2QlXwwQRR",
        "outputId": "62fb68c4-893c-43e9-9f95-266edf422f78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 开始训练: 实验 0 (基线) ---\n",
            "[epoch=  1, iter=  200] loss: 2.217\n",
            "[epoch=  1, iter=  400] loss: 1.984\n",
            "[epoch=  1, iter=  600] loss: 1.856\n",
            "[epoch=  2, iter=  200] loss: 1.684\n",
            "[epoch=  2, iter=  400] loss: 1.634\n",
            "[epoch=  2, iter=  600] loss: 1.579\n",
            "[epoch=  3, iter=  200] loss: 1.471\n",
            "[epoch=  3, iter=  400] loss: 1.421\n",
            "[epoch=  3, iter=  600] loss: 1.398\n",
            "[epoch=  4, iter=  200] loss: 1.303\n",
            "[epoch=  4, iter=  400] loss: 1.281\n",
            "[epoch=  4, iter=  600] loss: 1.258\n",
            "[epoch=  5, iter=  200] loss: 1.173\n",
            "[epoch=  5, iter=  400] loss: 1.173\n",
            "[epoch=  5, iter=  600] loss: 1.134\n",
            "[epoch=  6, iter=  200] loss: 1.103\n",
            "[epoch=  6, iter=  400] loss: 1.088\n",
            "[epoch=  6, iter=  600] loss: 1.067\n",
            "[epoch=  7, iter=  200] loss: 1.036\n",
            "[epoch=  7, iter=  400] loss: 1.021\n",
            "[epoch=  7, iter=  600] loss: 1.000\n",
            "[epoch=  8, iter=  200] loss: 0.970\n",
            "[epoch=  8, iter=  400] loss: 0.952\n",
            "[epoch=  8, iter=  600] loss: 0.949\n",
            "[epoch=  9, iter=  200] loss: 0.929\n",
            "[epoch=  9, iter=  400] loss: 0.911\n",
            "[epoch=  9, iter=  600] loss: 0.916\n",
            "[epoch= 10, iter=  200] loss: 0.869\n",
            "[epoch= 10, iter=  400] loss: 0.874\n",
            "[epoch= 10, iter=  600] loss: 0.873\n",
            "[epoch= 11, iter=  200] loss: 0.833\n",
            "[epoch= 11, iter=  400] loss: 0.828\n",
            "[epoch= 11, iter=  600] loss: 0.826\n",
            "[epoch= 12, iter=  200] loss: 0.813\n",
            "[epoch= 12, iter=  400] loss: 0.799\n",
            "[epoch= 12, iter=  600] loss: 0.784\n",
            "[epoch= 13, iter=  200] loss: 0.757\n",
            "[epoch= 13, iter=  400] loss: 0.767\n",
            "[epoch= 13, iter=  600] loss: 0.768\n",
            "[epoch= 14, iter=  200] loss: 0.740\n",
            "[epoch= 14, iter=  400] loss: 0.755\n",
            "[epoch= 14, iter=  600] loss: 0.744\n",
            "[epoch= 15, iter=  200] loss: 0.698\n",
            "[epoch= 15, iter=  400] loss: 0.707\n",
            "[epoch= 15, iter=  600] loss: 0.704\n",
            "[epoch= 16, iter=  200] loss: 0.704\n",
            "[epoch= 16, iter=  400] loss: 0.688\n",
            "[epoch= 16, iter=  600] loss: 0.683\n",
            "[epoch= 17, iter=  200] loss: 0.655\n",
            "[epoch= 17, iter=  400] loss: 0.677\n",
            "[epoch= 17, iter=  600] loss: 0.676\n",
            "[epoch= 18, iter=  200] loss: 0.655\n",
            "[epoch= 18, iter=  400] loss: 0.641\n",
            "[epoch= 18, iter=  600] loss: 0.653\n",
            "[epoch= 19, iter=  200] loss: 0.631\n",
            "[epoch= 19, iter=  400] loss: 0.633\n",
            "[epoch= 19, iter=  600] loss: 0.624\n",
            "[epoch= 20, iter=  200] loss: 0.606\n",
            "[epoch= 20, iter=  400] loss: 0.605\n",
            "[epoch= 20, iter=  600] loss: 0.620\n",
            "[epoch= 21, iter=  200] loss: 0.584\n",
            "[epoch= 21, iter=  400] loss: 0.612\n",
            "[epoch= 21, iter=  600] loss: 0.603\n",
            "[epoch= 22, iter=  200] loss: 0.560\n",
            "[epoch= 22, iter=  400] loss: 0.579\n",
            "[epoch= 22, iter=  600] loss: 0.589\n",
            "[epoch= 23, iter=  200] loss: 0.583\n",
            "[epoch= 23, iter=  400] loss: 0.563\n",
            "[epoch= 23, iter=  600] loss: 0.558\n",
            "[epoch= 24, iter=  200] loss: 0.543\n",
            "[epoch= 24, iter=  400] loss: 0.558\n",
            "[epoch= 24, iter=  600] loss: 0.561\n",
            "[epoch= 25, iter=  200] loss: 0.530\n",
            "[epoch= 25, iter=  400] loss: 0.553\n",
            "[epoch= 25, iter=  600] loss: 0.528\n",
            "[epoch= 26, iter=  200] loss: 0.545\n",
            "[epoch= 26, iter=  400] loss: 0.531\n",
            "[epoch= 26, iter=  600] loss: 0.517\n",
            "[epoch= 27, iter=  200] loss: 0.497\n",
            "[epoch= 27, iter=  400] loss: 0.527\n",
            "[epoch= 27, iter=  600] loss: 0.528\n",
            "[epoch= 28, iter=  200] loss: 0.521\n",
            "[epoch= 28, iter=  400] loss: 0.498\n",
            "[epoch= 28, iter=  600] loss: 0.501\n",
            "[epoch= 29, iter=  200] loss: 0.495\n",
            "[epoch= 29, iter=  400] loss: 0.515\n",
            "[epoch= 29, iter=  600] loss: 0.512\n",
            "[epoch= 30, iter=  200] loss: 0.479\n",
            "[epoch= 30, iter=  400] loss: 0.489\n",
            "[epoch= 30, iter=  600] loss: 0.490\n",
            "[epoch= 31, iter=  200] loss: 0.474\n",
            "[epoch= 31, iter=  400] loss: 0.486\n",
            "[epoch= 31, iter=  600] loss: 0.492\n",
            "[epoch= 32, iter=  200] loss: 0.488\n",
            "[epoch= 32, iter=  400] loss: 0.476\n",
            "[epoch= 32, iter=  600] loss: 0.479\n",
            "[epoch= 33, iter=  200] loss: 0.467\n",
            "[epoch= 33, iter=  400] loss: 0.487\n",
            "[epoch= 33, iter=  600] loss: 0.466\n",
            "[epoch= 34, iter=  200] loss: 0.438\n",
            "[epoch= 34, iter=  400] loss: 0.454\n",
            "[epoch= 34, iter=  600] loss: 0.465\n",
            "[epoch= 35, iter=  200] loss: 0.443\n",
            "[epoch= 35, iter=  400] loss: 0.427\n",
            "[epoch= 35, iter=  600] loss: 0.464\n",
            "[epoch= 36, iter=  200] loss: 0.445\n",
            "[epoch= 36, iter=  400] loss: 0.431\n",
            "[epoch= 36, iter=  600] loss: 0.452\n",
            "[epoch= 37, iter=  200] loss: 0.421\n",
            "[epoch= 37, iter=  400] loss: 0.440\n",
            "[epoch= 37, iter=  600] loss: 0.441\n",
            "[epoch= 38, iter=  200] loss: 0.430\n",
            "[epoch= 38, iter=  400] loss: 0.426\n",
            "[epoch= 38, iter=  600] loss: 0.428\n",
            "[epoch= 39, iter=  200] loss: 0.411\n",
            "[epoch= 39, iter=  400] loss: 0.418\n",
            "[epoch= 39, iter=  600] loss: 0.424\n",
            "[epoch= 40, iter=  200] loss: 0.412\n",
            "[epoch= 40, iter=  400] loss: 0.405\n",
            "[epoch= 40, iter=  600] loss: 0.423\n",
            "--- 训练结束 ---\n",
            "实验 0 (基线) 准确率 @ 40 epochs: 85.09%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # 实验 1: 引入残差机制\n",
        "# # 改动: 1. num_epochs=40  2. 网络重写为ResNet风格\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# import torchvision.datasets as tv_datasets\n",
        "# import torchvision.transforms as tv_transforms\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "# # --- 实验配置 ---\n",
        "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# num_epochs = 40\n",
        "# batch_size = 64\n",
        "# num_workers = 2\n",
        "# print_every = 200\n",
        "# optim_name = \"Adam\"\n",
        "# optim_kwargs = dict(lr=3e-4, weight_decay=1e-6)\n",
        "\n",
        "# # --- 数据预处理 (与基线相同) ---\n",
        "# transformation = dict()\n",
        "# for data_type in (\"train\", \"test\"):\n",
        "#     is_train = data_type==\"train\"\n",
        "#     transformation[data_type] = tv_transforms.Compose(([\n",
        "#         tv_transforms.RandomRotation(degrees=15),\n",
        "#         tv_transforms.RandomHorizontalFlip(),\n",
        "#         tv_transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "#     ] if is_train else []) +\n",
        "#     [\n",
        "#         tv_transforms.ToTensor(),\n",
        "#         tv_transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "#     ])\n",
        "\n",
        "# # --- 数据加载器 (与基线相同) ---\n",
        "# dataset, loader = {}, {}\n",
        "# for data_type in (\"train\", \"test\"):\n",
        "#     is_train = data_type==\"train\"\n",
        "#     dataset[data_type] = tv_datasets.CIFAR10(root=\"./data\", train=is_train, download=True, transform=transformation[data_type])\n",
        "#     loader[data_type] = torch.utils.data.DataLoader(dataset[data_type], batch_size=batch_size, shuffle=is_train, num_workers=num_workers)\n",
        "\n",
        "# # --- 网络架构 (核心修改) ---\n",
        "# class ResidualBlock(nn.Module):\n",
        "#     def __init__(self, in_channels, out_channels, stride=1):\n",
        "#         super().__init__()\n",
        "#         self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "#         self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "#         self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "#         self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "#         self.shortcut = nn.Sequential()\n",
        "#         if stride != 1 or in_channels != out_channels:\n",
        "#             self.shortcut = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(out_channels))\n",
        "#     def forward(self, x):\n",
        "#         out = F.relu(self.bn1(self.conv1(x)))\n",
        "#         out = self.bn2(self.conv2(out))\n",
        "#         out += self.shortcut(x)\n",
        "#         return F.relu(out)\n",
        "\n",
        "# net = nn.Sequential(\n",
        "#     nn.Conv2d(3, 128, 3, padding=1), nn.ReLU(inplace=True),\n",
        "#     ResidualBlock(128, 128), nn.MaxPool2d(2), nn.Dropout(0.3),\n",
        "#     ResidualBlock(128, 256), nn.MaxPool2d(2), nn.Dropout(0.3),\n",
        "#     ResidualBlock(256, 512),\n",
        "#     ResidualBlock(512, 512),\n",
        "#     ResidualBlock(512, 256), nn.MaxPool2d(2), nn.Dropout(0.3),\n",
        "#     nn.Flatten(),\n",
        "#     nn.Linear(256 * 4 * 4, 512), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
        "#     nn.Linear(512, 256), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
        "#     nn.Linear(256, 128), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
        "#     nn.Linear(128, 10),\n",
        "# ).to(device)\n",
        "\n",
        "# # --- 优化器与损失函数 (与基线相同) ---\n",
        "# optimizer = getattr(optim, optim_name)(net.parameters(), **optim_kwargs)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# # --- 训练循环 ---\n",
        "# print(\"--- 开始训练: 实验 1 (残差机制) ---\")\n",
        "# net.train()\n",
        "# for epoch in range(num_epochs):\n",
        "#     running_loss = 0.0\n",
        "#     for i, (img, target) in enumerate(loader[\"train\"]):\n",
        "#         img, target = img.to(device), target.to(device)\n",
        "#         pred = net(img)\n",
        "#         loss = criterion(pred, target)\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         running_loss += loss.item()\n",
        "#         if i % print_every == print_every - 1:\n",
        "#             print(f\"[epoch={epoch + 1:3d}, iter={i + 1:5d}] loss: {running_loss / print_every:.3f}\")\n",
        "#             running_loss = 0.0\n",
        "# print(\"--- 训练结束 ---\")\n",
        "\n",
        "# # --- 评估 ---\n",
        "# net.eval()\n",
        "# correct, total = 0, 0\n",
        "# with torch.no_grad():\n",
        "#     for img, target in loader[\"test\"]:\n",
        "#         img, target = img.to(device), target.to(device)\n",
        "#         pred = net(img)\n",
        "#         total += len(target)\n",
        "#         correct += (torch.argmax(pred, dim=1) == target).sum().item()\n",
        "# print(f\"实验 1 (残差机制) 准确率 @ 40 epochs: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YlnKOR_z1146",
        "outputId": "1cd8aa42-b946-4969-b20e-fea460bb5cae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 开始训练: 实验 1 (残差机制) ---\n",
            "[epoch=  1, iter=  200] loss: 2.167\n",
            "[epoch=  1, iter=  400] loss: 1.888\n",
            "[epoch=  1, iter=  600] loss: 1.735\n",
            "[epoch=  2, iter=  200] loss: 1.550\n",
            "[epoch=  2, iter=  400] loss: 1.498\n",
            "[epoch=  2, iter=  600] loss: 1.395\n",
            "[epoch=  3, iter=  200] loss: 1.303\n",
            "[epoch=  3, iter=  400] loss: 1.268\n",
            "[epoch=  3, iter=  600] loss: 1.230\n",
            "[epoch=  4, iter=  200] loss: 1.164\n",
            "[epoch=  4, iter=  400] loss: 1.127\n",
            "[epoch=  4, iter=  600] loss: 1.102\n",
            "[epoch=  5, iter=  200] loss: 1.062\n",
            "[epoch=  5, iter=  400] loss: 1.036\n",
            "[epoch=  5, iter=  600] loss: 1.032\n",
            "[epoch=  6, iter=  200] loss: 0.999\n",
            "[epoch=  6, iter=  400] loss: 0.968\n",
            "[epoch=  6, iter=  600] loss: 0.945\n",
            "[epoch=  7, iter=  200] loss: 0.921\n",
            "[epoch=  7, iter=  400] loss: 0.894\n",
            "[epoch=  7, iter=  600] loss: 0.901\n",
            "[epoch=  8, iter=  200] loss: 0.859\n",
            "[epoch=  8, iter=  400] loss: 0.850\n",
            "[epoch=  8, iter=  600] loss: 0.854\n",
            "[epoch=  9, iter=  200] loss: 0.814\n",
            "[epoch=  9, iter=  400] loss: 0.805\n",
            "[epoch=  9, iter=  600] loss: 0.766\n",
            "[epoch= 10, iter=  200] loss: 0.755\n",
            "[epoch= 10, iter=  400] loss: 0.751\n",
            "[epoch= 10, iter=  600] loss: 0.748\n",
            "[epoch= 11, iter=  200] loss: 0.721\n",
            "[epoch= 11, iter=  400] loss: 0.727\n",
            "[epoch= 11, iter=  600] loss: 0.716\n",
            "[epoch= 12, iter=  200] loss: 0.676\n",
            "[epoch= 12, iter=  400] loss: 0.683\n",
            "[epoch= 12, iter=  600] loss: 0.693\n",
            "[epoch= 13, iter=  200] loss: 0.649\n",
            "[epoch= 13, iter=  400] loss: 0.641\n",
            "[epoch= 13, iter=  600] loss: 0.653\n",
            "[epoch= 14, iter=  200] loss: 0.605\n",
            "[epoch= 14, iter=  400] loss: 0.613\n",
            "[epoch= 14, iter=  600] loss: 0.617\n",
            "[epoch= 15, iter=  200] loss: 0.589\n",
            "[epoch= 15, iter=  400] loss: 0.600\n",
            "[epoch= 15, iter=  600] loss: 0.569\n",
            "[epoch= 16, iter=  200] loss: 0.551\n",
            "[epoch= 16, iter=  400] loss: 0.564\n",
            "[epoch= 16, iter=  600] loss: 0.546\n",
            "[epoch= 17, iter=  200] loss: 0.533\n",
            "[epoch= 17, iter=  400] loss: 0.542\n",
            "[epoch= 17, iter=  600] loss: 0.532\n",
            "[epoch= 18, iter=  200] loss: 0.534\n",
            "[epoch= 18, iter=  400] loss: 0.517\n",
            "[epoch= 18, iter=  600] loss: 0.504\n",
            "[epoch= 19, iter=  200] loss: 0.489\n",
            "[epoch= 19, iter=  400] loss: 0.501\n",
            "[epoch= 19, iter=  600] loss: 0.504\n",
            "[epoch= 20, iter=  200] loss: 0.485\n",
            "[epoch= 20, iter=  400] loss: 0.479\n",
            "[epoch= 20, iter=  600] loss: 0.475\n",
            "[epoch= 21, iter=  200] loss: 0.467\n",
            "[epoch= 21, iter=  400] loss: 0.458\n",
            "[epoch= 21, iter=  600] loss: 0.456\n",
            "[epoch= 22, iter=  200] loss: 0.454\n",
            "[epoch= 22, iter=  400] loss: 0.441\n",
            "[epoch= 22, iter=  600] loss: 0.438\n",
            "[epoch= 23, iter=  200] loss: 0.439\n",
            "[epoch= 23, iter=  400] loss: 0.441\n",
            "[epoch= 23, iter=  600] loss: 0.410\n",
            "[epoch= 24, iter=  200] loss: 0.419\n",
            "[epoch= 24, iter=  400] loss: 0.409\n",
            "[epoch= 24, iter=  600] loss: 0.423\n",
            "[epoch= 25, iter=  200] loss: 0.392\n",
            "[epoch= 25, iter=  400] loss: 0.410\n",
            "[epoch= 25, iter=  600] loss: 0.405\n",
            "[epoch= 26, iter=  200] loss: 0.389\n",
            "[epoch= 26, iter=  400] loss: 0.387\n",
            "[epoch= 26, iter=  600] loss: 0.398\n",
            "[epoch= 27, iter=  200] loss: 0.384\n",
            "[epoch= 27, iter=  400] loss: 0.376\n",
            "[epoch= 27, iter=  600] loss: 0.377\n",
            "[epoch= 28, iter=  200] loss: 0.360\n",
            "[epoch= 28, iter=  400] loss: 0.369\n",
            "[epoch= 28, iter=  600] loss: 0.363\n",
            "[epoch= 29, iter=  200] loss: 0.357\n",
            "[epoch= 29, iter=  400] loss: 0.358\n",
            "[epoch= 29, iter=  600] loss: 0.362\n",
            "[epoch= 30, iter=  200] loss: 0.337\n",
            "[epoch= 30, iter=  400] loss: 0.358\n",
            "[epoch= 30, iter=  600] loss: 0.343\n",
            "[epoch= 31, iter=  200] loss: 0.339\n",
            "[epoch= 31, iter=  400] loss: 0.331\n",
            "[epoch= 31, iter=  600] loss: 0.343\n",
            "[epoch= 32, iter=  200] loss: 0.316\n",
            "[epoch= 32, iter=  400] loss: 0.315\n",
            "[epoch= 32, iter=  600] loss: 0.330\n",
            "[epoch= 33, iter=  200] loss: 0.294\n",
            "[epoch= 33, iter=  400] loss: 0.325\n",
            "[epoch= 33, iter=  600] loss: 0.300\n",
            "[epoch= 34, iter=  200] loss: 0.308\n",
            "[epoch= 34, iter=  400] loss: 0.314\n",
            "[epoch= 34, iter=  600] loss: 0.291\n",
            "[epoch= 35, iter=  200] loss: 0.289\n",
            "[epoch= 35, iter=  400] loss: 0.305\n",
            "[epoch= 35, iter=  600] loss: 0.305\n",
            "[epoch= 36, iter=  200] loss: 0.290\n",
            "[epoch= 36, iter=  400] loss: 0.273\n",
            "[epoch= 36, iter=  600] loss: 0.297\n",
            "[epoch= 37, iter=  200] loss: 0.283\n",
            "[epoch= 37, iter=  400] loss: 0.290\n",
            "[epoch= 37, iter=  600] loss: 0.291\n",
            "[epoch= 38, iter=  200] loss: 0.272\n",
            "[epoch= 38, iter=  400] loss: 0.275\n",
            "[epoch= 38, iter=  600] loss: 0.271\n",
            "[epoch= 39, iter=  200] loss: 0.273\n",
            "[epoch= 39, iter=  400] loss: 0.272\n",
            "[epoch= 39, iter=  600] loss: 0.277\n",
            "[epoch= 40, iter=  200] loss: 0.262\n",
            "[epoch= 40, iter=  400] loss: 0.245\n",
            "[epoch= 40, iter=  600] loss: 0.264\n",
            "--- 训练结束 ---\n",
            "实验 1 (残差机制) 准确率 @ 40 epochs: 90.53%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # 实验 2: 更宽的网络\n",
        "# # 改动: 1. num_epochs=40  2. 网络各层通道数乘以1.5\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# import torchvision.datasets as tv_datasets\n",
        "# import torchvision.transforms as tv_transforms\n",
        "\n",
        "# # --- 实验配置 (与基线相同) ---\n",
        "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# num_epochs = 40\n",
        "# batch_size = 64\n",
        "# num_workers = 2\n",
        "# print_every = 200\n",
        "# optim_name = \"Adam\"\n",
        "# optim_kwargs = dict(lr=3e-4, weight_decay=1e-6)\n",
        "\n",
        "# # --- 数据预处理 (与基线相同) ---\n",
        "# transformation = dict()\n",
        "# for data_type in (\"train\", \"test\"):\n",
        "#     is_train = data_type==\"train\"\n",
        "#     transformation[data_type] = tv_transforms.Compose(([\n",
        "#         tv_transforms.RandomRotation(degrees=15),\n",
        "#         tv_transforms.RandomHorizontalFlip(),\n",
        "#         tv_transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "#     ] if is_train else []) +\n",
        "#     [\n",
        "#         tv_transforms.ToTensor(),\n",
        "#         tv_transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "#     ])\n",
        "\n",
        "# # --- 数据加载器 (与基线相同) ---\n",
        "# dataset, loader = {}, {}\n",
        "# for data_type in (\"train\", \"test\"):\n",
        "#     is_train = data_type==\"train\"\n",
        "#     dataset[data_type] = tv_datasets.CIFAR10(root=\"./data\", train=is_train, download=True, transform=transformation[data_type])\n",
        "#     loader[data_type] = torch.utils.data.DataLoader(dataset[data_type], batch_size=batch_size, shuffle=is_train, num_workers=num_workers)\n",
        "\n",
        "# # --- 网络架构 (核心修改) ---\n",
        "# wm = 1.5 # width_multiplier\n",
        "# c1, c2, c3, c4 = int(128*wm), int(256*wm), int(512*wm), int(256*wm)\n",
        "# l1, l2, l3, l4 = int(512*wm), int(256*wm), int(128*wm), 10\n",
        "\n",
        "# net = nn.Sequential(\n",
        "#     nn.Conv2d(3, c1, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),\n",
        "#     nn.Conv2d(c1, c2, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),\n",
        "#     nn.Conv2d(c2, c3, 3, padding=1), nn.ReLU(inplace=True),\n",
        "#     nn.Conv2d(c3, c3, 3, padding=1), nn.ReLU(inplace=True),\n",
        "#     nn.Conv2d(c3, c4, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),\n",
        "#     nn.Flatten(),\n",
        "#     nn.Linear(c4 * 4 * 4, l1), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
        "#     nn.Linear(l1, l2), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
        "#     nn.Linear(l2, l3), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
        "#     nn.Linear(l3, l4),\n",
        "# ).to(device)\n",
        "\n",
        "# # --- 优化器与损失函数 (与基线相同) ---\n",
        "# optimizer = getattr(optim, optim_name)(net.parameters(), **optim_kwargs)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# # --- 训练循环 ---\n",
        "# print(\"--- 开始训练: 实验 2 (更宽网络) ---\")\n",
        "# net.train()\n",
        "# for epoch in range(num_epochs):\n",
        "#     running_loss = 0.0\n",
        "#     for i, (img, target) in enumerate(loader[\"train\"]):\n",
        "#         img, target = img.to(device), target.to(device)\n",
        "#         pred = net(img)\n",
        "#         loss = criterion(pred, target)\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         running_loss += loss.item()\n",
        "#         if i % print_every == print_every - 1:\n",
        "#             print(f\"[epoch={epoch + 1:3d}, iter={i + 1:5d}] loss: {running_loss / print_every:.3f}\")\n",
        "#             running_loss = 0.0\n",
        "# print(\"--- 训练结束 ---\")\n",
        "\n",
        "# # --- 评估 ---\n",
        "# net.eval()\n",
        "# correct, total = 0, 0\n",
        "# with torch.no_grad():\n",
        "#     for img, target in loader[\"test\"]:\n",
        "#         img, target = img.to(device), target.to(device)\n",
        "#         pred = net(img)\n",
        "#         total += len(target)\n",
        "#         correct += (torch.argmax(pred, dim=1) == target).sum().item()\n",
        "# print(f\"实验 2 (更宽网络) 准确率 @ 40 epochs: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsNRp2fY_R9B",
        "outputId": "e00db015-2c2f-4b46-d2a9-001027a61a36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 开始训练: 实验 2 (更宽网络) ---\n",
            "[epoch=  1, iter=  200] loss: 2.173\n",
            "[epoch=  1, iter=  400] loss: 1.914\n",
            "[epoch=  1, iter=  600] loss: 1.799\n",
            "[epoch=  2, iter=  200] loss: 1.602\n",
            "[epoch=  2, iter=  400] loss: 1.545\n",
            "[epoch=  2, iter=  600] loss: 1.479\n",
            "[epoch=  3, iter=  200] loss: 1.391\n",
            "[epoch=  3, iter=  400] loss: 1.346\n",
            "[epoch=  3, iter=  600] loss: 1.316\n",
            "[epoch=  4, iter=  200] loss: 1.248\n",
            "[epoch=  4, iter=  400] loss: 1.191\n",
            "[epoch=  4, iter=  600] loss: 1.211\n",
            "[epoch=  5, iter=  200] loss: 1.134\n",
            "[epoch=  5, iter=  400] loss: 1.109\n",
            "[epoch=  5, iter=  600] loss: 1.094\n",
            "[epoch=  6, iter=  200] loss: 1.052\n",
            "[epoch=  6, iter=  400] loss: 1.027\n",
            "[epoch=  6, iter=  600] loss: 1.002\n",
            "[epoch=  7, iter=  200] loss: 0.963\n",
            "[epoch=  7, iter=  400] loss: 0.962\n",
            "[epoch=  7, iter=  600] loss: 0.954\n",
            "[epoch=  8, iter=  200] loss: 0.896\n",
            "[epoch=  8, iter=  400] loss: 0.912\n",
            "[epoch=  8, iter=  600] loss: 0.891\n",
            "[epoch=  9, iter=  200] loss: 0.850\n",
            "[epoch=  9, iter=  400] loss: 0.857\n",
            "[epoch=  9, iter=  600] loss: 0.851\n",
            "[epoch= 10, iter=  200] loss: 0.829\n",
            "[epoch= 10, iter=  400] loss: 0.827\n",
            "[epoch= 10, iter=  600] loss: 0.807\n",
            "[epoch= 11, iter=  200] loss: 0.776\n",
            "[epoch= 11, iter=  400] loss: 0.781\n",
            "[epoch= 11, iter=  600] loss: 0.784\n",
            "[epoch= 12, iter=  200] loss: 0.757\n",
            "[epoch= 12, iter=  400] loss: 0.745\n",
            "[epoch= 12, iter=  600] loss: 0.743\n",
            "[epoch= 13, iter=  200] loss: 0.716\n",
            "[epoch= 13, iter=  400] loss: 0.698\n",
            "[epoch= 13, iter=  600] loss: 0.729\n",
            "[epoch= 14, iter=  200] loss: 0.703\n",
            "[epoch= 14, iter=  400] loss: 0.694\n",
            "[epoch= 14, iter=  600] loss: 0.675\n",
            "[epoch= 15, iter=  200] loss: 0.662\n",
            "[epoch= 15, iter=  400] loss: 0.673\n",
            "[epoch= 15, iter=  600] loss: 0.686\n",
            "[epoch= 16, iter=  200] loss: 0.635\n",
            "[epoch= 16, iter=  400] loss: 0.651\n",
            "[epoch= 16, iter=  600] loss: 0.663\n",
            "[epoch= 17, iter=  200] loss: 0.625\n",
            "[epoch= 17, iter=  400] loss: 0.624\n",
            "[epoch= 17, iter=  600] loss: 0.631\n",
            "[epoch= 18, iter=  200] loss: 0.601\n",
            "[epoch= 18, iter=  400] loss: 0.603\n",
            "[epoch= 18, iter=  600] loss: 0.621\n",
            "[epoch= 19, iter=  200] loss: 0.593\n",
            "[epoch= 19, iter=  400] loss: 0.604\n",
            "[epoch= 19, iter=  600] loss: 0.590\n",
            "[epoch= 20, iter=  200] loss: 0.577\n",
            "[epoch= 20, iter=  400] loss: 0.598\n",
            "[epoch= 20, iter=  600] loss: 0.590\n",
            "[epoch= 21, iter=  200] loss: 0.566\n",
            "[epoch= 21, iter=  400] loss: 0.569\n",
            "[epoch= 21, iter=  600] loss: 0.562\n",
            "[epoch= 22, iter=  200] loss: 0.537\n",
            "[epoch= 22, iter=  400] loss: 0.559\n",
            "[epoch= 22, iter=  600] loss: 0.562\n",
            "[epoch= 23, iter=  200] loss: 0.546\n",
            "[epoch= 23, iter=  400] loss: 0.535\n",
            "[epoch= 23, iter=  600] loss: 0.537\n",
            "[epoch= 24, iter=  200] loss: 0.509\n",
            "[epoch= 24, iter=  400] loss: 0.526\n",
            "[epoch= 24, iter=  600] loss: 0.549\n",
            "[epoch= 25, iter=  200] loss: 0.519\n",
            "[epoch= 25, iter=  400] loss: 0.512\n",
            "[epoch= 25, iter=  600] loss: 0.505\n",
            "[epoch= 26, iter=  200] loss: 0.507\n",
            "[epoch= 26, iter=  400] loss: 0.500\n",
            "[epoch= 26, iter=  600] loss: 0.508\n",
            "[epoch= 27, iter=  200] loss: 0.486\n",
            "[epoch= 27, iter=  400] loss: 0.502\n",
            "[epoch= 27, iter=  600] loss: 0.491\n",
            "[epoch= 28, iter=  200] loss: 0.473\n",
            "[epoch= 28, iter=  400] loss: 0.484\n",
            "[epoch= 28, iter=  600] loss: 0.500\n",
            "[epoch= 29, iter=  200] loss: 0.477\n",
            "[epoch= 29, iter=  400] loss: 0.475\n",
            "[epoch= 29, iter=  600] loss: 0.478\n",
            "[epoch= 30, iter=  200] loss: 0.451\n",
            "[epoch= 30, iter=  400] loss: 0.470\n",
            "[epoch= 30, iter=  600] loss: 0.487\n",
            "[epoch= 31, iter=  200] loss: 0.454\n",
            "[epoch= 31, iter=  400] loss: 0.465\n",
            "[epoch= 31, iter=  600] loss: 0.468\n",
            "[epoch= 32, iter=  200] loss: 0.433\n",
            "[epoch= 32, iter=  400] loss: 0.462\n",
            "[epoch= 32, iter=  600] loss: 0.467\n",
            "[epoch= 33, iter=  200] loss: 0.440\n",
            "[epoch= 33, iter=  400] loss: 0.450\n",
            "[epoch= 33, iter=  600] loss: 0.450\n",
            "[epoch= 34, iter=  200] loss: 0.436\n",
            "[epoch= 34, iter=  400] loss: 0.438\n",
            "[epoch= 34, iter=  600] loss: 0.449\n",
            "[epoch= 35, iter=  200] loss: 0.425\n",
            "[epoch= 35, iter=  400] loss: 0.432\n",
            "[epoch= 35, iter=  600] loss: 0.439\n",
            "[epoch= 36, iter=  200] loss: 0.418\n",
            "[epoch= 36, iter=  400] loss: 0.438\n",
            "[epoch= 36, iter=  600] loss: 0.435\n",
            "[epoch= 37, iter=  200] loss: 0.417\n",
            "[epoch= 37, iter=  400] loss: 0.404\n",
            "[epoch= 37, iter=  600] loss: 0.424\n",
            "[epoch= 38, iter=  200] loss: 0.396\n",
            "[epoch= 38, iter=  400] loss: 0.430\n",
            "[epoch= 38, iter=  600] loss: 0.433\n",
            "[epoch= 39, iter=  200] loss: 0.428\n",
            "[epoch= 39, iter=  400] loss: 0.407\n",
            "[epoch= 39, iter=  600] loss: 0.430\n",
            "[epoch= 40, iter=  200] loss: 0.401\n",
            "[epoch= 40, iter=  400] loss: 0.405\n",
            "[epoch= 40, iter=  600] loss: 0.420\n",
            "--- 训练结束 ---\n",
            "实验 2 (更宽网络) 准确率 @ 40 epochs: 85.02%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # 实验 3: 使用AdamW优化器\n",
        "# # 改动: 1. num_epochs=40  2. optim_name改为\"AdamW\"  3. 调整weight_decay\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# import torchvision.datasets as tv_datasets\n",
        "# import torchvision.transforms as tv_transforms\n",
        "\n",
        "# # --- 实验配置 (核心修改) ---\n",
        "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# num_epochs = 40\n",
        "# batch_size = 64\n",
        "# num_workers = 2\n",
        "# print_every = 200\n",
        "# optim_name = \"AdamW\"\n",
        "# optim_kwargs = dict(lr=3e-4, weight_decay=1e-4) # AdamW使用更典型的weight_decay\n",
        "\n",
        "# # --- 数据预处理 (与基线相同) ---\n",
        "# transformation = dict()\n",
        "# for data_type in (\"train\", \"test\"):\n",
        "#     is_train = data_type==\"train\"\n",
        "#     transformation[data_type] = tv_transforms.Compose(([\n",
        "#         tv_transforms.RandomRotation(degrees=15),\n",
        "#         tv_transforms.RandomHorizontalFlip(),\n",
        "#         tv_transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "#     ] if is_train else []) +\n",
        "#     [\n",
        "#         tv_transforms.ToTensor(),\n",
        "#         tv_transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "#     ])\n",
        "\n",
        "# # --- 数据加载器 (与基线相同) ---\n",
        "# dataset, loader = {}, {}\n",
        "# for data_type in (\"train\", \"test\"):\n",
        "#     is_train = data_type==\"train\"\n",
        "#     dataset[data_type] = tv_datasets.CIFAR10(root=\"./data\", train=is_train, download=True, transform=transformation[data_type])\n",
        "#     loader[data_type] = torch.utils.data.DataLoader(dataset[data_type], batch_size=batch_size, shuffle=is_train, num_workers=num_workers)\n",
        "\n",
        "# # --- 网络架构 (与基线相同) ---\n",
        "# net = nn.Sequential(\n",
        "#     nn.Conv2d(3, 128, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),\n",
        "#     nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),\n",
        "#     nn.Conv2d(256, 512, 3, padding=1), nn.ReLU(inplace=True),\n",
        "#     nn.Conv2d(512, 512, 3, padding=1), nn.ReLU(inplace=True),\n",
        "#     nn.Conv2d(512, 256, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),\n",
        "#     nn.Flatten(),\n",
        "#     nn.Linear(256 * 4 * 4, 512), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
        "#     nn.Linear(512, 256), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
        "#     nn.Linear(256, 128), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
        "#     nn.Linear(128, 10),\n",
        "# ).to(device)\n",
        "\n",
        "# # --- 优化器与损失函数 ---\n",
        "# optimizer = getattr(optim, optim_name)(net.parameters(), **optim_kwargs)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# # --- 训练循环 ---\n",
        "# print(\"--- 开始训练: 实验 3 (AdamW优化器) ---\")\n",
        "# net.train()\n",
        "# for epoch in range(num_epochs):\n",
        "#     running_loss = 0.0\n",
        "#     for i, (img, target) in enumerate(loader[\"train\"]):\n",
        "#         img, target = img.to(device), target.to(device)\n",
        "#         pred = net(img)\n",
        "#         loss = criterion(pred, target)\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         running_loss += loss.item()\n",
        "#         if i % print_every == print_every - 1:\n",
        "#             print(f\"[epoch={epoch + 1:3d}, iter={i + 1:5d}] loss: {running_loss / print_every:.3f}\")\n",
        "#             running_loss = 0.0\n",
        "# print(\"--- 训练结束 ---\")\n",
        "\n",
        "# # --- 评估 ---\n",
        "# net.eval()\n",
        "# correct, total = 0, 0\n",
        "# with torch.no_grad():\n",
        "#     for img, target in loader[\"test\"]:\n",
        "#         img, target = img.to(device), target.to(device)\n",
        "#         pred = net(img)\n",
        "#         total += len(target)\n",
        "#         correct += (torch.argmax(pred, dim=1) == target).sum().item()\n",
        "# print(f\"实验 3 (AdamW优化器) 准确率 @ 40 epochs: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "id": "kelX9JosEoGw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05d9e5c1-2d49-47be-ba9b-0add78eb8d73"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:05<00:00, 28.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 开始训练: 实验 3 (AdamW优化器) ---\n",
            "[epoch=  1, iter=  200] loss: 2.191\n",
            "[epoch=  1, iter=  400] loss: 1.972\n",
            "[epoch=  1, iter=  600] loss: 1.818\n",
            "[epoch=  2, iter=  200] loss: 1.664\n",
            "[epoch=  2, iter=  400] loss: 1.608\n",
            "[epoch=  2, iter=  600] loss: 1.549\n",
            "[epoch=  3, iter=  200] loss: 1.469\n",
            "[epoch=  3, iter=  400] loss: 1.430\n",
            "[epoch=  3, iter=  600] loss: 1.365\n",
            "[epoch=  4, iter=  200] loss: 1.300\n",
            "[epoch=  4, iter=  400] loss: 1.256\n",
            "[epoch=  4, iter=  600] loss: 1.228\n",
            "[epoch=  5, iter=  200] loss: 1.183\n",
            "[epoch=  5, iter=  400] loss: 1.147\n",
            "[epoch=  5, iter=  600] loss: 1.145\n",
            "[epoch=  6, iter=  200] loss: 1.096\n",
            "[epoch=  6, iter=  400] loss: 1.076\n",
            "[epoch=  6, iter=  600] loss: 1.051\n",
            "[epoch=  7, iter=  200] loss: 1.010\n",
            "[epoch=  7, iter=  400] loss: 1.000\n",
            "[epoch=  7, iter=  600] loss: 0.996\n",
            "[epoch=  8, iter=  200] loss: 0.960\n",
            "[epoch=  8, iter=  400] loss: 0.953\n",
            "[epoch=  8, iter=  600] loss: 0.940\n",
            "[epoch=  9, iter=  200] loss: 0.886\n",
            "[epoch=  9, iter=  400] loss: 0.908\n",
            "[epoch=  9, iter=  600] loss: 0.877\n",
            "[epoch= 10, iter=  200] loss: 0.864\n",
            "[epoch= 10, iter=  400] loss: 0.849\n",
            "[epoch= 10, iter=  600] loss: 0.851\n",
            "[epoch= 11, iter=  200] loss: 0.828\n",
            "[epoch= 11, iter=  400] loss: 0.805\n",
            "[epoch= 11, iter=  600] loss: 0.825\n",
            "[epoch= 12, iter=  200] loss: 0.797\n",
            "[epoch= 12, iter=  400] loss: 0.783\n",
            "[epoch= 12, iter=  600] loss: 0.783\n",
            "[epoch= 13, iter=  200] loss: 0.749\n",
            "[epoch= 13, iter=  400] loss: 0.749\n",
            "[epoch= 13, iter=  600] loss: 0.764\n",
            "[epoch= 14, iter=  200] loss: 0.727\n",
            "[epoch= 14, iter=  400] loss: 0.719\n",
            "[epoch= 14, iter=  600] loss: 0.720\n",
            "[epoch= 15, iter=  200] loss: 0.681\n",
            "[epoch= 15, iter=  400] loss: 0.688\n",
            "[epoch= 15, iter=  600] loss: 0.691\n",
            "[epoch= 16, iter=  200] loss: 0.678\n",
            "[epoch= 16, iter=  400] loss: 0.669\n",
            "[epoch= 16, iter=  600] loss: 0.659\n",
            "[epoch= 17, iter=  200] loss: 0.633\n",
            "[epoch= 17, iter=  400] loss: 0.646\n",
            "[epoch= 17, iter=  600] loss: 0.642\n",
            "[epoch= 18, iter=  200] loss: 0.632\n",
            "[epoch= 18, iter=  400] loss: 0.617\n",
            "[epoch= 18, iter=  600] loss: 0.630\n",
            "[epoch= 19, iter=  200] loss: 0.596\n",
            "[epoch= 19, iter=  400] loss: 0.615\n",
            "[epoch= 19, iter=  600] loss: 0.611\n",
            "[epoch= 20, iter=  200] loss: 0.592\n",
            "[epoch= 20, iter=  400] loss: 0.595\n",
            "[epoch= 20, iter=  600] loss: 0.592\n",
            "[epoch= 21, iter=  200] loss: 0.560\n",
            "[epoch= 21, iter=  400] loss: 0.572\n",
            "[epoch= 21, iter=  600] loss: 0.580\n",
            "[epoch= 22, iter=  200] loss: 0.562\n",
            "[epoch= 22, iter=  400] loss: 0.558\n",
            "[epoch= 22, iter=  600] loss: 0.560\n",
            "[epoch= 23, iter=  200] loss: 0.546\n",
            "[epoch= 23, iter=  400] loss: 0.546\n",
            "[epoch= 23, iter=  600] loss: 0.557\n",
            "[epoch= 24, iter=  200] loss: 0.515\n",
            "[epoch= 24, iter=  400] loss: 0.525\n",
            "[epoch= 24, iter=  600] loss: 0.535\n",
            "[epoch= 25, iter=  200] loss: 0.516\n",
            "[epoch= 25, iter=  400] loss: 0.526\n",
            "[epoch= 25, iter=  600] loss: 0.525\n",
            "[epoch= 26, iter=  200] loss: 0.500\n",
            "[epoch= 26, iter=  400] loss: 0.508\n",
            "[epoch= 26, iter=  600] loss: 0.518\n",
            "[epoch= 27, iter=  200] loss: 0.503\n",
            "[epoch= 27, iter=  400] loss: 0.491\n",
            "[epoch= 27, iter=  600] loss: 0.506\n",
            "[epoch= 28, iter=  200] loss: 0.485\n",
            "[epoch= 28, iter=  400] loss: 0.500\n",
            "[epoch= 28, iter=  600] loss: 0.490\n",
            "[epoch= 29, iter=  200] loss: 0.477\n",
            "[epoch= 29, iter=  400] loss: 0.481\n",
            "[epoch= 29, iter=  600] loss: 0.493\n",
            "[epoch= 30, iter=  200] loss: 0.471\n",
            "[epoch= 30, iter=  400] loss: 0.462\n",
            "[epoch= 30, iter=  600] loss: 0.475\n",
            "[epoch= 31, iter=  200] loss: 0.449\n",
            "[epoch= 31, iter=  400] loss: 0.454\n",
            "[epoch= 31, iter=  600] loss: 0.471\n",
            "[epoch= 32, iter=  200] loss: 0.441\n",
            "[epoch= 32, iter=  400] loss: 0.450\n",
            "[epoch= 32, iter=  600] loss: 0.460\n",
            "[epoch= 33, iter=  200] loss: 0.425\n",
            "[epoch= 33, iter=  400] loss: 0.445\n",
            "[epoch= 33, iter=  600] loss: 0.443\n",
            "[epoch= 34, iter=  200] loss: 0.431\n",
            "[epoch= 34, iter=  400] loss: 0.443\n",
            "[epoch= 34, iter=  600] loss: 0.448\n",
            "[epoch= 35, iter=  200] loss: 0.424\n",
            "[epoch= 35, iter=  400] loss: 0.419\n",
            "[epoch= 35, iter=  600] loss: 0.436\n",
            "[epoch= 36, iter=  200] loss: 0.414\n",
            "[epoch= 36, iter=  400] loss: 0.408\n",
            "[epoch= 36, iter=  600] loss: 0.432\n",
            "[epoch= 37, iter=  200] loss: 0.400\n",
            "[epoch= 37, iter=  400] loss: 0.429\n",
            "[epoch= 37, iter=  600] loss: 0.422\n",
            "[epoch= 38, iter=  200] loss: 0.405\n",
            "[epoch= 38, iter=  400] loss: 0.410\n",
            "[epoch= 38, iter=  600] loss: 0.422\n",
            "[epoch= 39, iter=  200] loss: 0.387\n",
            "[epoch= 39, iter=  400] loss: 0.400\n",
            "[epoch= 39, iter=  600] loss: 0.410\n",
            "[epoch= 40, iter=  200] loss: 0.398\n",
            "[epoch= 40, iter=  400] loss: 0.414\n",
            "[epoch= 40, iter=  600] loss: 0.396\n",
            "--- 训练结束 ---\n",
            "实验 3 (AdamW优化器) 准确率 @ 40 epochs: 85.74%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # 实验 4: 更丰富的数据增强\n",
        "# # 改动: 1. num_epochs=40  2. 增加了ColorJitter\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# import torchvision.datasets as tv_datasets\n",
        "# import torchvision.transforms as tv_transforms\n",
        "\n",
        "# # --- 实验配置 (与基线相同) ---\n",
        "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# num_epochs = 40\n",
        "# batch_size = 64\n",
        "# num_workers = 2\n",
        "# print_every = 200\n",
        "# optim_name = \"Adam\"\n",
        "# optim_kwargs = dict(lr=3e-4, weight_decay=1e-6)\n",
        "\n",
        "# # --- 数据预处理 (核心修改) ---\n",
        "# transformation = dict()\n",
        "# for data_type in (\"train\", \"test\"):\n",
        "#     is_train = data_type==\"train\"\n",
        "#     transformation[data_type] = tv_transforms.Compose(([\n",
        "#         tv_transforms.RandomRotation(degrees=15),\n",
        "#         tv_transforms.RandomHorizontalFlip(),\n",
        "#         tv_transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "#         tv_transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "#     ] if is_train else []) +\n",
        "#     [\n",
        "#         tv_transforms.ToTensor(),\n",
        "#         tv_transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "#     ])\n",
        "\n",
        "# # --- 数据加载器 ---\n",
        "# dataset, loader = {}, {}\n",
        "# for data_type in (\"train\", \"test\"):\n",
        "#     is_train = data_type==\"train\"\n",
        "#     dataset[data_type] = tv_datasets.CIFAR10(root=\"./data\", train=is_train, download=True, transform=transformation[data_type])\n",
        "#     loader[data_type] = torch.utils.data.DataLoader(dataset[data_type], batch_size=batch_size, shuffle=is_train, num_workers=num_workers)\n",
        "\n",
        "# # --- 网络架构 (与基线相同) ---\n",
        "# net = nn.Sequential(\n",
        "#     nn.Conv2d(3, 128, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),\n",
        "#     nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),\n",
        "#     nn.Conv2d(256, 512, 3, padding=1), nn.ReLU(inplace=True),\n",
        "#     nn.Conv2d(512, 512, 3, padding=1), nn.ReLU(inplace=True),\n",
        "#     nn.Conv2d(512, 256, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),\n",
        "#     nn.Flatten(),\n",
        "#     nn.Linear(256 * 4 * 4, 512), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
        "#     nn.Linear(512, 256), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
        "#     nn.Linear(256, 128), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
        "#     nn.Linear(128, 10),\n",
        "# ).to(device)\n",
        "\n",
        "# # --- 优化器与损失函数 (与基线相同) ---\n",
        "# optimizer = getattr(optim, optim_name)(net.parameters(), **optim_kwargs)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# # --- 训练循环 ---\n",
        "# print(\"--- 开始训练: 实验 4 (更丰富的数据增强) ---\")\n",
        "# net.train()\n",
        "# for epoch in range(num_epochs):\n",
        "#     running_loss = 0.0\n",
        "#     for i, (img, target) in enumerate(loader[\"train\"]):\n",
        "#         img, target = img.to(device), target.to(device)\n",
        "#         pred = net(img)\n",
        "#         loss = criterion(pred, target)\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         running_loss += loss.item()\n",
        "#         if i % print_every == print_every - 1:\n",
        "#             print(f\"[epoch={epoch + 1:3d}, iter={i + 1:5d}] loss: {running_loss / print_every:.3f}\")\n",
        "#             running_loss = 0.0\n",
        "# print(\"--- 训练结束 ---\")\n",
        "\n",
        "# # --- 评估 ---\n",
        "# net.eval()\n",
        "# correct, total = 0, 0\n",
        "# with torch.no_grad():\n",
        "#     for img, target in loader[\"test\"]:\n",
        "#         img, target = img.to(device), target.to(device)\n",
        "#         pred = net(img)\n",
        "#         total += len(target)\n",
        "#         correct += (torch.argmax(pred, dim=1) == target).sum().item()\n",
        "# print(f\"实验 4 (更丰富的数据增强) 准确率 @ 40 epochs: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u41munbjdxvt",
        "outputId": "e9170aad-ff42-4c87-d5e4-bb21166c631d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 开始训练: 实验 4 (更丰富的数据增强) ---\n",
            "[epoch=  1, iter=  200] loss: 2.219\n",
            "[epoch=  1, iter=  400] loss: 2.000\n",
            "[epoch=  1, iter=  600] loss: 1.880\n",
            "[epoch=  2, iter=  200] loss: 1.713\n",
            "[epoch=  2, iter=  400] loss: 1.662\n",
            "[epoch=  2, iter=  600] loss: 1.619\n",
            "[epoch=  3, iter=  200] loss: 1.521\n",
            "[epoch=  3, iter=  400] loss: 1.472\n",
            "[epoch=  3, iter=  600] loss: 1.451\n",
            "[epoch=  4, iter=  200] loss: 1.374\n",
            "[epoch=  4, iter=  400] loss: 1.332\n",
            "[epoch=  4, iter=  600] loss: 1.320\n",
            "[epoch=  5, iter=  200] loss: 1.253\n",
            "[epoch=  5, iter=  400] loss: 1.215\n",
            "[epoch=  5, iter=  600] loss: 1.213\n",
            "[epoch=  6, iter=  200] loss: 1.180\n",
            "[epoch=  6, iter=  400] loss: 1.148\n",
            "[epoch=  6, iter=  600] loss: 1.127\n",
            "[epoch=  7, iter=  200] loss: 1.099\n",
            "[epoch=  7, iter=  400] loss: 1.062\n",
            "[epoch=  7, iter=  600] loss: 1.051\n",
            "[epoch=  8, iter=  200] loss: 1.026\n",
            "[epoch=  8, iter=  400] loss: 1.028\n",
            "[epoch=  8, iter=  600] loss: 1.005\n",
            "[epoch=  9, iter=  200] loss: 0.980\n",
            "[epoch=  9, iter=  400] loss: 0.960\n",
            "[epoch=  9, iter=  600] loss: 0.959\n",
            "[epoch= 10, iter=  200] loss: 0.940\n",
            "[epoch= 10, iter=  400] loss: 0.915\n",
            "[epoch= 10, iter=  600] loss: 0.927\n",
            "[epoch= 11, iter=  200] loss: 0.878\n",
            "[epoch= 11, iter=  400] loss: 0.892\n",
            "[epoch= 11, iter=  600] loss: 0.886\n",
            "[epoch= 12, iter=  200] loss: 0.849\n",
            "[epoch= 12, iter=  400] loss: 0.865\n",
            "[epoch= 12, iter=  600] loss: 0.847\n",
            "[epoch= 13, iter=  200] loss: 0.816\n",
            "[epoch= 13, iter=  400] loss: 0.838\n",
            "[epoch= 13, iter=  600] loss: 0.825\n",
            "[epoch= 14, iter=  200] loss: 0.778\n",
            "[epoch= 14, iter=  400] loss: 0.792\n",
            "[epoch= 14, iter=  600] loss: 0.793\n",
            "[epoch= 15, iter=  200] loss: 0.765\n",
            "[epoch= 15, iter=  400] loss: 0.768\n",
            "[epoch= 15, iter=  600] loss: 0.755\n",
            "[epoch= 16, iter=  200] loss: 0.745\n",
            "[epoch= 16, iter=  400] loss: 0.750\n",
            "[epoch= 16, iter=  600] loss: 0.742\n",
            "[epoch= 17, iter=  200] loss: 0.715\n",
            "[epoch= 17, iter=  400] loss: 0.739\n",
            "[epoch= 17, iter=  600] loss: 0.717\n",
            "[epoch= 18, iter=  200] loss: 0.693\n",
            "[epoch= 18, iter=  400] loss: 0.703\n",
            "[epoch= 18, iter=  600] loss: 0.715\n",
            "[epoch= 19, iter=  200] loss: 0.680\n",
            "[epoch= 19, iter=  400] loss: 0.693\n",
            "[epoch= 19, iter=  600] loss: 0.672\n",
            "[epoch= 20, iter=  200] loss: 0.653\n",
            "[epoch= 20, iter=  400] loss: 0.668\n",
            "[epoch= 20, iter=  600] loss: 0.666\n",
            "[epoch= 21, iter=  200] loss: 0.646\n",
            "[epoch= 21, iter=  400] loss: 0.669\n",
            "[epoch= 21, iter=  600] loss: 0.639\n",
            "[epoch= 22, iter=  200] loss: 0.639\n",
            "[epoch= 22, iter=  400] loss: 0.657\n",
            "[epoch= 22, iter=  600] loss: 0.624\n",
            "[epoch= 23, iter=  200] loss: 0.619\n",
            "[epoch= 23, iter=  400] loss: 0.630\n",
            "[epoch= 23, iter=  600] loss: 0.648\n",
            "[epoch= 24, iter=  200] loss: 0.616\n",
            "[epoch= 24, iter=  400] loss: 0.611\n",
            "[epoch= 24, iter=  600] loss: 0.604\n",
            "[epoch= 25, iter=  200] loss: 0.606\n",
            "[epoch= 25, iter=  400] loss: 0.597\n",
            "[epoch= 25, iter=  600] loss: 0.609\n",
            "[epoch= 26, iter=  200] loss: 0.598\n",
            "[epoch= 26, iter=  400] loss: 0.599\n",
            "[epoch= 26, iter=  600] loss: 0.594\n",
            "[epoch= 27, iter=  200] loss: 0.598\n",
            "[epoch= 27, iter=  400] loss: 0.584\n",
            "[epoch= 27, iter=  600] loss: 0.569\n",
            "[epoch= 28, iter=  200] loss: 0.571\n",
            "[epoch= 28, iter=  400] loss: 0.567\n",
            "[epoch= 28, iter=  600] loss: 0.570\n",
            "[epoch= 29, iter=  200] loss: 0.563\n",
            "[epoch= 29, iter=  400] loss: 0.569\n",
            "[epoch= 29, iter=  600] loss: 0.563\n",
            "[epoch= 30, iter=  200] loss: 0.555\n",
            "[epoch= 30, iter=  400] loss: 0.559\n",
            "[epoch= 30, iter=  600] loss: 0.559\n",
            "[epoch= 31, iter=  200] loss: 0.558\n",
            "[epoch= 31, iter=  400] loss: 0.544\n",
            "[epoch= 31, iter=  600] loss: 0.550\n",
            "[epoch= 32, iter=  200] loss: 0.536\n",
            "[epoch= 32, iter=  400] loss: 0.548\n",
            "[epoch= 32, iter=  600] loss: 0.534\n",
            "[epoch= 33, iter=  200] loss: 0.540\n",
            "[epoch= 33, iter=  400] loss: 0.529\n",
            "[epoch= 33, iter=  600] loss: 0.540\n",
            "[epoch= 34, iter=  200] loss: 0.517\n",
            "[epoch= 34, iter=  400] loss: 0.517\n",
            "[epoch= 34, iter=  600] loss: 0.531\n",
            "[epoch= 35, iter=  200] loss: 0.509\n",
            "[epoch= 35, iter=  400] loss: 0.501\n",
            "[epoch= 35, iter=  600] loss: 0.534\n",
            "[epoch= 36, iter=  200] loss: 0.496\n",
            "[epoch= 36, iter=  400] loss: 0.500\n",
            "[epoch= 36, iter=  600] loss: 0.515\n",
            "[epoch= 37, iter=  200] loss: 0.492\n",
            "[epoch= 37, iter=  400] loss: 0.515\n",
            "[epoch= 37, iter=  600] loss: 0.511\n",
            "[epoch= 38, iter=  200] loss: 0.501\n",
            "[epoch= 38, iter=  400] loss: 0.500\n",
            "[epoch= 38, iter=  600] loss: 0.511\n",
            "[epoch= 39, iter=  200] loss: 0.499\n",
            "[epoch= 39, iter=  400] loss: 0.496\n",
            "[epoch= 39, iter=  600] loss: 0.502\n",
            "[epoch= 40, iter=  200] loss: 0.486\n",
            "[epoch= 40, iter=  400] loss: 0.479\n",
            "[epoch= 40, iter=  600] loss: 0.484\n",
            "--- 训练结束 ---\n",
            "实验 4 (更丰富的数据增强) 准确率 @ 40 epochs: 85.40%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # 实验 5: 引入注意力机制\n",
        "# # 改动: 1. num_epochs=40  2. 网络重写为带SE-Block的结构\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# import torchvision.datasets as tv_datasets\n",
        "# import torchvision.transforms as tv_transforms\n",
        "\n",
        "# # --- 实验配置 (与基线相同) ---\n",
        "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# num_epochs = 40\n",
        "# batch_size = 64\n",
        "# num_workers = 2\n",
        "# print_every = 200\n",
        "# optim_name = \"Adam\"\n",
        "# optim_kwargs = dict(lr=3e-4, weight_decay=1e-6)\n",
        "\n",
        "# # --- 数据预处理 (与基线相同) ---\n",
        "# transformation = dict()\n",
        "# for data_type in (\"train\", \"test\"):\n",
        "#     is_train = data_type==\"train\"\n",
        "#     transformation[data_type] = tv_transforms.Compose(([\n",
        "#         tv_transforms.RandomRotation(degrees=15),\n",
        "#         tv_transforms.RandomHorizontalFlip(),\n",
        "#         tv_transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "#     ] if is_train else []) +\n",
        "#     [\n",
        "#         tv_transforms.ToTensor(),\n",
        "#         tv_transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "#     ])\n",
        "\n",
        "# # --- 数据加载器 (与基线相同) ---\n",
        "# dataset, loader = {}, {}\n",
        "# for data_type in (\"train\", \"test\"):\n",
        "#     is_train = data_type==\"train\"\n",
        "#     dataset[data_type] = tv_datasets.CIFAR10(root=\"./data\", train=is_train, download=True, transform=transformation[data_type])\n",
        "#     loader[data_type] = torch.utils.data.DataLoader(dataset[data_type], batch_size=batch_size, shuffle=is_train, num_workers=num_workers)\n",
        "\n",
        "# # --- 网络架构 (核心修改) ---\n",
        "# class SEBlock(nn.Module):\n",
        "#     def __init__(self, channel, reduction=16):\n",
        "#         super().__init__()\n",
        "#         self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "#         self.fc = nn.Sequential(\n",
        "#             nn.Linear(channel, channel // reduction, bias=False), nn.ReLU(inplace=True),\n",
        "#             nn.Linear(channel // reduction, channel, bias=False), nn.Sigmoid())\n",
        "#     def forward(self, x):\n",
        "#         b, c, _, _ = x.size()\n",
        "#         y = self.avg_pool(x).view(b, c)\n",
        "#         y = self.fc(y).view(b, c, 1, 1)\n",
        "#         return x * y.expand_as(x)\n",
        "\n",
        "# class NetWithAttention(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "#         self.conv_block1 = nn.Sequential(\n",
        "#             nn.Conv2d(3, 128, 3, padding=1), nn.ReLU(inplace=True),\n",
        "#             SEBlock(128),\n",
        "#             nn.MaxPool2d(2), nn.Dropout(0.3)\n",
        "#         )\n",
        "#         self.conv_block2 = nn.Sequential(\n",
        "#             nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(inplace=True),\n",
        "#             SEBlock(256),\n",
        "#             nn.MaxPool2d(2), nn.Dropout(0.3)\n",
        "#         )\n",
        "#         self.conv_block3 = nn.Sequential(\n",
        "#             nn.Conv2d(256, 512, 3, padding=1), nn.ReLU(inplace=True),\n",
        "#             nn.Conv2d(512, 512, 3, padding=1), nn.ReLU(inplace=True),\n",
        "#             nn.Conv2d(512, 256, 3, padding=1), nn.ReLU(inplace=True),\n",
        "#             SEBlock(256),\n",
        "#             nn.MaxPool2d(2), nn.Dropout(0.3)\n",
        "#         )\n",
        "#         self.fc_block = nn.Sequential(\n",
        "#             nn.Flatten(),\n",
        "#             nn.Linear(256 * 4 * 4, 512), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
        "#             nn.Linear(512, 256), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
        "#             nn.Linear(256, 128), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
        "#             nn.Linear(128, 10),\n",
        "#         )\n",
        "#     def forward(self, x):\n",
        "#         x = self.conv_block1(x)\n",
        "#         x = self.conv_block2(x)\n",
        "#         x = self.conv_block3(x)\n",
        "#         x = self.fc_block(x)\n",
        "#         return x\n",
        "# net = NetWithAttention().to(device)\n",
        "\n",
        "# # --- 优化器与损失函数 (与基线相同) ---\n",
        "# optimizer = getattr(optim, optim_name)(net.parameters(), **optim_kwargs)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# # --- 训练循环 ---\n",
        "# print(\"--- 开始训练: 实验 5 (注意力机制) ---\")\n",
        "# net.train()\n",
        "# for epoch in range(num_epochs):\n",
        "#     running_loss = 0.0\n",
        "#     for i, (img, target) in enumerate(loader[\"train\"]):\n",
        "#         img, target = img.to(device), target.to(device)\n",
        "#         pred = net(img)\n",
        "#         loss = criterion(pred, target)\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         running_loss += loss.item()\n",
        "#         if i % print_every == print_every - 1:\n",
        "#             print(f\"[epoch={epoch + 1:3d}, iter={i + 1:5d}] loss: {running_loss / print_every:.3f}\")\n",
        "#             running_loss = 0.0\n",
        "# print(\"--- 训练结束 ---\")\n",
        "\n",
        "# # --- 评估 ---\n",
        "# net.eval()\n",
        "# correct, total = 0, 0\n",
        "# with torch.no_grad():\n",
        "#     for img, target in loader[\"test\"]:\n",
        "#         img, target = img.to(device), target.to(device)\n",
        "#         pred = net(img)\n",
        "#         total += len(target)\n",
        "#         correct += (torch.argmax(pred, dim=1) == target).sum().item()\n",
        "# print(f\"实验 5 (注意力机制) 准确率 @ 40 epochs: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6dVfVx8lCde",
        "outputId": "75b19087-d5c6-41b4-8c1c-558b38bef263"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 开始训练: 实验 5 (注意力机制) ---\n",
            "[epoch=  1, iter=  200] loss: 2.210\n",
            "[epoch=  1, iter=  400] loss: 1.993\n",
            "[epoch=  1, iter=  600] loss: 1.828\n",
            "[epoch=  2, iter=  200] loss: 1.691\n",
            "[epoch=  2, iter=  400] loss: 1.622\n",
            "[epoch=  2, iter=  600] loss: 1.598\n",
            "[epoch=  3, iter=  200] loss: 1.489\n",
            "[epoch=  3, iter=  400] loss: 1.432\n",
            "[epoch=  3, iter=  600] loss: 1.411\n",
            "[epoch=  4, iter=  200] loss: 1.361\n",
            "[epoch=  4, iter=  400] loss: 1.312\n",
            "[epoch=  4, iter=  600] loss: 1.288\n",
            "[epoch=  5, iter=  200] loss: 1.236\n",
            "[epoch=  5, iter=  400] loss: 1.224\n",
            "[epoch=  5, iter=  600] loss: 1.190\n",
            "[epoch=  6, iter=  200] loss: 1.149\n",
            "[epoch=  6, iter=  400] loss: 1.115\n",
            "[epoch=  6, iter=  600] loss: 1.116\n",
            "[epoch=  7, iter=  200] loss: 1.072\n",
            "[epoch=  7, iter=  400] loss: 1.061\n",
            "[epoch=  7, iter=  600] loss: 1.051\n",
            "[epoch=  8, iter=  200] loss: 1.003\n",
            "[epoch=  8, iter=  400] loss: 1.009\n",
            "[epoch=  8, iter=  600] loss: 1.024\n",
            "[epoch=  9, iter=  200] loss: 0.962\n",
            "[epoch=  9, iter=  400] loss: 0.961\n",
            "[epoch=  9, iter=  600] loss: 0.951\n",
            "[epoch= 10, iter=  200] loss: 0.937\n",
            "[epoch= 10, iter=  400] loss: 0.910\n",
            "[epoch= 10, iter=  600] loss: 0.919\n",
            "[epoch= 11, iter=  200] loss: 0.856\n",
            "[epoch= 11, iter=  400] loss: 0.872\n",
            "[epoch= 11, iter=  600] loss: 0.887\n",
            "[epoch= 12, iter=  200] loss: 0.828\n",
            "[epoch= 12, iter=  400] loss: 0.855\n",
            "[epoch= 12, iter=  600] loss: 0.834\n",
            "[epoch= 13, iter=  200] loss: 0.801\n",
            "[epoch= 13, iter=  400] loss: 0.796\n",
            "[epoch= 13, iter=  600] loss: 0.802\n",
            "[epoch= 14, iter=  200] loss: 0.788\n",
            "[epoch= 14, iter=  400] loss: 0.775\n",
            "[epoch= 14, iter=  600] loss: 0.764\n",
            "[epoch= 15, iter=  200] loss: 0.760\n",
            "[epoch= 15, iter=  400] loss: 0.752\n",
            "[epoch= 15, iter=  600] loss: 0.738\n",
            "[epoch= 16, iter=  200] loss: 0.745\n",
            "[epoch= 16, iter=  400] loss: 0.723\n",
            "[epoch= 16, iter=  600] loss: 0.703\n",
            "[epoch= 17, iter=  200] loss: 0.685\n",
            "[epoch= 17, iter=  400] loss: 0.685\n",
            "[epoch= 17, iter=  600] loss: 0.715\n",
            "[epoch= 18, iter=  200] loss: 0.680\n",
            "[epoch= 18, iter=  400] loss: 0.672\n",
            "[epoch= 18, iter=  600] loss: 0.665\n",
            "[epoch= 19, iter=  200] loss: 0.648\n",
            "[epoch= 19, iter=  400] loss: 0.664\n",
            "[epoch= 19, iter=  600] loss: 0.654\n",
            "[epoch= 20, iter=  200] loss: 0.617\n",
            "[epoch= 20, iter=  400] loss: 0.654\n",
            "[epoch= 20, iter=  600] loss: 0.628\n",
            "[epoch= 21, iter=  200] loss: 0.609\n",
            "[epoch= 21, iter=  400] loss: 0.617\n",
            "[epoch= 21, iter=  600] loss: 0.628\n",
            "[epoch= 22, iter=  200] loss: 0.587\n",
            "[epoch= 22, iter=  400] loss: 0.605\n",
            "[epoch= 22, iter=  600] loss: 0.608\n",
            "[epoch= 23, iter=  200] loss: 0.581\n",
            "[epoch= 23, iter=  400] loss: 0.585\n",
            "[epoch= 23, iter=  600] loss: 0.582\n",
            "[epoch= 24, iter=  200] loss: 0.544\n",
            "[epoch= 24, iter=  400] loss: 0.578\n",
            "[epoch= 24, iter=  600] loss: 0.562\n",
            "[epoch= 25, iter=  200] loss: 0.540\n",
            "[epoch= 25, iter=  400] loss: 0.563\n",
            "[epoch= 25, iter=  600] loss: 0.555\n",
            "[epoch= 26, iter=  200] loss: 0.524\n",
            "[epoch= 26, iter=  400] loss: 0.546\n",
            "[epoch= 26, iter=  600] loss: 0.537\n",
            "[epoch= 27, iter=  200] loss: 0.516\n",
            "[epoch= 27, iter=  400] loss: 0.522\n",
            "[epoch= 27, iter=  600] loss: 0.520\n",
            "[epoch= 28, iter=  200] loss: 0.511\n",
            "[epoch= 28, iter=  400] loss: 0.520\n",
            "[epoch= 28, iter=  600] loss: 0.518\n",
            "[epoch= 29, iter=  200] loss: 0.490\n",
            "[epoch= 29, iter=  400] loss: 0.501\n",
            "[epoch= 29, iter=  600] loss: 0.508\n",
            "[epoch= 30, iter=  200] loss: 0.487\n",
            "[epoch= 30, iter=  400] loss: 0.480\n",
            "[epoch= 30, iter=  600] loss: 0.488\n",
            "[epoch= 31, iter=  200] loss: 0.479\n",
            "[epoch= 31, iter=  400] loss: 0.496\n",
            "[epoch= 31, iter=  600] loss: 0.478\n",
            "[epoch= 32, iter=  200] loss: 0.473\n",
            "[epoch= 32, iter=  400] loss: 0.486\n",
            "[epoch= 32, iter=  600] loss: 0.468\n",
            "[epoch= 33, iter=  200] loss: 0.453\n",
            "[epoch= 33, iter=  400] loss: 0.462\n",
            "[epoch= 33, iter=  600] loss: 0.459\n",
            "[epoch= 34, iter=  200] loss: 0.442\n",
            "[epoch= 34, iter=  400] loss: 0.445\n",
            "[epoch= 34, iter=  600] loss: 0.453\n",
            "[epoch= 35, iter=  200] loss: 0.455\n",
            "[epoch= 35, iter=  400] loss: 0.430\n",
            "[epoch= 35, iter=  600] loss: 0.447\n",
            "[epoch= 36, iter=  200] loss: 0.438\n",
            "[epoch= 36, iter=  400] loss: 0.428\n",
            "[epoch= 36, iter=  600] loss: 0.443\n",
            "[epoch= 37, iter=  200] loss: 0.409\n",
            "[epoch= 37, iter=  400] loss: 0.440\n",
            "[epoch= 37, iter=  600] loss: 0.440\n",
            "[epoch= 38, iter=  200] loss: 0.411\n",
            "[epoch= 38, iter=  400] loss: 0.414\n",
            "[epoch= 38, iter=  600] loss: 0.422\n",
            "[epoch= 39, iter=  200] loss: 0.409\n",
            "[epoch= 39, iter=  400] loss: 0.407\n",
            "[epoch= 39, iter=  600] loss: 0.418\n",
            "[epoch= 40, iter=  200] loss: 0.407\n",
            "[epoch= 40, iter=  400] loss: 0.406\n",
            "[epoch= 40, iter=  600] loss: 0.410\n",
            "--- 训练结束 ---\n",
            "实验 5 (注意力机制) 准确率 @ 40 epochs: 83.97%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task3**——60 points"
      ],
      "metadata": {
        "id": "Ou0YXbAnpv2c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we performed the same operations on the new dataset, Tiny ImageNet, as we did in Task 2."
      ],
      "metadata": {
        "id": "JY8BbQG-qPil"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 准备工作 1: 数据集处理 ---\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as tv_datasets\n",
        "import torchvision.transforms as tv_transforms\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# 下载并解压数据集 (如果尚未执行)\n",
        "if not os.path.exists('tiny-imagenet-200'):\n",
        "    print(\"Downloading and unzipping Tiny ImageNet...\")\n",
        "    !wget -q http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
        "    !unzip -q tiny-imagenet-200.zip\n",
        "    print(\"Download complete.\")\n",
        "\n",
        "class TinyImageNetDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"用于加载Tiny ImageNet的自定义Dataset类\"\"\"\n",
        "    def __init__(self, root, split='train', transform=None):\n",
        "        self.root = os.path.join(root, split)\n",
        "        self.transform = transform\n",
        "        self.class_to_idx = {}\n",
        "        self.data = []\n",
        "\n",
        "        # 建立类别到索引的映射\n",
        "        with open(os.path.join(root, 'wnids.txt'), 'r') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                self.class_to_idx[line.strip()] = i\n",
        "\n",
        "        # 加载图像路径和标签\n",
        "        if split == 'train':\n",
        "            for class_name in os.listdir(self.root):\n",
        "                class_dir = os.path.join(self.root, class_name, 'images')\n",
        "                label = self.class_to_idx[class_name]\n",
        "                for img_name in os.listdir(class_dir):\n",
        "                    self.data.append((os.path.join(class_dir, img_name), label))\n",
        "        elif split == 'val':\n",
        "            with open(os.path.join(root, 'val', 'val_annotations.txt'), 'r') as f:\n",
        "                for line in f:\n",
        "                    parts = line.strip().split('\\t')\n",
        "                    img_name, class_name = parts[0], parts[1]\n",
        "                    label = self.class_to_idx[class_name]\n",
        "                    img_path = os.path.join(root, 'val', 'images', img_name)\n",
        "                    self.data.append((img_path, label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.data[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yh2BE3GkqeRD",
        "outputId": "42d2fedf-2916-4e92-b956-6acef89a7bbf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and unzipping Tiny ImageNet...\n",
            "Download complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 准备工作 2: 定义数据增强 ---\n",
        "\n",
        "# Tiny ImageNet的均值和标准差\n",
        "TINY_IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "TINY_IMAGENET_STD = [0.229, 0.224, 0.225]\n",
        "\n",
        "transformation = dict()\n",
        "for data_type in (\"train\", \"val\"): # Tiny ImageNet使用'val'作为测试集\n",
        "    is_train = data_type == \"train\"\n",
        "    transformation[data_type] = tv_transforms.Compose(([\n",
        "        tv_transforms.RandomRotation(degrees=15),\n",
        "        tv_transforms.RandomHorizontalFlip(),\n",
        "        tv_transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "    ] if is_train else []) +\n",
        "    [\n",
        "        tv_transforms.ToTensor(),\n",
        "        tv_transforms.Normalize(mean=TINY_IMAGENET_MEAN, std=TINY_IMAGENET_STD),\n",
        "    ])"
      ],
      "metadata": {
        "id": "4K4hcdjQqpqY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 实验 0: Tiny ImageNet 基线\n",
        "# # 改动: 1. 使用TinyImageNet的Dataset和transform\n",
        "# #       2. 修改网络以适配64x64输入和200个分类\n",
        "# #       3. epoch设置为40\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "\n",
        "# # --- 实验配置 ---\n",
        "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# num_epochs = 40\n",
        "# batch_size = 64\n",
        "# num_workers = 2\n",
        "# print_every = 200\n",
        "# optim_name = \"Adam\"\n",
        "# optim_kwargs = dict(lr=3e-4, weight_decay=1e-6)\n",
        "# data_root = 'tiny-imagenet-200'\n",
        "\n",
        "# # --- 数据加载器 ---\n",
        "# dataset, loader = {}, {}\n",
        "# for data_type in (\"train\", \"val\"):\n",
        "#     is_train = data_type == \"train\"\n",
        "#     dataset[data_type] = TinyImageNetDataset(root=data_root, split=data_type, transform=transformation[data_type])\n",
        "#     loader[data_type] = torch.utils.data.DataLoader(dataset[data_type], batch_size=batch_size, shuffle=is_train, num_workers=num_workers)\n",
        "\n",
        "# # --- 网络架构 (核心修改) ---\n",
        "# net = nn.Sequential(\n",
        "#     nn.Conv2d(3, 128, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),     # 64x64 -> 32x32\n",
        "#     nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),  # 32x32 -> 16x16\n",
        "#     nn.Conv2d(256, 512, 3, padding=1), nn.ReLU(inplace=True),\n",
        "#     nn.Conv2d(512, 512, 3, padding=1), nn.ReLU(inplace=True),\n",
        "#     nn.Conv2d(512, 256, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),  # 16x16 -> 8x8\n",
        "#     nn.Flatten(),\n",
        "#     nn.Linear(256 * 8 * 8, 512), nn.ReLU(inplace=True), nn.Dropout(0.5), # 适配64x64输入\n",
        "#     nn.Linear(512, 256), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
        "#     nn.Linear(256, 128), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
        "#     nn.Linear(128, 200), # 适配200个分类\n",
        "# ).to(device)\n",
        "\n",
        "# # --- 优化器与损失函数 ---\n",
        "# optimizer = getattr(optim, optim_name)(net.parameters(), **optim_kwargs)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# # --- 训练循环 ---\n",
        "# print(\"--- 开始训练: 实验 0 (Tiny ImageNet 基线) ---\")\n",
        "# net.train()\n",
        "# for epoch in range(num_epochs):\n",
        "#     running_loss = 0.0\n",
        "#     for i, (img, target) in enumerate(loader[\"train\"]):\n",
        "#         img, target = img.to(device), target.to(device)\n",
        "#         pred = net(img)\n",
        "#         loss = criterion(pred, target)\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         running_loss += loss.item()\n",
        "#         if i % print_every == print_every - 1:\n",
        "#             print(f\"[epoch={epoch + 1:3d}, iter={i + 1:5d}] loss: {running_loss / print_every:.3f}\")\n",
        "#             running_loss = 0.0\n",
        "# print(\"--- 训练结束 ---\")\n",
        "\n",
        "# # --- 评估 ---\n",
        "# net.eval()\n",
        "# correct, total = 0, 0\n",
        "# with torch.no_grad():\n",
        "#     for img, target in loader[\"val\"]: # 使用 'val' loader\n",
        "#         img, target = img.to(device), target.to(device)\n",
        "#         pred = net(img)\n",
        "#         total += len(target)\n",
        "#         correct += (torch.argmax(pred, dim=1) == target).sum().item()\n",
        "# print(f\"实验 0 (Tiny ImageNet 基线) 准确率 @ 40 epochs: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wm7uTsQUquo-",
        "outputId": "28a2de27-1320-429d-f15c-1f14ca5d0fb9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 开始训练: 实验 0 (Tiny ImageNet 基线) ---\n",
            "[epoch=  1, iter=  200] loss: 5.300\n",
            "[epoch=  1, iter=  400] loss: 5.300\n",
            "[epoch=  1, iter=  600] loss: 5.300\n",
            "[epoch=  1, iter=  800] loss: 5.300\n",
            "[epoch=  1, iter= 1000] loss: 5.300\n",
            "[epoch=  1, iter= 1200] loss: 5.300\n",
            "[epoch=  1, iter= 1400] loss: 5.300\n",
            "[epoch=  2, iter=  200] loss: 5.299\n",
            "[epoch=  2, iter=  400] loss: 5.299\n",
            "[epoch=  2, iter=  600] loss: 5.299\n",
            "[epoch=  2, iter=  800] loss: 5.299\n",
            "[epoch=  2, iter= 1000] loss: 5.299\n",
            "[epoch=  2, iter= 1200] loss: 5.300\n",
            "[epoch=  2, iter= 1400] loss: 5.299\n",
            "[epoch=  3, iter=  200] loss: 5.282\n",
            "[epoch=  3, iter=  400] loss: 5.235\n",
            "[epoch=  3, iter=  600] loss: 5.170\n",
            "[epoch=  3, iter=  800] loss: 5.148\n",
            "[epoch=  3, iter= 1000] loss: 5.124\n",
            "[epoch=  3, iter= 1200] loss: 5.098\n",
            "[epoch=  3, iter= 1400] loss: 5.077\n",
            "[epoch=  4, iter=  200] loss: 5.049\n",
            "[epoch=  4, iter=  400] loss: 5.020\n",
            "[epoch=  4, iter=  600] loss: 5.005\n",
            "[epoch=  4, iter=  800] loss: 4.990\n",
            "[epoch=  4, iter= 1000] loss: 4.966\n",
            "[epoch=  4, iter= 1200] loss: 4.931\n",
            "[epoch=  4, iter= 1400] loss: 4.927\n",
            "[epoch=  5, iter=  200] loss: 4.855\n",
            "[epoch=  5, iter=  400] loss: 4.852\n",
            "[epoch=  5, iter=  600] loss: 4.827\n",
            "[epoch=  5, iter=  800] loss: 4.815\n",
            "[epoch=  5, iter= 1000] loss: 4.791\n",
            "[epoch=  5, iter= 1200] loss: 4.768\n",
            "[epoch=  5, iter= 1400] loss: 4.765\n",
            "[epoch=  6, iter=  200] loss: 4.712\n",
            "[epoch=  6, iter=  400] loss: 4.693\n",
            "[epoch=  6, iter=  600] loss: 4.668\n",
            "[epoch=  6, iter=  800] loss: 4.668\n",
            "[epoch=  6, iter= 1000] loss: 4.648\n",
            "[epoch=  6, iter= 1200] loss: 4.639\n",
            "[epoch=  6, iter= 1400] loss: 4.609\n",
            "[epoch=  7, iter=  200] loss: 4.589\n",
            "[epoch=  7, iter=  400] loss: 4.591\n",
            "[epoch=  7, iter=  600] loss: 4.566\n",
            "[epoch=  7, iter=  800] loss: 4.566\n",
            "[epoch=  7, iter= 1000] loss: 4.537\n",
            "[epoch=  7, iter= 1200] loss: 4.536\n",
            "[epoch=  7, iter= 1400] loss: 4.537\n",
            "[epoch=  8, iter=  200] loss: 4.512\n",
            "[epoch=  8, iter=  400] loss: 4.493\n",
            "[epoch=  8, iter=  600] loss: 4.490\n",
            "[epoch=  8, iter=  800] loss: 4.488\n",
            "[epoch=  8, iter= 1000] loss: 4.469\n",
            "[epoch=  8, iter= 1200] loss: 4.466\n",
            "[epoch=  8, iter= 1400] loss: 4.454\n",
            "[epoch=  9, iter=  200] loss: 4.430\n",
            "[epoch=  9, iter=  400] loss: 4.423\n",
            "[epoch=  9, iter=  600] loss: 4.416\n",
            "[epoch=  9, iter=  800] loss: 4.406\n",
            "[epoch=  9, iter= 1000] loss: 4.398\n",
            "[epoch=  9, iter= 1200] loss: 4.395\n",
            "[epoch=  9, iter= 1400] loss: 4.399\n",
            "[epoch= 10, iter=  200] loss: 4.364\n",
            "[epoch= 10, iter=  400] loss: 4.351\n",
            "[epoch= 10, iter=  600] loss: 4.349\n",
            "[epoch= 10, iter=  800] loss: 4.325\n",
            "[epoch= 10, iter= 1000] loss: 4.362\n",
            "[epoch= 10, iter= 1200] loss: 4.349\n",
            "[epoch= 10, iter= 1400] loss: 4.334\n",
            "[epoch= 11, iter=  200] loss: 4.320\n",
            "[epoch= 11, iter=  400] loss: 4.312\n",
            "[epoch= 11, iter=  600] loss: 4.301\n",
            "[epoch= 11, iter=  800] loss: 4.311\n",
            "[epoch= 11, iter= 1000] loss: 4.303\n",
            "[epoch= 11, iter= 1200] loss: 4.268\n",
            "[epoch= 11, iter= 1400] loss: 4.270\n",
            "[epoch= 12, iter=  200] loss: 4.265\n",
            "[epoch= 12, iter=  400] loss: 4.233\n",
            "[epoch= 12, iter=  600] loss: 4.235\n",
            "[epoch= 12, iter=  800] loss: 4.262\n",
            "[epoch= 12, iter= 1000] loss: 4.237\n",
            "[epoch= 12, iter= 1200] loss: 4.251\n",
            "[epoch= 12, iter= 1400] loss: 4.251\n",
            "[epoch= 13, iter=  200] loss: 4.226\n",
            "[epoch= 13, iter=  400] loss: 4.236\n",
            "[epoch= 13, iter=  600] loss: 4.201\n",
            "[epoch= 13, iter=  800] loss: 4.223\n",
            "[epoch= 13, iter= 1000] loss: 4.208\n",
            "[epoch= 13, iter= 1200] loss: 4.219\n",
            "[epoch= 13, iter= 1400] loss: 4.199\n",
            "[epoch= 14, iter=  200] loss: 4.202\n",
            "[epoch= 14, iter=  400] loss: 4.173\n",
            "[epoch= 14, iter=  600] loss: 4.168\n",
            "[epoch= 14, iter=  800] loss: 4.188\n",
            "[epoch= 14, iter= 1000] loss: 4.167\n",
            "[epoch= 14, iter= 1200] loss: 4.167\n",
            "[epoch= 14, iter= 1400] loss: 4.160\n",
            "[epoch= 15, iter=  200] loss: 4.167\n",
            "[epoch= 15, iter=  400] loss: 4.154\n",
            "[epoch= 15, iter=  600] loss: 4.137\n",
            "[epoch= 15, iter=  800] loss: 4.124\n",
            "[epoch= 15, iter= 1000] loss: 4.154\n",
            "[epoch= 15, iter= 1200] loss: 4.155\n",
            "[epoch= 15, iter= 1400] loss: 4.123\n",
            "[epoch= 16, iter=  200] loss: 4.119\n",
            "[epoch= 16, iter=  400] loss: 4.137\n",
            "[epoch= 16, iter=  600] loss: 4.118\n",
            "[epoch= 16, iter=  800] loss: 4.076\n",
            "[epoch= 16, iter= 1000] loss: 4.127\n",
            "[epoch= 16, iter= 1200] loss: 4.113\n",
            "[epoch= 16, iter= 1400] loss: 4.090\n",
            "[epoch= 17, iter=  200] loss: 4.084\n",
            "[epoch= 17, iter=  400] loss: 4.097\n",
            "[epoch= 17, iter=  600] loss: 4.091\n",
            "[epoch= 17, iter=  800] loss: 4.069\n",
            "[epoch= 17, iter= 1000] loss: 4.040\n",
            "[epoch= 17, iter= 1200] loss: 4.080\n",
            "[epoch= 17, iter= 1400] loss: 4.084\n",
            "[epoch= 18, iter=  200] loss: 4.049\n",
            "[epoch= 18, iter=  400] loss: 4.057\n",
            "[epoch= 18, iter=  600] loss: 4.059\n",
            "[epoch= 18, iter=  800] loss: 4.049\n",
            "[epoch= 18, iter= 1000] loss: 4.058\n",
            "[epoch= 18, iter= 1200] loss: 4.042\n",
            "[epoch= 18, iter= 1400] loss: 4.055\n",
            "[epoch= 19, iter=  200] loss: 4.025\n",
            "[epoch= 19, iter=  400] loss: 4.024\n",
            "[epoch= 19, iter=  600] loss: 4.031\n",
            "[epoch= 19, iter=  800] loss: 4.020\n",
            "[epoch= 19, iter= 1000] loss: 4.014\n",
            "[epoch= 19, iter= 1200] loss: 4.012\n",
            "[epoch= 19, iter= 1400] loss: 4.017\n",
            "[epoch= 20, iter=  200] loss: 3.983\n",
            "[epoch= 20, iter=  400] loss: 4.009\n",
            "[epoch= 20, iter=  600] loss: 4.000\n",
            "[epoch= 20, iter=  800] loss: 4.010\n",
            "[epoch= 20, iter= 1000] loss: 3.986\n",
            "[epoch= 20, iter= 1200] loss: 3.974\n",
            "[epoch= 20, iter= 1400] loss: 3.997\n",
            "[epoch= 21, iter=  200] loss: 3.983\n",
            "[epoch= 21, iter=  400] loss: 3.975\n",
            "[epoch= 21, iter=  600] loss: 3.986\n",
            "[epoch= 21, iter=  800] loss: 3.981\n",
            "[epoch= 21, iter= 1000] loss: 3.964\n",
            "[epoch= 21, iter= 1200] loss: 3.958\n",
            "[epoch= 21, iter= 1400] loss: 3.927\n",
            "[epoch= 22, iter=  200] loss: 3.920\n",
            "[epoch= 22, iter=  400] loss: 3.945\n",
            "[epoch= 22, iter=  600] loss: 3.953\n",
            "[epoch= 22, iter=  800] loss: 3.960\n",
            "[epoch= 22, iter= 1000] loss: 3.926\n",
            "[epoch= 22, iter= 1200] loss: 3.939\n",
            "[epoch= 22, iter= 1400] loss: 3.951\n",
            "[epoch= 23, iter=  200] loss: 3.958\n",
            "[epoch= 23, iter=  400] loss: 3.927\n",
            "[epoch= 23, iter=  600] loss: 3.915\n",
            "[epoch= 23, iter=  800] loss: 3.893\n",
            "[epoch= 23, iter= 1000] loss: 3.932\n",
            "[epoch= 23, iter= 1200] loss: 3.937\n",
            "[epoch= 23, iter= 1400] loss: 3.924\n",
            "[epoch= 24, iter=  200] loss: 3.898\n",
            "[epoch= 24, iter=  400] loss: 3.880\n",
            "[epoch= 24, iter=  600] loss: 3.922\n",
            "[epoch= 24, iter=  800] loss: 3.901\n",
            "[epoch= 24, iter= 1000] loss: 3.909\n",
            "[epoch= 24, iter= 1200] loss: 3.907\n",
            "[epoch= 24, iter= 1400] loss: 3.886\n",
            "[epoch= 25, iter=  200] loss: 3.866\n",
            "[epoch= 25, iter=  400] loss: 3.882\n",
            "[epoch= 25, iter=  600] loss: 3.876\n",
            "[epoch= 25, iter=  800] loss: 3.898\n",
            "[epoch= 25, iter= 1000] loss: 3.869\n",
            "[epoch= 25, iter= 1200] loss: 3.881\n",
            "[epoch= 25, iter= 1400] loss: 3.904\n",
            "[epoch= 26, iter=  200] loss: 3.868\n",
            "[epoch= 26, iter=  400] loss: 3.847\n",
            "[epoch= 26, iter=  600] loss: 3.860\n",
            "[epoch= 26, iter=  800] loss: 3.868\n",
            "[epoch= 26, iter= 1000] loss: 3.887\n",
            "[epoch= 26, iter= 1200] loss: 3.870\n",
            "[epoch= 26, iter= 1400] loss: 3.857\n",
            "[epoch= 27, iter=  200] loss: 3.846\n",
            "[epoch= 27, iter=  400] loss: 3.833\n",
            "[epoch= 27, iter=  600] loss: 3.854\n",
            "[epoch= 27, iter=  800] loss: 3.853\n",
            "[epoch= 27, iter= 1000] loss: 3.858\n",
            "[epoch= 27, iter= 1200] loss: 3.836\n",
            "[epoch= 27, iter= 1400] loss: 3.836\n",
            "[epoch= 28, iter=  200] loss: 3.818\n",
            "[epoch= 28, iter=  400] loss: 3.831\n",
            "[epoch= 28, iter=  600] loss: 3.825\n",
            "[epoch= 28, iter=  800] loss: 3.824\n",
            "[epoch= 28, iter= 1000] loss: 3.838\n",
            "[epoch= 28, iter= 1200] loss: 3.821\n",
            "[epoch= 28, iter= 1400] loss: 3.843\n",
            "[epoch= 29, iter=  200] loss: 3.825\n",
            "[epoch= 29, iter=  400] loss: 3.808\n",
            "[epoch= 29, iter=  600] loss: 3.829\n",
            "[epoch= 29, iter=  800] loss: 3.806\n",
            "[epoch= 29, iter= 1000] loss: 3.821\n",
            "[epoch= 29, iter= 1200] loss: 3.836\n",
            "[epoch= 29, iter= 1400] loss: 3.817\n",
            "[epoch= 30, iter=  200] loss: 3.767\n",
            "[epoch= 30, iter=  400] loss: 3.800\n",
            "[epoch= 30, iter=  600] loss: 3.794\n",
            "[epoch= 30, iter=  800] loss: 3.789\n",
            "[epoch= 30, iter= 1000] loss: 3.808\n",
            "[epoch= 30, iter= 1200] loss: 3.807\n",
            "[epoch= 30, iter= 1400] loss: 3.803\n",
            "[epoch= 31, iter=  200] loss: 3.807\n",
            "[epoch= 31, iter=  400] loss: 3.804\n",
            "[epoch= 31, iter=  600] loss: 3.763\n",
            "[epoch= 31, iter=  800] loss: 3.784\n",
            "[epoch= 31, iter= 1000] loss: 3.780\n",
            "[epoch= 31, iter= 1200] loss: 3.766\n",
            "[epoch= 31, iter= 1400] loss: 3.768\n",
            "[epoch= 32, iter=  200] loss: 3.776\n",
            "[epoch= 32, iter=  400] loss: 3.775\n",
            "[epoch= 32, iter=  600] loss: 3.752\n",
            "[epoch= 32, iter=  800] loss: 3.782\n",
            "[epoch= 32, iter= 1000] loss: 3.773\n",
            "[epoch= 32, iter= 1200] loss: 3.745\n",
            "[epoch= 32, iter= 1400] loss: 3.763\n",
            "[epoch= 33, iter=  200] loss: 3.773\n",
            "[epoch= 33, iter=  400] loss: 3.758\n",
            "[epoch= 33, iter=  600] loss: 3.766\n",
            "[epoch= 33, iter=  800] loss: 3.750\n",
            "[epoch= 33, iter= 1000] loss: 3.770\n",
            "[epoch= 33, iter= 1200] loss: 3.740\n",
            "[epoch= 33, iter= 1400] loss: 3.772\n",
            "[epoch= 34, iter=  200] loss: 3.761\n",
            "[epoch= 34, iter=  400] loss: 3.735\n",
            "[epoch= 34, iter=  600] loss: 3.763\n",
            "[epoch= 34, iter=  800] loss: 3.741\n",
            "[epoch= 34, iter= 1000] loss: 3.737\n",
            "[epoch= 34, iter= 1200] loss: 3.717\n",
            "[epoch= 34, iter= 1400] loss: 3.756\n",
            "[epoch= 35, iter=  200] loss: 3.729\n",
            "[epoch= 35, iter=  400] loss: 3.749\n",
            "[epoch= 35, iter=  600] loss: 3.744\n",
            "[epoch= 35, iter=  800] loss: 3.726\n",
            "[epoch= 35, iter= 1000] loss: 3.737\n",
            "[epoch= 35, iter= 1200] loss: 3.702\n",
            "[epoch= 35, iter= 1400] loss: 3.741\n",
            "[epoch= 36, iter=  200] loss: 3.693\n",
            "[epoch= 36, iter=  400] loss: 3.700\n",
            "[epoch= 36, iter=  600] loss: 3.691\n",
            "[epoch= 36, iter=  800] loss: 3.742\n",
            "[epoch= 36, iter= 1000] loss: 3.718\n",
            "[epoch= 36, iter= 1200] loss: 3.745\n",
            "[epoch= 36, iter= 1400] loss: 3.713\n",
            "[epoch= 37, iter=  200] loss: 3.719\n",
            "[epoch= 37, iter=  400] loss: 3.706\n",
            "[epoch= 37, iter=  600] loss: 3.711\n",
            "[epoch= 37, iter=  800] loss: 3.704\n",
            "[epoch= 37, iter= 1000] loss: 3.732\n",
            "[epoch= 37, iter= 1200] loss: 3.727\n",
            "[epoch= 37, iter= 1400] loss: 3.715\n",
            "[epoch= 38, iter=  200] loss: 3.702\n",
            "[epoch= 38, iter=  400] loss: 3.677\n",
            "[epoch= 38, iter=  600] loss: 3.704\n",
            "[epoch= 38, iter=  800] loss: 3.705\n",
            "[epoch= 38, iter= 1000] loss: 3.691\n",
            "[epoch= 38, iter= 1200] loss: 3.704\n",
            "[epoch= 38, iter= 1400] loss: 3.699\n",
            "[epoch= 39, iter=  200] loss: 3.675\n",
            "[epoch= 39, iter=  400] loss: 3.681\n",
            "[epoch= 39, iter=  600] loss: 3.694\n",
            "[epoch= 39, iter=  800] loss: 3.700\n",
            "[epoch= 39, iter= 1000] loss: 3.698\n",
            "[epoch= 39, iter= 1200] loss: 3.685\n",
            "[epoch= 39, iter= 1400] loss: 3.687\n",
            "[epoch= 40, iter=  200] loss: 3.670\n",
            "[epoch= 40, iter=  400] loss: 3.656\n",
            "[epoch= 40, iter=  600] loss: 3.679\n",
            "[epoch= 40, iter=  800] loss: 3.675\n",
            "[epoch= 40, iter= 1000] loss: 3.696\n",
            "[epoch= 40, iter= 1200] loss: 3.687\n",
            "[epoch= 40, iter= 1400] loss: 3.706\n",
            "--- 训练结束 ---\n",
            "实验 0 (Tiny ImageNet 基线) 准确率 @ 40 epochs: 20.90%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 实验 1: Tiny ImageNet 残差机制\n",
        "# 改动: 1. 网络重写为带跳跃连接的ResNet风格\n",
        "#       2. 适配TinyImageNet\n",
        "#       3. epoch设置为40\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# --- 实验配置 ---\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_epochs = 40\n",
        "batch_size = 64\n",
        "num_workers = 2\n",
        "print_every = 200\n",
        "optim_name = \"Adam\"\n",
        "optim_kwargs = dict(lr=3e-4, weight_decay=1e-6)\n",
        "data_root = 'tiny-imagenet-200'\n",
        "\n",
        "# --- 数据加载器 (与基线相同) ---\n",
        "dataset, loader = {}, {}\n",
        "for data_type in (\"train\", \"val\"):\n",
        "    is_train = data_type == \"train\"\n",
        "    dataset[data_type] = TinyImageNetDataset(root=data_root, split=data_type, transform=transformation[data_type])\n",
        "    loader[data_type] = torch.utils.data.DataLoader(dataset[data_type], batch_size=batch_size, shuffle=is_train, num_workers=num_workers)\n",
        "\n",
        "# --- 网络架构 (核心修改) ---\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"一个简单的残差块\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
        "        # 如果维度不匹配(通道数或尺寸变化)，则需要一个1x1卷积来调整shortcut\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Conv2d(in_channels, out_channels, 1, stride=stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.conv1(x))\n",
        "        out = self.conv2(out)\n",
        "        out += self.shortcut(x) # 核心：跳跃连接\n",
        "        return F.relu(out)\n",
        "\n",
        "class NetWithResidual(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv_block1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 128, 3, padding=1), nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2), nn.Dropout(0.3)\n",
        "        )\n",
        "        self.conv_block2 = nn.Sequential(\n",
        "            ResidualBlock(128, 256, stride=1), # stride=1, 尺寸不变\n",
        "            nn.MaxPool2d(2), nn.Dropout(0.3)\n",
        "        )\n",
        "        self.conv_block3 = nn.Sequential(\n",
        "            ResidualBlock(256, 512, stride=1),\n",
        "            ResidualBlock(512, 512, stride=1),\n",
        "            ResidualBlock(512, 256, stride=1),\n",
        "            nn.MaxPool2d(2), nn.Dropout(0.3)\n",
        "        )\n",
        "        self.fc_block = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256 * 8 * 8, 512), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
        "            nn.Linear(512, 256), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
        "            nn.Linear(256, 128), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
        "            nn.Linear(128, 200),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_block1(x)\n",
        "        x = self.conv_block2(x)\n",
        "        x = self.conv_block3(x)\n",
        "        x = self.fc_block(x)\n",
        "        return x\n",
        "\n",
        "net = NetWithResidual().to(device)\n",
        "\n",
        "# --- 优化器与损失函数 ---\n",
        "optimizer = getattr(optim, optim_name)(net.parameters(), **optim_kwargs)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# --- 训练循环 ---\n",
        "print(\"--- 开始训练: 实验 1 (Tiny ImageNet 残差机制) ---\")\n",
        "net.train()\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, (img, target) in enumerate(loader[\"train\"]):\n",
        "        img, target = img.to(device), target.to(device)\n",
        "        pred = net(img)\n",
        "        loss = criterion(pred, target)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if i % print_every == print_every - 1:\n",
        "            print(f\"[epoch={epoch + 1:3d}, iter={i + 1:5d}] loss: {running_loss / print_every:.3f}\")\n",
        "            running_loss = 0.0\n",
        "print(\"--- 训练结束 ---\")\n",
        "\n",
        "# --- 评估 ---\n",
        "net.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for img, target in loader[\"val\"]:\n",
        "        img, target = img.to(device), target.to(device)\n",
        "        pred = net(img)\n",
        "        total += len(target)\n",
        "        correct += (torch.argmax(pred, dim=1) == target).sum().item()\n",
        "print(f\"实验 1 (Tiny ImageNet 残差机制) 准确率 @ 40 epochs: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rlq6idTw8qXg",
        "outputId": "67e64cbb-a0a5-484f-84a9-9a7f4a535328"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 开始训练: 实验 1 (Tiny ImageNet 残差机制) ---\n",
            "[epoch=  1, iter=  200] loss: 5.300\n",
            "[epoch=  1, iter=  400] loss: 5.300\n",
            "[epoch=  1, iter=  600] loss: 5.289\n",
            "[epoch=  1, iter=  800] loss: 5.248\n",
            "[epoch=  1, iter= 1000] loss: 5.226\n",
            "[epoch=  1, iter= 1200] loss: 5.194\n",
            "[epoch=  1, iter= 1400] loss: 5.156\n",
            "[epoch=  2, iter=  200] loss: 5.114\n",
            "[epoch=  2, iter=  400] loss: 5.090\n",
            "[epoch=  2, iter=  600] loss: 5.061\n",
            "[epoch=  2, iter=  800] loss: 5.035\n",
            "[epoch=  2, iter= 1000] loss: 5.014\n",
            "[epoch=  2, iter= 1200] loss: 4.980\n",
            "[epoch=  2, iter= 1400] loss: 4.948\n",
            "[epoch=  3, iter=  200] loss: 4.899\n",
            "[epoch=  3, iter=  400] loss: 4.897\n",
            "[epoch=  3, iter=  600] loss: 4.842\n",
            "[epoch=  3, iter=  800] loss: 4.812\n",
            "[epoch=  3, iter= 1000] loss: 4.764\n",
            "[epoch=  3, iter= 1200] loss: 4.731\n",
            "[epoch=  3, iter= 1400] loss: 4.695\n",
            "[epoch=  4, iter=  200] loss: 4.637\n",
            "[epoch=  4, iter=  400] loss: 4.596\n",
            "[epoch=  4, iter=  600] loss: 4.598\n",
            "[epoch=  4, iter=  800] loss: 4.533\n",
            "[epoch=  4, iter= 1000] loss: 4.526\n",
            "[epoch=  4, iter= 1200] loss: 4.495\n",
            "[epoch=  4, iter= 1400] loss: 4.472\n",
            "[epoch=  5, iter=  200] loss: 4.456\n",
            "[epoch=  5, iter=  400] loss: 4.406\n",
            "[epoch=  5, iter=  600] loss: 4.387\n",
            "[epoch=  5, iter=  800] loss: 4.382\n",
            "[epoch=  5, iter= 1000] loss: 4.347\n",
            "[epoch=  5, iter= 1200] loss: 4.347\n",
            "[epoch=  5, iter= 1400] loss: 4.347\n",
            "[epoch=  6, iter=  200] loss: 4.269\n",
            "[epoch=  6, iter=  400] loss: 4.279\n",
            "[epoch=  6, iter=  600] loss: 4.275\n",
            "[epoch=  6, iter=  800] loss: 4.234\n",
            "[epoch=  6, iter= 1000] loss: 4.233\n",
            "[epoch=  6, iter= 1200] loss: 4.239\n",
            "[epoch=  6, iter= 1400] loss: 4.191\n",
            "[epoch=  7, iter=  200] loss: 4.163\n",
            "[epoch=  7, iter=  400] loss: 4.139\n",
            "[epoch=  7, iter=  600] loss: 4.141\n",
            "[epoch=  7, iter=  800] loss: 4.140\n",
            "[epoch=  7, iter= 1000] loss: 4.118\n",
            "[epoch=  7, iter= 1200] loss: 4.109\n",
            "[epoch=  7, iter= 1400] loss: 4.103\n",
            "[epoch=  8, iter=  200] loss: 4.042\n",
            "[epoch=  8, iter=  400] loss: 4.067\n",
            "[epoch=  8, iter=  600] loss: 4.055\n",
            "[epoch=  8, iter=  800] loss: 4.050\n",
            "[epoch=  8, iter= 1000] loss: 4.031\n",
            "[epoch=  8, iter= 1200] loss: 4.005\n",
            "[epoch=  8, iter= 1400] loss: 3.996\n",
            "[epoch=  9, iter=  200] loss: 3.965\n",
            "[epoch=  9, iter=  400] loss: 3.967\n",
            "[epoch=  9, iter=  600] loss: 3.956\n",
            "[epoch=  9, iter=  800] loss: 3.925\n",
            "[epoch=  9, iter= 1000] loss: 3.942\n",
            "[epoch=  9, iter= 1200] loss: 3.939\n",
            "[epoch=  9, iter= 1400] loss: 3.941\n",
            "[epoch= 10, iter=  200] loss: 3.881\n",
            "[epoch= 10, iter=  400] loss: 3.866\n",
            "[epoch= 10, iter=  600] loss: 3.891\n",
            "[epoch= 10, iter=  800] loss: 3.888\n",
            "[epoch= 10, iter= 1000] loss: 3.876\n",
            "[epoch= 10, iter= 1200] loss: 3.856\n",
            "[epoch= 10, iter= 1400] loss: 3.847\n",
            "[epoch= 11, iter=  200] loss: 3.814\n",
            "[epoch= 11, iter=  400] loss: 3.829\n",
            "[epoch= 11, iter=  600] loss: 3.798\n",
            "[epoch= 11, iter=  800] loss: 3.796\n",
            "[epoch= 11, iter= 1000] loss: 3.793\n",
            "[epoch= 11, iter= 1200] loss: 3.803\n",
            "[epoch= 11, iter= 1400] loss: 3.812\n",
            "[epoch= 12, iter=  200] loss: 3.758\n",
            "[epoch= 12, iter=  400] loss: 3.745\n",
            "[epoch= 12, iter=  600] loss: 3.743\n",
            "[epoch= 12, iter=  800] loss: 3.777\n",
            "[epoch= 12, iter= 1000] loss: 3.758\n",
            "[epoch= 12, iter= 1200] loss: 3.747\n",
            "[epoch= 12, iter= 1400] loss: 3.762\n",
            "[epoch= 13, iter=  200] loss: 3.673\n",
            "[epoch= 13, iter=  400] loss: 3.717\n",
            "[epoch= 13, iter=  600] loss: 3.707\n",
            "[epoch= 13, iter=  800] loss: 3.703\n",
            "[epoch= 13, iter= 1000] loss: 3.694\n",
            "[epoch= 13, iter= 1200] loss: 3.702\n",
            "[epoch= 13, iter= 1400] loss: 3.688\n",
            "[epoch= 14, iter=  200] loss: 3.641\n",
            "[epoch= 14, iter=  400] loss: 3.642\n",
            "[epoch= 14, iter=  600] loss: 3.646\n",
            "[epoch= 14, iter=  800] loss: 3.627\n",
            "[epoch= 14, iter= 1000] loss: 3.687\n",
            "[epoch= 14, iter= 1200] loss: 3.646\n",
            "[epoch= 14, iter= 1400] loss: 3.657\n",
            "[epoch= 15, iter=  200] loss: 3.606\n",
            "[epoch= 15, iter=  400] loss: 3.590\n",
            "[epoch= 15, iter=  600] loss: 3.621\n",
            "[epoch= 15, iter=  800] loss: 3.599\n",
            "[epoch= 15, iter= 1000] loss: 3.633\n",
            "[epoch= 15, iter= 1200] loss: 3.633\n",
            "[epoch= 15, iter= 1400] loss: 3.623\n",
            "[epoch= 16, iter=  200] loss: 3.566\n",
            "[epoch= 16, iter=  400] loss: 3.568\n",
            "[epoch= 16, iter=  600] loss: 3.563\n",
            "[epoch= 16, iter=  800] loss: 3.572\n",
            "[epoch= 16, iter= 1000] loss: 3.571\n",
            "[epoch= 16, iter= 1200] loss: 3.564\n",
            "[epoch= 16, iter= 1400] loss: 3.605\n",
            "[epoch= 17, iter=  200] loss: 3.506\n",
            "[epoch= 17, iter=  400] loss: 3.545\n",
            "[epoch= 17, iter=  600] loss: 3.520\n",
            "[epoch= 17, iter=  800] loss: 3.545\n",
            "[epoch= 17, iter= 1000] loss: 3.529\n",
            "[epoch= 17, iter= 1200] loss: 3.518\n",
            "[epoch= 17, iter= 1400] loss: 3.535\n",
            "[epoch= 18, iter=  200] loss: 3.513\n",
            "[epoch= 18, iter=  400] loss: 3.503\n",
            "[epoch= 18, iter=  600] loss: 3.501\n",
            "[epoch= 18, iter=  800] loss: 3.520\n",
            "[epoch= 18, iter= 1000] loss: 3.480\n",
            "[epoch= 18, iter= 1200] loss: 3.525\n",
            "[epoch= 18, iter= 1400] loss: 3.491\n",
            "[epoch= 19, iter=  200] loss: 3.513\n",
            "[epoch= 19, iter=  400] loss: 3.466\n",
            "[epoch= 19, iter=  600] loss: 3.482\n",
            "[epoch= 19, iter=  800] loss: 3.432\n",
            "[epoch= 19, iter= 1000] loss: 3.497\n",
            "[epoch= 19, iter= 1200] loss: 3.466\n",
            "[epoch= 19, iter= 1400] loss: 3.493\n",
            "[epoch= 20, iter=  200] loss: 3.437\n",
            "[epoch= 20, iter=  400] loss: 3.415\n",
            "[epoch= 20, iter=  600] loss: 3.423\n",
            "[epoch= 20, iter=  800] loss: 3.467\n",
            "[epoch= 20, iter= 1000] loss: 3.470\n",
            "[epoch= 20, iter= 1200] loss: 3.434\n",
            "[epoch= 20, iter= 1400] loss: 3.473\n",
            "[epoch= 21, iter=  200] loss: 3.415\n",
            "[epoch= 21, iter=  400] loss: 3.431\n",
            "[epoch= 21, iter=  600] loss: 3.424\n",
            "[epoch= 21, iter=  800] loss: 3.418\n",
            "[epoch= 21, iter= 1000] loss: 3.400\n",
            "[epoch= 21, iter= 1200] loss: 3.420\n",
            "[epoch= 21, iter= 1400] loss: 3.400\n",
            "[epoch= 22, iter=  200] loss: 3.416\n",
            "[epoch= 22, iter=  400] loss: 3.388\n",
            "[epoch= 22, iter=  600] loss: 3.404\n",
            "[epoch= 22, iter=  800] loss: 3.395\n",
            "[epoch= 22, iter= 1000] loss: 3.383\n",
            "[epoch= 22, iter= 1200] loss: 3.386\n",
            "[epoch= 22, iter= 1400] loss: 3.401\n",
            "[epoch= 23, iter=  200] loss: 3.353\n",
            "[epoch= 23, iter=  400] loss: 3.387\n",
            "[epoch= 23, iter=  600] loss: 3.357\n",
            "[epoch= 23, iter=  800] loss: 3.387\n",
            "[epoch= 23, iter= 1000] loss: 3.370\n",
            "[epoch= 23, iter= 1200] loss: 3.381\n",
            "[epoch= 23, iter= 1400] loss: 3.383\n",
            "[epoch= 24, iter=  200] loss: 3.333\n",
            "[epoch= 24, iter=  400] loss: 3.341\n",
            "[epoch= 24, iter=  600] loss: 3.357\n",
            "[epoch= 24, iter=  800] loss: 3.359\n",
            "[epoch= 24, iter= 1000] loss: 3.358\n",
            "[epoch= 24, iter= 1200] loss: 3.343\n",
            "[epoch= 24, iter= 1400] loss: 3.355\n",
            "[epoch= 25, iter=  200] loss: 3.319\n",
            "[epoch= 25, iter=  400] loss: 3.330\n",
            "[epoch= 25, iter=  600] loss: 3.332\n",
            "[epoch= 25, iter=  800] loss: 3.315\n",
            "[epoch= 25, iter= 1000] loss: 3.366\n",
            "[epoch= 25, iter= 1200] loss: 3.347\n",
            "[epoch= 25, iter= 1400] loss: 3.325\n",
            "[epoch= 26, iter=  200] loss: 3.312\n",
            "[epoch= 26, iter=  400] loss: 3.285\n",
            "[epoch= 26, iter=  600] loss: 3.314\n",
            "[epoch= 26, iter=  800] loss: 3.294\n",
            "[epoch= 26, iter= 1000] loss: 3.316\n",
            "[epoch= 26, iter= 1200] loss: 3.316\n",
            "[epoch= 26, iter= 1400] loss: 3.345\n",
            "[epoch= 27, iter=  200] loss: 3.291\n",
            "[epoch= 27, iter=  400] loss: 3.287\n",
            "[epoch= 27, iter=  600] loss: 3.268\n",
            "[epoch= 27, iter=  800] loss: 3.316\n",
            "[epoch= 27, iter= 1000] loss: 3.328\n",
            "[epoch= 27, iter= 1200] loss: 3.291\n",
            "[epoch= 27, iter= 1400] loss: 3.289\n",
            "[epoch= 28, iter=  200] loss: 3.270\n",
            "[epoch= 28, iter=  400] loss: 3.254\n",
            "[epoch= 28, iter=  600] loss: 3.261\n",
            "[epoch= 28, iter=  800] loss: 3.265\n",
            "[epoch= 28, iter= 1000] loss: 3.297\n",
            "[epoch= 28, iter= 1200] loss: 3.298\n",
            "[epoch= 28, iter= 1400] loss: 3.273\n",
            "[epoch= 29, iter=  200] loss: 3.292\n",
            "[epoch= 29, iter=  400] loss: 3.250\n",
            "[epoch= 29, iter=  600] loss: 3.261\n",
            "[epoch= 29, iter=  800] loss: 3.223\n",
            "[epoch= 29, iter= 1000] loss: 3.259\n",
            "[epoch= 29, iter= 1200] loss: 3.246\n",
            "[epoch= 29, iter= 1400] loss: 3.266\n",
            "[epoch= 30, iter=  200] loss: 3.196\n",
            "[epoch= 30, iter=  400] loss: 3.227\n",
            "[epoch= 30, iter=  600] loss: 3.228\n",
            "[epoch= 30, iter=  800] loss: 3.229\n",
            "[epoch= 30, iter= 1000] loss: 3.248\n",
            "[epoch= 30, iter= 1200] loss: 3.277\n",
            "[epoch= 30, iter= 1400] loss: 3.261\n",
            "[epoch= 31, iter=  200] loss: 3.185\n",
            "[epoch= 31, iter=  400] loss: 3.225\n",
            "[epoch= 31, iter=  600] loss: 3.205\n",
            "[epoch= 31, iter=  800] loss: 3.231\n",
            "[epoch= 31, iter= 1000] loss: 3.229\n",
            "[epoch= 31, iter= 1200] loss: 3.233\n",
            "[epoch= 31, iter= 1400] loss: 3.231\n",
            "[epoch= 32, iter=  200] loss: 3.195\n",
            "[epoch= 32, iter=  400] loss: 3.202\n",
            "[epoch= 32, iter=  600] loss: 3.186\n",
            "[epoch= 32, iter=  800] loss: 3.202\n",
            "[epoch= 32, iter= 1000] loss: 3.230\n",
            "[epoch= 32, iter= 1200] loss: 3.196\n",
            "[epoch= 32, iter= 1400] loss: 3.210\n",
            "[epoch= 33, iter=  200] loss: 3.175\n",
            "[epoch= 33, iter=  400] loss: 3.187\n",
            "[epoch= 33, iter=  600] loss: 3.205\n",
            "[epoch= 33, iter=  800] loss: 3.182\n",
            "[epoch= 33, iter= 1000] loss: 3.178\n",
            "[epoch= 33, iter= 1200] loss: 3.208\n",
            "[epoch= 33, iter= 1400] loss: 3.183\n",
            "[epoch= 34, iter=  200] loss: 3.157\n",
            "[epoch= 34, iter=  400] loss: 3.183\n",
            "[epoch= 34, iter=  600] loss: 3.193\n",
            "[epoch= 34, iter=  800] loss: 3.169\n",
            "[epoch= 34, iter= 1000] loss: 3.194\n",
            "[epoch= 34, iter= 1200] loss: 3.180\n",
            "[epoch= 34, iter= 1400] loss: 3.186\n",
            "[epoch= 35, iter=  200] loss: 3.156\n",
            "[epoch= 35, iter=  400] loss: 3.186\n",
            "[epoch= 35, iter=  600] loss: 3.172\n",
            "[epoch= 35, iter=  800] loss: 3.164\n",
            "[epoch= 35, iter= 1000] loss: 3.188\n",
            "[epoch= 35, iter= 1200] loss: 3.160\n",
            "[epoch= 35, iter= 1400] loss: 3.185\n",
            "[epoch= 36, iter=  200] loss: 3.145\n",
            "[epoch= 36, iter=  400] loss: 3.136\n",
            "[epoch= 36, iter=  600] loss: 3.139\n",
            "[epoch= 36, iter=  800] loss: 3.178\n",
            "[epoch= 36, iter= 1000] loss: 3.147\n",
            "[epoch= 36, iter= 1200] loss: 3.145\n",
            "[epoch= 36, iter= 1400] loss: 3.145\n",
            "[epoch= 37, iter=  200] loss: 3.127\n",
            "[epoch= 37, iter=  400] loss: 3.130\n",
            "[epoch= 37, iter=  600] loss: 3.118\n",
            "[epoch= 37, iter=  800] loss: 3.133\n",
            "[epoch= 37, iter= 1000] loss: 3.190\n",
            "[epoch= 37, iter= 1200] loss: 3.160\n",
            "[epoch= 37, iter= 1400] loss: 3.158\n",
            "[epoch= 38, iter=  200] loss: 3.105\n",
            "[epoch= 38, iter=  400] loss: 3.146\n",
            "[epoch= 38, iter=  600] loss: 3.150\n",
            "[epoch= 38, iter=  800] loss: 3.105\n",
            "[epoch= 38, iter= 1000] loss: 3.120\n",
            "[epoch= 38, iter= 1200] loss: 3.129\n",
            "[epoch= 38, iter= 1400] loss: 3.156\n",
            "[epoch= 39, iter=  200] loss: 3.094\n",
            "[epoch= 39, iter=  400] loss: 3.117\n",
            "[epoch= 39, iter=  600] loss: 3.108\n",
            "[epoch= 39, iter=  800] loss: 3.163\n",
            "[epoch= 39, iter= 1000] loss: 3.082\n",
            "[epoch= 39, iter= 1200] loss: 3.122\n",
            "[epoch= 39, iter= 1400] loss: 3.151\n",
            "[epoch= 40, iter=  200] loss: 3.087\n",
            "[epoch= 40, iter=  400] loss: 3.105\n",
            "[epoch= 40, iter=  600] loss: 3.090\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 实验 2: Tiny ImageNet 更宽的网络\n",
        "# 改动: 1. 网络各层通道数乘以1.5\n",
        "#       2. 适配TinyImageNet\n",
        "#       3. epoch设置为40\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "# (假设 TinyImageNetDataset 和 transformation 已定义)\n",
        "\n",
        "# --- 实验配置 ---\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_epochs = 40\n",
        "batch_size = 64\n",
        "num_workers = 2\n",
        "print_every = 200\n",
        "optim_name = \"Adam\"\n",
        "optim_kwargs = dict(lr=3e-4, weight_decay=1e-6)\n",
        "data_root = 'tiny-imagenet-200'\n",
        "\n",
        "# --- 数据加载器 (与基线相同) ---\n",
        "dataset, loader = {}, {}\n",
        "for data_type in (\"train\", \"val\"):\n",
        "    is_train = data_type == \"train\"\n",
        "    dataset[data_type] = TinyImageNetDataset(root=data_root, split=data_type, transform=transformation[data_type])\n",
        "    loader[data_type] = torch.utils.data.DataLoader(dataset[data_type], batch_size=batch_size, shuffle=is_train, num_workers=num_workers)\n",
        "\n",
        "# --- 网络架构 (核心修改) ---\n",
        "wm = 1.5 # 宽度乘数\n",
        "c1, c2, c3, c4 = int(128*wm), int(256*wm), int(512*wm), int(256*wm)\n",
        "l1, l2, l3, l4 = int(512*wm), int(256*wm), int(128*wm), 200\n",
        "\n",
        "net = nn.Sequential(\n",
        "    nn.Conv2d(3, c1, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),\n",
        "    nn.Conv2d(c1, c2, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),\n",
        "    nn.Conv2d(c2, c3, 3, padding=1), nn.ReLU(inplace=True),\n",
        "    nn.Conv2d(c3, c3, 3, padding=1), nn.ReLU(inplace=True),\n",
        "    nn.Conv2d(c3, c4, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(c4 * 8 * 8, l1), nn.ReLU(inplace=True), nn.Dropout(0.5), # 适配64x64输入\n",
        "    nn.Linear(l1, l2), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
        "    nn.Linear(l2, l3), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
        "    nn.Linear(l3, l4), # 适配200个分类\n",
        ").to(device)\n",
        "\n",
        "# --- 优化器与损失函数 ---\n",
        "optimizer = getattr(optim, optim_name)(net.parameters(), **optim_kwargs)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# --- 训练循环 ---\n",
        "print(\"--- 开始训练: 实验 2 (Tiny ImageNet 更宽网络) ---\")\n",
        "net.train()\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, (img, target) in enumerate(loader[\"train\"]):\n",
        "        img, target = img.to(device), target.to(device)\n",
        "        pred = net(img)\n",
        "        loss = criterion(pred, target)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if i % print_every == print_every - 1:\n",
        "            print(f\"[epoch={epoch + 1:3d}, iter={i + 1:5d}] loss: {running_loss / print_every:.3f}\")\n",
        "            running_loss = 0.0\n",
        "print(\"--- 训练结束 ---\")\n",
        "\n",
        "# --- 评估 ---\n",
        "net.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for img, target in loader[\"val\"]:\n",
        "        img, target = img.to(device), target.to(device)\n",
        "        pred = net(img)\n",
        "        total += len(target)\n",
        "        correct += (torch.argmax(pred, dim=1) == target).sum().item()\n",
        "print(f\"实验 2 (Tiny ImageNet 更宽网络) 准确率 @ 40 epochs: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "id": "U3IDsru_hpm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 实验 3: Tiny ImageNet AdamW优化器\n",
        "# 改动: 1. optim_name改为\"AdamW\", 调整weight_decay\n",
        "#       2. 适配TinyImageNet\n",
        "#       3. epoch设置为40\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "# (假设 TinyImageNetDataset 和 transformation 已定义)\n",
        "\n",
        "# --- 实验配置 (核心修改) ---\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_epochs = 40\n",
        "batch_size = 64\n",
        "num_workers = 2\n",
        "print_every = 200\n",
        "optim_name = \"AdamW\"\n",
        "optim_kwargs = dict(lr=3e-4, weight_decay=1e-4) # AdamW使用更典型的weight_decay\n",
        "data_root = 'tiny-imagenet-200'\n",
        "\n",
        "# --- 数据加载器 (与基线相同) ---\n",
        "dataset, loader = {}, {}\n",
        "for data_type in (\"train\", \"val\"):\n",
        "    is_train = data_type == \"train\"\n",
        "    dataset[data_type] = TinyImageNetDataset(root=data_root, split=data_type, transform=transformation[data_type])\n",
        "    loader[data_type] = torch.utils.data.DataLoader(dataset[data_type], batch_size=batch_size, shuffle=is_train, num_workers=num_workers)\n",
        "\n",
        "# --- 网络架构 (与基线相同) ---\n",
        "net = nn.Sequential(\n",
        "    nn.Conv2d(3, 128, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),\n",
        "    nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),\n",
        "    nn.Conv2d(256, 512, 3, padding=1), nn.ReLU(inplace=True),\n",
        "    nn.Conv2d(512, 512, 3, padding=1), nn.ReLU(inplace=True),\n",
        "    nn.Conv2d(512, 256, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(256 * 8 * 8, 512), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
        "    nn.Linear(512, 256), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
        "    nn.Linear(256, 128), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
        "    nn.Linear(128, 200),\n",
        ").to(device)\n",
        "\n",
        "# --- 优化器与损失函数 ---\n",
        "optimizer = getattr(optim, optim_name)(net.parameters(), **optim_kwargs)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# --- 训练循环 ---\n",
        "print(\"--- 开始训练: 实验 3 (Tiny ImageNet AdamW优化器) ---\")\n",
        "net.train()\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, (img, target) in enumerate(loader[\"train\"]):\n",
        "        img, target = img.to(device), target.to(device)\n",
        "        pred = net(img)\n",
        "        loss = criterion(pred, target)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if i % print_every == print_every - 1:\n",
        "            print(f\"[epoch={epoch + 1:3d}, iter={i + 1:5d}] loss: {running_loss / print_every:.3f}\")\n",
        "            running_loss = 0.0\n",
        "print(\"--- 训练结束 ---\")\n",
        "\n",
        "# --- 评估 ---\n",
        "net.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for img, target in loader[\"val\"]:\n",
        "        img, target = img.to(device), target.to(device)\n",
        "        pred = net(img)\n",
        "        total += len(target)\n",
        "        correct += (torch.argmax(pred, dim=1) == target).sum().item()\n",
        "print(f\"实验 3 (Tiny ImageNet AdamW优化器) 准确率 @ 40 epochs: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "id": "aNJAY7S2iOHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 实验 4: Tiny ImageNet 更丰富的数据增强\n",
        "# 改动: 1. 在数据增强中加入ColorJitter\n",
        "#       2. 适配TinyImageNet\n",
        "#       3. epoch设置为40\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "# (假设 TinyImageNetDataset 和 TINY_IMAGENET... 已定义)\n",
        "\n",
        "# --- 实验配置 ---\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_epochs = 40\n",
        "batch_size = 64\n",
        "num_workers = 2\n",
        "print_every = 200\n",
        "optim_name = \"Adam\"\n",
        "optim_kwargs = dict(lr=3e-4, weight_decay=1e-6)\n",
        "data_root = 'tiny-imagenet-200'\n",
        "\n",
        "# --- 数据预处理 (核心修改) ---\n",
        "transformation_augmented = dict()\n",
        "for data_type in (\"train\", \"val\"):\n",
        "    is_train = data_type == \"train\"\n",
        "    transformation_augmented[data_type] = tv_transforms.Compose(([\n",
        "        tv_transforms.RandomRotation(degrees=15),\n",
        "        tv_transforms.RandomHorizontalFlip(),\n",
        "        tv_transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "        tv_transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1), # 新增项\n",
        "    ] if is_train else []) +\n",
        "    [\n",
        "        tv_transforms.ToTensor(),\n",
        "        tv_transforms.Normalize(mean=TINY_IMAGENET_MEAN, std=TINY_IMAGENET_STD),\n",
        "    ])\n",
        "\n",
        "# --- 数据加载器 ---\n",
        "dataset, loader = {}, {}\n",
        "for data_type in (\"train\", \"val\"):\n",
        "    is_train = data_type == \"train\"\n",
        "    dataset[data_type] = TinyImageNetDataset(root=data_root, split=data_type, transform=transformation_augmented[data_type])\n",
        "    loader[data_type] = torch.utils.data.DataLoader(dataset[data_type], batch_size=batch_size, shuffle=is_train, num_workers=num_workers)\n",
        "\n",
        "# --- 网络架构 (与基线相同) ---\n",
        "net = nn.Sequential(\n",
        "    nn.Conv2d(3, 128, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),\n",
        "    nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),\n",
        "    nn.Conv2d(256, 512, 3, padding=1), nn.ReLU(inplace=True),\n",
        "    nn.Conv2d(512, 512, 3, padding=1), nn.ReLU(inplace=True),\n",
        "    nn.Conv2d(512, 256, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(256 * 8 * 8, 512), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
        "    nn.Linear(512, 256), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
        "    nn.Linear(256, 128), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
        "    nn.Linear(128, 200),\n",
        ").to(device)\n",
        "\n",
        "# --- 优化器与损失函数 ---\n",
        "optimizer = getattr(optim, optim_name)(net.parameters(), **optim_kwargs)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# --- 训练循环 ---\n",
        "print(\"--- 开始训练: 实验 4 (Tiny ImageNet 更丰富的数据增强) ---\")\n",
        "net.train()\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, (img, target) in enumerate(loader[\"train\"]):\n",
        "        img, target = img.to(device), target.to(device)\n",
        "        pred = net(img)\n",
        "        loss = criterion(pred, target)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if i % print_every == print_every - 1:\n",
        "            print(f\"[epoch={epoch + 1:3d}, iter={i + 1:5d}] loss: {running_loss / print_every:.3f}\")\n",
        "            running_loss = 0.0\n",
        "print(\"--- 训练结束 ---\")\n",
        "\n",
        "# --- 评估 ---\n",
        "net.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for img, target in loader[\"val\"]:\n",
        "        img, target = img.to(device), target.to(device)\n",
        "        pred = net(img)\n",
        "        total += len(target)\n",
        "        correct += (torch.argmax(pred, dim=1) == target).sum().item()\n",
        "print(f\"实验 4 (Tiny ImageNet 更丰富的数据增强) 准确率 @ 40 epochs: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "id": "X-zSkh47iQi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 实验 5: Tiny ImageNet 注意力机制\n",
        "# 改动: 1. 网络重写为带SE-Block的结构\n",
        "#       2. 适配TinyImageNet\n",
        "#       3. epoch设置为40\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "# (假设 TinyImageNetDataset 和 transformation 已定义)\n",
        "\n",
        "# --- 实验配置 ---\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_epochs = 40\n",
        "batch_size = 64\n",
        "num_workers = 2\n",
        "print_every = 200\n",
        "optim_name = \"Adam\"\n",
        "optim_kwargs = dict(lr=3e-4, weight_decay=1e-6)\n",
        "data_root = 'tiny-imagenet-200'\n",
        "\n",
        "# --- 数据加载器 (与基线相同) ---\n",
        "dataset, loader = {}, {}\n",
        "for data_type in (\"train\", \"val\"):\n",
        "    is_train = data_type == \"train\"\n",
        "    dataset[data_type] = TinyImageNetDataset(root=data_root, split=data_type, transform=transformation[data_type])\n",
        "    loader[data_type] = torch.utils.data.DataLoader(dataset[data_type], batch_size=batch_size, shuffle=is_train, num_workers=num_workers)\n",
        "\n",
        "# --- 网络架构 (核心修改) ---\n",
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, channel, reduction=16):\n",
        "        super().__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channel, channel // reduction, bias=False), nn.ReLU(inplace=True),\n",
        "            nn.Linear(channel // reduction, channel, bias=False), nn.Sigmoid())\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y.expand_as(x)\n",
        "\n",
        "class NetWithAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv_block1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 128, 3, padding=1), nn.ReLU(inplace=True),\n",
        "            SEBlock(128),\n",
        "            nn.MaxPool2d(2), nn.Dropout(0.3)\n",
        "        )\n",
        "        self.conv_block2 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(inplace=True),\n",
        "            SEBlock(256),\n",
        "            nn.MaxPool2d(2), nn.Dropout(0.3)\n",
        "        )\n",
        "        self.conv_block3 = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, 3, padding=1), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, 3, padding=1), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 256, 3, padding=1), nn.ReLU(inplace=True),\n",
        "            SEBlock(256),\n",
        "            nn.MaxPool2d(2), nn.Dropout(0.3)\n",
        "        )\n",
        "        self.fc_block = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256 * 8 * 8, 512), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
        "            nn.Linear(512, 256), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
        "            nn.Linear(256, 128), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
        "            nn.Linear(128, 200),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.conv_block1(x)\n",
        "        x = self.conv_block2(x)\n",
        "        x = self.conv_block3(x)\n",
        "        x = self.fc_block(x)\n",
        "        return x\n",
        "net = NetWithAttention().to(device)\n",
        "\n",
        "# --- 优化器与损失函数 ---\n",
        "optimizer = getattr(optim, optim_name)(net.parameters(), **optim_kwargs)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# --- 训练循环 ---\n",
        "print(\"--- 开始训练: 实验 5 (Tiny ImageNet 注意力机制) ---\")\n",
        "net.train()\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, (img, target) in enumerate(loader[\"train\"]):\n",
        "        img, target = img.to(device), target.to(device)\n",
        "        pred = net(img)\n",
        "        loss = criterion(pred, target)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if i % print_every == print_every - 1:\n",
        "            print(f\"[epoch={epoch + 1:3d}, iter={i + 1:5d}] loss: {running_loss / print_every:.3f}\")\n",
        "            running_loss = 0.0\n",
        "print(\"--- 训练结束 ---\")\n",
        "\n",
        "# --- 评估 ---\n",
        "net.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for img, target in loader[\"val\"]:\n",
        "        img, target = img.to(device), target.to(device)\n",
        "        pred = net(img)\n",
        "        total += len(target)\n",
        "        correct += (torch.argmax(pred, dim=1) == target).sum().item()\n",
        "print(f\"实验 5 (Tiny ImageNet 注意力机制) 准确率 @ 40 epochs: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "id": "hrxaRHK4iScd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}