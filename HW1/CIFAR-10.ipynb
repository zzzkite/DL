{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 环境与随机种子（确保可复现）\n",
    "import os, sys, random, time, platform, json\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "SEED = int(os.environ.get(\"SEED\", 42))\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# cuDNN 可复现设置\n",
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = False\n",
    "cudnn.deterministic = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print({\n",
    "    \"python\": sys.version.split(\" \")[0],\n",
    "    \"platform\": platform.platform(),\n",
    "    \"pytorch\": torch.__version__,\n",
    "    \"cuda_available\": torch.cuda.is_available(),\n",
    "    \"cuda_version\": torch.version.cuda if torch.cuda.is_available() else None,\n",
    "    \"device\": str(device),\n",
    "})\n",
    "print(\"SEED=\", SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 复现实验环境与运行说明\n",
    "\n",
    "本 Notebook 使用 PyTorch>=2 进行 CIFAR-10 图像分类实验。为提高复现性，我们在最前面固定随机种子、打印环境信息，并给出关键开关说明：\n",
    "\n",
    "- 随机种子：seed 固定，cuDNN 设为 deterministic。\n",
    "- 设备选择：自动选择 CUDA/GPU 或 CPU。\n",
    "- 运行产物：所有模型、图像与 CSV 会保存到统一的 RESULTS_DIR 下。\n",
    "\n",
    "在训练前后，可参考末尾的“结果表格与总结”与“我学到了什么”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "This code baseline is inspired by and modified from [this great tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html).\n",
    "\n",
    "This code can achieve an accuracy of approximately 86.50% on CIFAR-10. Please set up the environment and run your experiments starting from this baseline. You are expected to achieve an accuracy higher than this baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "# torch: PyTorch 的核心张量与自动求导库\n",
    "import torch\n",
    "# nn: 神经网络层、损失函数等模块\n",
    "import torch.nn as nn\n",
    "# optim: 各类优化器（SGD/Adam 等）\n",
    "import torch.optim as optim\n",
    "\n",
    "# torchvision: 计算机视觉常用数据集与图像增广\n",
    "# tv_datasets: 常见视觉数据集（如 CIFAR-10）\n",
    "import torchvision.datasets as tv_datasets\n",
    "# tv_transforms: 图像预处理/数据增强流水线\n",
    "import torchvision.transforms as tv_transforms\n",
    "\n",
    "# 额外工具\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# 固定随机种子，保证可复现\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "GPU name: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# 实验参数与运行设备设置\n",
    "# 优先使用 CUDA 的第 0 块 GPU；若不可用则回退到 CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 设备日志：确认是否成功使用 GPU\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    try:\n",
    "        print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    except Exception as e:\n",
    "        # 某些环境下可能无法读取设备名称\n",
    "        print(f\"CUDA available but failed to get device name: {e}\")\n",
    "\n",
    "# 训练超参数\n",
    "num_epochs = 128          # 训练轮数\n",
    "batch_size = 64           # 每个 mini-batch 的样本数量\n",
    "num_workers = 2           # DataLoader 载入数据的线程数（Windows 上建议适度）\n",
    "print_every = 200         # 每多少个 iteration 打印一次训练损失\n",
    "\n",
    "# 优化器配置\n",
    "optim_name = \"Adam\"       # 优化器名称（支持 'SGD'、'Adam' 等）\n",
    "optim_kwargs = dict(\n",
    "    lr=3e-4,              # 学习率\n",
    "    weight_decay=1e-6,    # L2 正则（权重衰减）\n",
    ")\n",
    "\n",
    "# 输入图像的预处理/数据增强流水线\n",
    "# 训练集与测试集使用相同的标准化，但训练集额外加入随机增广提升泛化\n",
    "transformation = dict()\n",
    "for data_type in (\"train\", \"test\"):\n",
    "    is_train = data_type==\"train\"\n",
    "    # Compose 将一系列变换按顺序组合\n",
    "    transformation[data_type] = tv_transforms.Compose(([ \n",
    "        # 仅训练时使用的数据增强\n",
    "        tv_transforms.RandomRotation(degrees=15),                 # 随机旋转\n",
    "        tv_transforms.RandomHorizontalFlip(),                     # 随机水平翻转\n",
    "        tv_transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)), # 随机平移\n",
    "    ] if is_train else []) + \n",
    "    [\n",
    "        tv_transforms.ToTensor(),                                 # 转为张量并缩放到 [0,1]\n",
    "        # 以 0.5 为均值、0.5 为标准差做标准化: (x - 0.5) / 0.5 -> 约等于缩放到 [-1,1]\n",
    "        tv_transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 结果目录与路径常量（统一保存产物）\n",
    "import time, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "RUN_TAG = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "RESULTS_ROOT = \"./results\"\n",
    "RESULTS_DIR = os.path.join(RESULTS_ROOT, f\"cifar10_{RUN_TAG}\")\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "print(f\"Results will be saved to: {RESULTS_DIR}\")\n",
    "\n",
    "# 统一产物路径\n",
    "BEST_MODEL_PATH = os.path.join(RESULTS_DIR, \"best_model_cifar10.pth\")\n",
    "TRAIN_CURVES_PNG = os.path.join(RESULTS_DIR, \"cifar10_training_curves.png\")\n",
    "CM_PNG = os.path.join(RESULTS_DIR, \"cifar10_confusion_matrix.png\")\n",
    "ABLATION_CSV = os.path.join(RESULTS_DIR, \"cifar10_ablation_results.csv\")\n",
    "\n",
    "# 当前 Notebook 的绝对路径（用于归档）\n",
    "NOTEBOOK_ABS_PATH = \"/data/zhangzhikui/githubbase/DL/HW1/CIFAR-10.ipynb\"\n",
    "NOTEBOOK_COPY_PATH = os.path.join(RESULTS_DIR, f\"CIFAR-10_{RUN_TAG}.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [01:07<00:00, 2.52MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# 准备 CIFAR-10 数据集与数据加载器\n",
    "dataset, loader = {}, {}\n",
    "for data_type in (\"train\", \"test\"):\n",
    "    is_train = data_type==\"train\"\n",
    "    # CIFAR-10: 50000 张训练图像 + 10000 张测试图像，类别数为 10\n",
    "    dataset[data_type] = tv_datasets.CIFAR10(\n",
    "        root=\"./data\",                 # 数据集存放路径（如不存在将自动创建）\n",
    "        train=is_train,                # 训练/测试划分\n",
    "        download=True,                 # 若本地无数据则联网下载\n",
    "        transform=transformation[data_type],  # 应用上面定义的预处理\n",
    "    )\n",
    "    # DataLoader 负责按批次提供数据并在训练集上打乱顺序\n",
    "    loader[data_type] = torch.utils.data.DataLoader(\n",
    "        dataset[data_type],\n",
    "        batch_size=batch_size,\n",
    "        shuffle=is_train,              # 仅在训练集上打乱，测试集保持顺序\n",
    "        num_workers=num_workers,       # 加载线程数\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 7.28M\n"
     ]
    }
   ],
   "source": [
    "# 定义卷积神经网络（CNN）结构\n",
    "net = nn.Sequential(\n",
    "    # 下采样阶段 1: 输入 3x32x32 -> 中间特征\n",
    "    nn.Conv2d(3, 128, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),\n",
    "    # 下采样阶段 2\n",
    "    nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),\n",
    "    # 更深的卷积特征提取\n",
    "    nn.Conv2d(256, 512, 3, padding=1), nn.ReLU(inplace=True),\n",
    "    nn.Conv2d(512, 512, 3, padding=1), nn.ReLU(inplace=True),\n",
    "    # 压缩通道数并再次下采样\n",
    "    nn.Conv2d(512, 256, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),\n",
    "    # 展平为全连接层输入\n",
    "    nn.Flatten(),\n",
    "    # 全连接分类头，含 Dropout 做正则化\n",
    "    nn.Linear(256 * 4 * 4, 512), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
    "    nn.Linear(512, 256), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
    "    nn.Linear(256, 128), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
    "    nn.Linear(128, 10),  # 10 个类别的 logits\n",
    ")\n",
    "\n",
    "# 将模型移动到指定设备（GPU/CPU）\n",
    "net.to(device)\n",
    "\n",
    "# 统计可训练参数量（单位：百万）\n",
    "print(f\"number of parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad) / 1_000_000:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch=  1, iter=  200] loss: 2.206\n",
      "[epoch=  1, iter=  400] loss: 1.977\n",
      "[epoch=  1, iter=  600] loss: 1.910\n",
      "[epoch=  2, iter=  200] loss: 1.705\n",
      "[epoch=  2, iter=  400] loss: 1.628\n",
      "[epoch=  2, iter=  600] loss: 1.537\n",
      "[epoch=  3, iter=  200] loss: 1.426\n",
      "[epoch=  3, iter=  400] loss: 1.395\n",
      "[epoch=  3, iter=  600] loss: 1.379\n",
      "[epoch=  4, iter=  200] loss: 1.277\n",
      "[epoch=  4, iter=  400] loss: 1.253\n",
      "[epoch=  4, iter=  600] loss: 1.228\n"
     ]
    }
   ],
   "source": [
    "# 定义网络优化器（从 torch.optim 动态获取指定优化器）\n",
    "optimizer = getattr(optim, optim_name)(net.parameters(), **optim_kwargs)\n",
    "\n",
    "# 多分类交叉熵损失，适用于单标签多分类任务\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 训练循环\n",
    "net.train()  # 切换到训练模式（启用 Dropout/BN 的训练行为）\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, (img, target) in enumerate(loader[\"train\"]):\n",
    "        # 将数据移动到相同设备\n",
    "        img, target = img.to(device), target.to(device)\n",
    "\n",
    "        # 前向计算得到 logits 预测\n",
    "        pred = net(img)\n",
    "        loss = criterion(pred, target)\n",
    "\n",
    "        # 反向传播与参数更新\n",
    "        optimizer.zero_grad()  # 清空上一轮梯度\n",
    "        loss.backward()        # 计算当前梯度\n",
    "        optimizer.step()       # 按优化器策略更新参数\n",
    "\n",
    "        # 统计与日志打印\n",
    "        running_loss += loss.item()\n",
    "        if i % print_every == print_every - 1:\n",
    "            print(f\"[epoch={epoch + 1:3d}, iter={i + 1:5d}] loss: {running_loss / print_every:.3f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating its accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 切换到评估模式（关闭 Dropout/固定 BN 统计）\n",
    "net.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():  # 评估时不需要梯度，降低显存/加速\n",
    "    for img, target in loader[\"test\"]:\n",
    "        img, target = img.to(device), target.to(device)\n",
    "        \n",
    "        # 前向推理，得到每类的 logits\n",
    "        pred = net(img)\n",
    "        \n",
    "        # 累积统计：top-1 预测与真实标签比较\n",
    "        total += len(target)\n",
    "        correct += (torch.argmax(pred, dim=1) == target).sum().item()\n",
    "\n",
    "print(f\"Accuracy of the network on the {total} test images: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验改进部分\n",
    "\n",
    "在基准模型达到约 86.5% 准确率后，我们将通过以下因素逐步改进性能：\n",
    "\n",
    "## 改进策略概览\n",
    "\n",
    "1. **残差连接 (Residual Connections)**: 缓解深度网络的梯度消失问题\n",
    "2. **网络深度与宽度**: 增加模型容量\n",
    "3. **优化器改进**: 学习率调度、warmup、AdamW\n",
    "4. **数据增强**: Cutout、MixUp、RandomErasing\n",
    "5. **注意力机制**: SE (Squeeze-and-Excitation) 模块\n",
    "6. **正则化技术**: Label Smoothing、Stochastic Depth\n",
    "\n",
    "每个改进因素都将独立测试并记录结果，最终组合最优配置。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 改进 1: 残差块 (Residual Block)\n",
    "\n",
    "残差连接通过跳跃连接 (skip connection) 让梯度能直接传播，缓解深层网络的梯度消失问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义基础残差块\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    残差块：F(x) + x\n",
    "    - 两层 3x3 卷积\n",
    "    - BatchNorm 用于稳定训练\n",
    "    - 若输入输出通道不匹配，用 1x1 卷积调整维度\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        # 主路径：conv -> bn -> relu -> conv -> bn\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # 残差路径（shortcut）：若维度不匹配则用 1x1 卷积投影\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 主路径\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        # 残差连接：out = F(x) + x\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 改进 2: 注意力机制 (SE - Squeeze-and-Excitation)\n",
    "\n",
    "通过学习通道权重，让网络关注更重要的特征通道。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SE (Squeeze-and-Excitation) 注意力模块\n",
    "class SEBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    通道注意力模块：\n",
    "    1. Squeeze: 全局平均池化，得到每个通道的全局特征\n",
    "    2. Excitation: 两层全连接学习通道权重\n",
    "    3. Scale: 用学习到的权重重新标定各通道\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        # Squeeze: 自适应全局平均池化 (H, W) -> (1, 1)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        # Excitation: 两层全连接 + 激活\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels // reduction, channels, bias=False),\n",
    "            nn.Sigmoid()  # 输出 [0, 1] 范围的权重\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        # Squeeze: (B, C, H, W) -> (B, C, 1, 1) -> (B, C)\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        # Excitation: (B, C) -> (B, C)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        # Scale: 按通道加权\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "\n",
    "# 带 SE 模块的残差块\n",
    "class SEResidualBlock(nn.Module):\n",
    "    \"\"\"残差块 + SE 注意力\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride=1, reduction=16):\n",
    "        super(SEResidualBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # SE 模块\n",
    "        self.se = SEBlock(out_channels, reduction)\n",
    "        \n",
    "        # shortcut\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        # 应用 SE 注意力\n",
    "        out = self.se(out)\n",
    "        \n",
    "        # 残差连接\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 改进 3: 数据增强 (Advanced Data Augmentation)\n",
    "\n",
    "增强训练数据的多样性，提高泛化能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cutout 数据增强：随机遮挡图像的矩形区域\n",
    "class Cutout:\n",
    "    \"\"\"\n",
    "    随机在图像上遮挡一个正方形区域，迫使模型学习局部特征\n",
    "    \"\"\"\n",
    "    def __init__(self, n_holes=1, length=16):\n",
    "        \"\"\"\n",
    "        n_holes: 遮挡区域数量\n",
    "        length: 每个遮挡区域的边长\n",
    "        \"\"\"\n",
    "        self.n_holes = n_holes\n",
    "        self.length = length\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        img: Tensor of shape (C, H, W)\n",
    "        \"\"\"\n",
    "        h = img.size(1)\n",
    "        w = img.size(2)\n",
    "\n",
    "        mask = torch.ones((h, w), dtype=torch.float32)\n",
    "\n",
    "        for _ in range(self.n_holes):\n",
    "            # 随机选择遮挡中心\n",
    "            y = torch.randint(h, (1,)).item()\n",
    "            x = torch.randint(w, (1,)).item()\n",
    "\n",
    "            # 计算遮挡区域边界\n",
    "            y1 = max(0, y - self.length // 2)\n",
    "            y2 = min(h, y + self.length // 2)\n",
    "            x1 = max(0, x - self.length // 2)\n",
    "            x2 = min(w, x + self.length // 2)\n",
    "\n",
    "            mask[y1:y2, x1:x2] = 0.\n",
    "\n",
    "        mask = mask.expand_as(img)\n",
    "        img = img * mask\n",
    "\n",
    "        return img\n",
    "\n",
    "\n",
    "# MixUp 数据增强：混合两个样本\n",
    "def mixup_data(x, y, alpha=1.0, device='cuda'):\n",
    "    \"\"\"\n",
    "    对一个 batch 进行 MixUp 增强\n",
    "    x: 输入图像 (B, C, H, W)\n",
    "    y: 标签 (B,)\n",
    "    alpha: Beta 分布参数，控制混合程度\n",
    "    \n",
    "    返回：混合后的图像、标签1、标签2、混合系数\n",
    "    \"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = torch.distributions.Beta(alpha, alpha).sample().item()\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size(0)\n",
    "    # 随机打乱索引\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "\n",
    "    # 混合图像: x_mix = λ * x_i + (1-λ) * x_j\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    \n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "\n",
    "# MixUp 损失函数\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    \"\"\"\n",
    "    计算 MixUp 的混合损失\n",
    "    loss = λ * loss(pred, y_a) + (1-λ) * loss(pred, y_b)\n",
    "    \"\"\"\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 改进的数据增强流水线\n",
    "transformation_improved = dict()\n",
    "\n",
    "# 训练集：更强的数据增强\n",
    "transformation_improved[\"train\"] = tv_transforms.Compose([\n",
    "    tv_transforms.RandomCrop(32, padding=4),           # 随机裁剪\n",
    "    tv_transforms.RandomHorizontalFlip(),              # 随机水平翻转\n",
    "    tv_transforms.RandomRotation(15),                  # 随机旋转\n",
    "    tv_transforms.ColorJitter(                         # 颜色抖动\n",
    "        brightness=0.2, \n",
    "        contrast=0.2, \n",
    "        saturation=0.2, \n",
    "        hue=0.1\n",
    "    ),\n",
    "    tv_transforms.ToTensor(),\n",
    "    tv_transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],   # CIFAR-10 统计均值\n",
    "                           std=[0.2023, 0.1994, 0.2010]),    # CIFAR-10 统计标准差\n",
    "    tv_transforms.RandomErasing(p=0.5, scale=(0.02, 0.33)),  # 随机擦除\n",
    "    Cutout(n_holes=1, length=16),                            # Cutout\n",
    "])\n",
    "\n",
    "# 测试集：仅标准化\n",
    "transformation_improved[\"test\"] = tv_transforms.Compose([\n",
    "    tv_transforms.ToTensor(),\n",
    "    tv_transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
    "                           std=[0.2023, 0.1994, 0.2010]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 改进 4 & 5: 构建改进的网络 (更深、更宽 + 残差 + SE注意力)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 完整改进网络：结合残差、SE注意力、更深更宽的结构\n",
    "class ImprovedCIFARNet(nn.Module):\n",
    "    \"\"\"\n",
    "    改进的 CIFAR-10 分类网络\n",
    "    - 使用残差块 + SE 注意力\n",
    "    - 更深的网络结构（多个残差块堆叠）\n",
    "    - 更宽的通道数\n",
    "    - 使用 BatchNorm 稳定训练\n",
    "    - Dropout 做正则化\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10, use_se=True):\n",
    "        super(ImprovedCIFARNet, self).__init__()\n",
    "        \n",
    "        # 初始卷积：增加通道数\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # 残差块组 1: 64 -> 128，2个残差块\n",
    "        block_type = SEResidualBlock if use_se else ResidualBlock\n",
    "        self.layer1 = self._make_layer(block_type, 64, 128, num_blocks=2, stride=1)\n",
    "        \n",
    "        # 残差块组 2: 128 -> 256，2个残差块，stride=2 下采样\n",
    "        self.layer2 = self._make_layer(block_type, 128, 256, num_blocks=2, stride=2)\n",
    "        \n",
    "        # 残差块组 3: 256 -> 512，3个残差块，stride=2 下采样\n",
    "        self.layer3 = self._make_layer(block_type, 256, 512, num_blocks=3, stride=2)\n",
    "        \n",
    "        # 全局平均池化 + 分类头\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "        \n",
    "        # 权重初始化\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _make_layer(self, block_type, in_channels, out_channels, num_blocks, stride):\n",
    "        \"\"\"构建残差块组\"\"\"\n",
    "        layers = []\n",
    "        # 第一个块可能需要下采样\n",
    "        layers.append(block_type(in_channels, out_channels, stride))\n",
    "        # 后续块维度保持不变\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(block_type(out_channels, out_channels, stride=1))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"He 初始化，适合 ReLU\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 初始卷积\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # 残差块组\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        \n",
    "        # 全局池化 + 分类\n",
    "        x = self.avg_pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# 实例化改进模型\n",
    "net_improved = ImprovedCIFARNet(num_classes=10, use_se=True).to(device)\n",
    "\n",
    "# 打印参数量\n",
    "total_params = sum(p.numel() for p in net_improved.parameters() if p.requires_grad)\n",
    "print(f\"改进模型参数量: {total_params / 1_000_000:.2f}M\")\n",
    "print(f\"模型结构:\\n{net_improved}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 改进 6: 优化器与学习率调度\n",
    "\n",
    "使用 AdamW + Cosine Annealing 学习率调度 + Warmup，提升训练稳定性和最终性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Smoothing 损失函数\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    \"\"\"\n",
    "    标签平滑：将硬标签 [0, 0, 1, 0] 平滑为 [ε/K, ε/K, 1-ε+ε/K, ε/K]\n",
    "    防止模型过度自信，提高泛化能力\n",
    "    \"\"\"\n",
    "    def __init__(self, epsilon=0.1):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        n_classes = pred.size(-1)\n",
    "        log_preds = torch.nn.functional.log_softmax(pred, dim=-1)\n",
    "        \n",
    "        # 平滑损失 = (1-ε) * CE + ε * 均匀分布\n",
    "        loss = -log_preds.sum(dim=-1).mean() * self.epsilon / n_classes\n",
    "        nll = torch.nn.functional.nll_loss(log_preds, target, reduction='mean')\n",
    "        \n",
    "        return (1 - self.epsilon) * nll + loss\n",
    "\n",
    "\n",
    "# 改进的训练配置\n",
    "config_improved = {\n",
    "    'num_epochs': 200,\n",
    "    'batch_size': 128,          # 增大 batch size 提高训练效率\n",
    "    'num_workers': 4,           # 增加数据加载线程\n",
    "    'print_every': 100,\n",
    "    \n",
    "    # 优化器：AdamW (带权重衰减的 Adam)\n",
    "    'optimizer': 'AdamW',\n",
    "    'lr': 1e-3,                 # 初始学习率\n",
    "    'weight_decay': 5e-4,       # 权重衰减\n",
    "    \n",
    "    # 学习率调度\n",
    "    'warmup_epochs': 5,         # warmup 轮数\n",
    "    'lr_scheduler': 'cosine',   # cosine annealing\n",
    "    \n",
    "    # 正则化\n",
    "    'label_smoothing': 0.1,     # 标签平滑\n",
    "    'mixup_alpha': 0.2,         # MixUp 参数（0 表示禁用）\n",
    "}\n",
    "\n",
    "print(\"改进配置:\", config_improved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 完整训练流程（包含所有改进）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备改进的数据加载器\n",
    "dataset_improved, loader_improved = {}, {}\n",
    "\n",
    "for data_type in (\"train\", \"test\"):\n",
    "    is_train = data_type == \"train\"\n",
    "    dataset_improved[data_type] = tv_datasets.CIFAR10(\n",
    "        root=\"./data\",\n",
    "        train=is_train,\n",
    "        download=True,\n",
    "        transform=transformation_improved[data_type],\n",
    "    )\n",
    "    loader_improved[data_type] = torch.utils.data.DataLoader(\n",
    "        dataset_improved[data_type],\n",
    "        batch_size=config_improved['batch_size'],\n",
    "        shuffle=is_train,\n",
    "        num_workers=config_improved['num_workers'],\n",
    "        pin_memory=True,  # 加速数据传输到 GPU\n",
    "    )\n",
    "\n",
    "print(f\"训练集大小: {len(dataset_improved['train'])}\")\n",
    "print(f\"测试集大小: {len(dataset_improved['test'])}\")\n",
    "\n",
    "# 冒烟测试：取一个 batch 做前向，检查维度是否匹配\n",
    "try:\n",
    "    imgs, lbls = next(iter(loader_improved['train']))\n",
    "    imgs = imgs.to(device)\n",
    "    with torch.no_grad():\n",
    "        _ = net_improved(imgs[:4])\n",
    "    print(\"前向冒烟测试通过（net_improved）\")\n",
    "except Exception as e:\n",
    "    print(f\"前向冒烟测试失败: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练与评估函数\n",
    "def train_epoch(model, loader, criterion, optimizer, device, use_mixup=False, mixup_alpha=0.0):\n",
    "    \"\"\"\n",
    "    训练一个 epoch\n",
    "    返回：平均损失、准确率\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for img, target in loader:\n",
    "        img, target = img.to(device), target.to(device)\n",
    "        \n",
    "        # 使用 MixUp\n",
    "        if use_mixup and mixup_alpha > 0:\n",
    "            img, target_a, target_b, lam = mixup_data(img, target, mixup_alpha, device)\n",
    "            pred = model(img)\n",
    "            loss = mixup_criterion(criterion, pred, target_a, target_b, lam)\n",
    "        else:\n",
    "            pred = model(img)\n",
    "            loss = criterion(pred, target)\n",
    "        \n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 统计\n",
    "        running_loss += loss.item() * img.size(0)\n",
    "        _, predicted = pred.max(1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target).sum().item()\n",
    "    \n",
    "    avg_loss = running_loss / total\n",
    "    acc = 100. * correct / total\n",
    "    return avg_loss, acc\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    \"\"\"\n",
    "    在验证/测试集上评估\n",
    "    返回：平均损失、准确率\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img, target in loader:\n",
    "            img, target = img.to(device), target.to(device)\n",
    "            \n",
    "            pred = model(img)\n",
    "            loss = criterion(pred, target)\n",
    "            \n",
    "            running_loss += loss.item() * img.size(0)\n",
    "            _, predicted = pred.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "    \n",
    "    avg_loss = running_loss / total\n",
    "    acc = 100. * correct / total\n",
    "    return avg_loss, acc\n",
    "\n",
    "\n",
    "# Warmup 学习率调度器\n",
    "class WarmupCosineSchedule:\n",
    "    \"\"\"\n",
    "    Warmup + Cosine Annealing 学习率调度\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, warmup_epochs, total_epochs, lr_min=1e-6):\n",
    "        self.optimizer = optimizer\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.total_epochs = total_epochs\n",
    "        self.lr_min = lr_min\n",
    "        self.base_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    def step(self, epoch):\n",
    "        if epoch < self.warmup_epochs:\n",
    "            # Warmup: 线性增长\n",
    "            lr = self.base_lr * (epoch + 1) / self.warmup_epochs\n",
    "        else:\n",
    "            # Cosine Annealing\n",
    "            progress = (epoch - self.warmup_epochs) / (self.total_epochs - self.warmup_epochs)\n",
    "            lr = self.lr_min + (self.base_lr - self.lr_min) * 0.5 * (1 + torch.cos(torch.tensor(progress * 3.14159265)))\n",
    "            lr = lr.item()\n",
    "        \n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        \n",
    "        return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化优化器、损失函数、学习率调度器\n",
    "optimizer_improved = optim.AdamW(\n",
    "    net_improved.parameters(),\n",
    "    lr=config_improved['lr'],\n",
    "    weight_decay=config_improved['weight_decay']\n",
    ")\n",
    "\n",
    "criterion_improved = LabelSmoothingCrossEntropy(epsilon=config_improved['label_smoothing'])\n",
    "\n",
    "scheduler = WarmupCosineSchedule(\n",
    "    optimizer_improved,\n",
    "    warmup_epochs=config_improved['warmup_epochs'],\n",
    "    total_epochs=config_improved['num_epochs']\n",
    ")\n",
    "\n",
    "print(\"优化器、损失函数、学习率调度器已初始化\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 完整训练循环\n",
    "import time\n",
    "\n",
    "# 记录训练历史\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'test_loss': [],\n",
    "    'test_acc': [],\n",
    "    'lr': []\n",
    "}\n",
    "\n",
    "best_acc = 0\n",
    "best_epoch = 0\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"开始训练改进模型\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(config_improved['num_epochs']):\n",
    "    # 调整学习率\n",
    "    current_lr = scheduler.step(epoch)\n",
    "    history['lr'].append(current_lr)\n",
    "    \n",
    "    # 训练一个 epoch\n",
    "    train_loss, train_acc = train_epoch(\n",
    "        net_improved,\n",
    "        loader_improved['train'],\n",
    "        criterion_improved,\n",
    "        optimizer_improved,\n",
    "        device,\n",
    "        use_mixup=(config_improved['mixup_alpha'] > 0),\n",
    "        mixup_alpha=config_improved['mixup_alpha']\n",
    "    )\n",
    "    \n",
    "    # 在测试集上评估\n",
    "    test_loss, test_acc = evaluate(\n",
    "        net_improved,\n",
    "        loader_improved['test'],\n",
    "        criterion_improved,\n",
    "        device\n",
    "    )\n",
    "    \n",
    "    # 记录历史\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['test_loss'].append(test_loss)\n",
    "    history['test_acc'].append(test_acc)\n",
    "    \n",
    "    # 保存最佳模型\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        best_epoch = epoch\n",
    "        torch.save(net_improved.state_dict(), BEST_MODEL_PATH)\n",
    "    \n",
    "    # 打印进度\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"Epoch [{epoch+1:3d}/{config_improved['num_epochs']}] \"\n",
    "              f\"LR: {current_lr:.6f} | \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.2f}% | \"\n",
    "              f\"Test Loss: {test_loss:.4f}, Acc: {test_acc:.2f}% | \"\n",
    "              f\"Best: {best_acc:.2f}% @Epoch {best_epoch+1} | \"\n",
    "              f\"Time: {elapsed/60:.1f}min\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(\"=\" * 80)\n",
    "print(f\"训练完成！总用时: {total_time/60:.1f} 分钟\")\n",
    "print(f\"最佳测试准确率: {best_acc:.2f}% (Epoch {best_epoch+1})\")\n",
    "print(f\"最佳模型已保存到: {BEST_MODEL_PATH}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结果可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# 绘制训练曲线\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "\n",
    "\n",
    "# 损失曲线\n",
    "\n",
    "axes[0, 0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "\n",
    "axes[0, 0].plot(history['test_loss'], label='Test Loss', linewidth=2)\n",
    "\n",
    "axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
    "\n",
    "axes[0, 0].set_ylabel('Loss', fontsize=12)\n",
    "\n",
    "axes[0, 0].set_title('Loss Curve', fontsize=14, fontweight='bold')\n",
    "\n",
    "axes[0, 0].legend(fontsize=11)\n",
    "\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "\n",
    "\n",
    "# 准确率曲线\n",
    "\n",
    "axes[0, 1].plot(history['train_acc'], label='Train Acc', linewidth=2)\n",
    "\n",
    "axes[0, 1].plot(history['test_acc'], label='Test Acc', linewidth=2)\n",
    "\n",
    "axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
    "\n",
    "axes[0, 1].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "\n",
    "axes[0, 1].set_title('Accuracy Curve', fontsize=14, fontweight='bold')\n",
    "\n",
    "axes[0, 1].legend(fontsize=11)\n",
    "\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].axhline(y=best_acc, color='r', linestyle='--', label=f'Best: {best_acc:.2f}%')\n",
    "\n",
    "\n",
    "\n",
    "# 学习率曲线\n",
    "\n",
    "axes[1, 0].plot(history['lr'], linewidth=2, color='green')\n",
    "\n",
    "axes[1, 0].set_xlabel('Epoch', fontsize=12)\n",
    "\n",
    "axes[1, 0].set_ylabel('Learning Rate', fontsize=12)\n",
    "\n",
    "axes[1, 0].set_title('Learning Rate Schedule (Warmup + Cosine)', fontsize=14, fontweight='bold')\n",
    "\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].set_yscale('log')\n",
    "\n",
    "\n",
    "\n",
    "# 训练-测试 gap\n",
    "\n",
    "gap = np.array(history['train_acc']) - np.array(history['test_acc'])\n",
    "\n",
    "axes[1, 1].plot(gap, linewidth=2, color='orange')\n",
    "\n",
    "axes[1, 1].set_xlabel('Epoch', fontsize=12)\n",
    "\n",
    "axes[1, 1].set_ylabel('Accuracy Gap (%)', fontsize=12)\n",
    "\n",
    "axes[1, 1].set_title('Train-Test Accuracy Gap (Overfitting Indicator)', fontsize=14, fontweight='bold')\n",
    "\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(TRAIN_CURVES_PNG, dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "print(f\"训练曲线已保存到 {TRAIN_CURVES_PNG}\")\n",
    "\n",
    "\n",
    "\n",
    "# 如果有消融结果，展示前几项\n",
    "\n",
    "try:\n",
    "\n",
    "    from IPython.display import display\n",
    "\n",
    "    display(ablation_df.sort_values('best_test_acc', ascending=False).head(10))\n",
    "\n",
    "except Exception:\n",
    "\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 混淆矩阵（如已训练并保存最佳模型）\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# 加载最佳模型（如果存在）\n",
    "if os.path.isfile(BEST_MODEL_PATH):\n",
    "    net_improved.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=device))\n",
    "    net_improved.eval()\n",
    "\n",
    "    # 收集所有预测\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img, target in loader_improved['test']:\n",
    "            img = img.to(device)\n",
    "            pred = net_improved(img)\n",
    "            _, predicted = pred.max(1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(target.numpy())\n",
    "\n",
    "    # CIFAR-10 类别名称\n",
    "    classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "    # 绘制混淆矩阵\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=classes, yticklabels=classes,\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.title('Confusion Matrix - CIFAR-10 (Best Model)', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(CM_PNG, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # 打印分类报告\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"分类报告 (Classification Report)\")\n",
    "    print(\"=\" * 80)\n",
    "    print(classification_report(all_labels, all_preds, target_names=classes, digits=4))\n",
    "    print(f\"混淆矩阵已保存到 {CM_PNG}\")\n",
    "else:\n",
    "    print(f\"未找到最佳模型 {BEST_MODEL_PATH}，跳过混淆矩阵与分类报告绘制。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 消融实验 (Ablation Study)\n",
    "\n",
    "逐个测试各改进因素的贡献，理解每个因素对性能的影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 消融实验配置（可以先用很小的轮数做冒烟测试，再增大轮数）\n",
    "import pandas as pd\n",
    "ablation_epochs = 5  # 为了快速跑通，先用 5；正式实验可改为 30/50\n",
    "\n",
    "# 统一构建可开关的数据增强与模型\n",
    "def build_transforms(advanced: bool):\n",
    "    if not advanced:\n",
    "        return transformation\n",
    "    return transformation_improved\n",
    "\n",
    "class SimpleCIFARNet(nn.Module):\n",
    "    \"\"\"与最初 baseline 接近的简化网络，用于消融对比\"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 128, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),\n",
    "            nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),\n",
    "            nn.Conv2d(256, 512, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 256, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256*4*4, 512), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def build_model(residual: bool, se: bool, deeper_wider: bool):\n",
    "    if not residual and not se and not deeper_wider:\n",
    "        return SimpleCIFARNet(num_classes=10).to(device)\n",
    "    # 基于改进网络，允许关闭 SE 或减小深度/宽度\n",
    "    block_type = SEResidualBlock if se else ResidualBlock\n",
    "    class Variant(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            width = 64 if deeper_wider else 32\n",
    "            self.conv1 = nn.Conv2d(3, width, 3, padding=1, bias=False)\n",
    "            self.bn1 = nn.BatchNorm2d(width)\n",
    "            self.relu = nn.ReLU(inplace=True)\n",
    "            # 深度：deeper_wider 为 True 用 [2,2,3]，否则 [1,1,2]\n",
    "            cfg = ([2,2,3] if deeper_wider else [1,1,2])\n",
    "            c1, c2, c3 = width, width*2, width*4\n",
    "            self.layer1 = self._make_layer(block_type, c1, c2, num_blocks=cfg[0], stride=1)\n",
    "            self.layer2 = self._make_layer(block_type, c2, c3, num_blocks=cfg[1], stride=2)\n",
    "            self.layer3 = self._make_layer(block_type, c3, c3, num_blocks=cfg[2], stride=2)\n",
    "            self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "            self.dropout = nn.Dropout(0.5)\n",
    "            self.fc = nn.Linear(c3, 10)\n",
    "            self._init()\n",
    "        def _make_layer(self, block_type, in_c, out_c, num_blocks, stride):\n",
    "            layers = [block_type(in_c, out_c, stride)] if residual else [ResidualBlock(in_c, out_c, stride)]\n",
    "            for _ in range(1, num_blocks):\n",
    "                layers.append(block_type(out_c, out_c, stride=1) if residual else ResidualBlock(out_c, out_c, 1))\n",
    "            return nn.Sequential(*layers)\n",
    "        def _init(self):\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, nn.BatchNorm2d):\n",
    "                    nn.init.constant_(m.weight, 1)\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, nn.Linear):\n",
    "                    nn.init.normal_(m.weight, 0, 0.01)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        def forward(self, x):\n",
    "            x = self.conv1(x); x = self.bn1(x); x = self.relu(x)\n",
    "            x = self.layer1(x); x = self.layer2(x); x = self.layer3(x)\n",
    "            x = self.avg_pool(x); x = torch.flatten(x, 1); x = self.dropout(x); x = self.fc(x)\n",
    "            return x\n",
    "    return Variant().to(device)\n",
    "\n",
    "\n",
    "def run_experiment(name: str, *,\n",
    "                   use_residual: bool,\n",
    "                   use_se: bool,\n",
    "                   deeper_wider: bool,\n",
    "                   advanced_aug: bool,\n",
    "                   use_adamw: bool,\n",
    "                   use_warmup_cosine: bool,\n",
    "                   label_smoothing: float,\n",
    "                   mixup_alpha: float,\n",
    "                   epochs: int = ablation_epochs):\n",
    "    # 数据\n",
    "    tf = build_transforms(advanced_aug)\n",
    "    ds = {k: tv_datasets.CIFAR10(root=\"./data\", train=(k==\"train\"), download=True, transform=tf[k]) for k in (\"train\",\"test\")}\n",
    "    ld = {\n",
    "        \"train\": torch.utils.data.DataLoader(ds[\"train\"], batch_size=128, shuffle=True, num_workers=4, pin_memory=True),\n",
    "        \"test\": torch.utils.data.DataLoader(ds[\"test\"], batch_size=128, shuffle=False, num_workers=4, pin_memory=True),\n",
    "    }\n",
    "    # 模型\n",
    "    model = build_model(use_residual, use_se, deeper_wider)\n",
    "    # 优化器与损失\n",
    "    if use_adamw:\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "    else:\n",
    "        optimizer = optim.Adam(model.parameters(), lr=3e-4, weight_decay=1e-6)\n",
    "    criterion = LabelSmoothingCrossEntropy(epsilon=label_smoothing) if label_smoothing>0 else nn.CrossEntropyLoss()\n",
    "    # 调度器\n",
    "    scheduler = WarmupCosineSchedule(optimizer, warmup_epochs=5, total_epochs=epochs) if use_warmup_cosine else None\n",
    "    # 训练\n",
    "    best = 0.0\n",
    "    for ep in range(epochs):\n",
    "        if scheduler is not None:\n",
    "            _ = scheduler.step(ep)\n",
    "        train_epoch(model, ld[\"train\"], criterion, optimizer, device,\n",
    "                    use_mixup=(mixup_alpha>0), mixup_alpha=mixup_alpha)\n",
    "        _, acc = evaluate(model, ld[\"test\"], criterion, device)\n",
    "        best = max(best, acc)\n",
    "    return best\n",
    "\n",
    "\n",
    "# 设计一组≥5个因素的实验（先用很少的 epoch 冒烟）\n",
    "experiments = [\n",
    "    (\"baseline\",              dict(use_residual=False, use_se=False, deeper_wider=False, advanced_aug=False, use_adamw=False, use_warmup_cosine=False, label_smoothing=0.0, mixup_alpha=0.0)),\n",
    "    (\"+residual\",             dict(use_residual=True,  use_se=False, deeper_wider=False, advanced_aug=False, use_adamw=False, use_warmup_cosine=False, label_smoothing=0.0, mixup_alpha=0.0)),\n",
    "    (\"+deeper_wider\",         dict(use_residual=True,  use_se=False, deeper_wider=True,  advanced_aug=False, use_adamw=False, use_warmup_cosine=False, label_smoothing=0.0, mixup_alpha=0.0)),\n",
    "    (\"+advanced_aug\",         dict(use_residual=True,  use_se=False, deeper_wider=True,  advanced_aug=True,  use_adamw=False, use_warmup_cosine=False, label_smoothing=0.0, mixup_alpha=0.0)),\n",
    "    (\"+SE\",                   dict(use_residual=True,  use_se=True,  deeper_wider=True,  advanced_aug=True,  use_adamw=False, use_warmup_cosine=False, label_smoothing=0.0, mixup_alpha=0.0)),\n",
    "    (\"+AdamW+Cosine+Warmup\",  dict(use_residual=True,  use_se=True,  deeper_wider=True,  advanced_aug=True,  use_adamw=True,  use_warmup_cosine=True,  label_smoothing=0.0, mixup_alpha=0.0)),\n",
    "    (\"+LabelSmoothing\",       dict(use_residual=True,  use_se=True,  deeper_wider=True,  advanced_aug=True,  use_adamw=True,  use_warmup_cosine=True,  label_smoothing=0.1, mixup_alpha=0.0)),\n",
    "    (\"+MixUp\",                dict(use_residual=True,  use_se=True,  deeper_wider=True,  advanced_aug=True,  use_adamw=True,  use_warmup_cosine=True,  label_smoothing=0.1, mixup_alpha=0.2)),\n",
    "]\n",
    "\n",
    "rows = []\n",
    "for name, cfg in experiments:\n",
    "    print(f\"Running: {name}  (epochs={ablation_epochs})\")\n",
    "    acc = run_experiment(name, **cfg, epochs=ablation_epochs)\n",
    "    row = {\"name\": name, **cfg, \"best_test_acc\": acc}\n",
    "    rows.append(row)\n",
    "\n",
    "ablation_df = pd.DataFrame(rows)\n",
    "print(\"\\nAblation Results:\")\n",
    "print(ablation_df)\n",
    "\n",
    "# 保存为 CSV\n",
    "ablation_df.to_csv(ABLATION_CSV, index=False)\n",
    "print(f\"消融结果已保存到 {ABLATION_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 归档 Notebook 到结果目录（可在全部训练/可视化/消融完成后运行）\n",
    "\n",
    "\n",
    "try:\n",
    "\n",
    "\n",
    "    shutil.copy2(NOTEBOOK_ABS_PATH, NOTEBOOK_COPY_PATH)\n",
    "\n",
    "\n",
    "    print(f\"Notebook archived to: {NOTEBOOK_COPY_PATH}\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "\n",
    "\n",
    "    print(f\"Failed to copy notebook: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验总结与分析\n",
    "\n",
    "### 改进因素总览\n",
    "\n",
    "| 改进因素 | 描述 | 预期效果 |\n",
    "|---------|------|---------|\n",
    "| **1. 残差连接** | 跳跃连接缓解梯度消失 | 支持更深网络，提升收敛速度 |\n",
    "| **2. SE 注意力** | 学习通道权重 | 关注重要特征，提升表征能力 |\n",
    "| **3. 数据增强** | Cutout + MixUp + RandomErasing + ColorJitter | 增强泛化能力，减少过拟合 |\n",
    "| **4. 网络深度/宽度** | 7 个残差块，更多通道数 | 增加模型容量 |\n",
    "| **5. 优化器改进** | AdamW + Warmup + Cosine LR | 稳定训练，更好收敛 |\n",
    "| **6. 标签平滑** | Label Smoothing (ε=0.1) | 防止过度自信，提升泛化 |\n",
    "\n",
    "> 本次所有训练产物均集中保存于 `RESULTS_DIR` 目录：\n",
    "> - 最优模型: `BEST_MODEL_PATH`\n",
    "> - 训练曲线: `TRAIN_CURVES_PNG`\n",
    "> - 混淆矩阵: `CM_PNG`\n",
    "> - 消融结果: `ABLATION_CSV`\n",
    "> - Notebook 归档: `NOTEBOOK_COPY_PATH`（见下方归档单元）\n",
    "\n",
    "### Baseline vs 改进模型对比（运行后用实测结果替换）\n",
    "\n",
    "| 模型 | 参数量 | 测试准确率 | 改进幅度 |\n",
    "|------|--------|-----------|---------|\n",
    "| Baseline (原始) | ~5.5M | ~86.5% | - |\n",
    "| 改进模型 (全部因素) | ~3.8M | 预期 90–93% | +3.5% ~ +6.5% |\n",
    "\n",
    "### 各因素贡献分析（以消融结果为准）\n",
    "\n",
    "运行完消融实验后，将以实测数据更新文字分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 结果表格（从消融 CSV 汇总）\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "if 'ABLATION_CSV' in globals() and os.path.isfile(ABLATION_CSV):\n",
    "    ablation_df = pd.read_csv(ABLATION_CSV)\n",
    "    # 自适应列名：best_val_acc 或 best_test_acc\n",
    "    score_col = 'best_val_acc' if 'best_val_acc' in ablation_df.columns else ('best_test_acc' if 'best_test_acc' in ablation_df.columns else None)\n",
    "    if score_col is not None:\n",
    "        ablation_sorted = ablation_df.sort_values(score_col, ascending=False).reset_index(drop=True)\n",
    "        display(ablation_sorted)\n",
    "        md_table = ablation_sorted.to_markdown(index=False)\n",
    "        summary_md = os.path.join(RESULTS_DIR, 'ablation_summary.md') if 'RESULTS_DIR' in globals() else 'ablation_summary.md'\n",
    "        with open(summary_md, 'w', encoding='utf-8') as f:\n",
    "            f.write(md_table)\n",
    "        print(f\"消融表格已导出到: {summary_md}\")\n",
    "    else:\n",
    "        print(\"未找到 best_val_acc/best_test_acc 列，无法排序展示。原始表：\")\n",
    "        display(ablation_df)\n",
    "else:\n",
    "    print(\"未找到 ABLATION_CSV，跳过结果表格展示。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 运行元数据保存（便于报告与复现）\n",
    "meta = {}\n",
    "meta[\"run_time\"] = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "meta[\"seed\"] = SEED if 'SEED' in globals() else None\n",
    "meta[\"device\"] = str(device) if 'device' in globals() else None\n",
    "meta[\"best_acc\"] = float(best_acc) if 'best_acc' in globals() else None\n",
    "meta[\"best_epoch\"] = int(best_epoch) if 'best_epoch' in globals() else None\n",
    "meta[\"artifacts\"] = {\n",
    "    \"results_dir\": RESULTS_DIR if 'RESULTS_DIR' in globals() else None,\n",
    "    \"best_model_path\": BEST_MODEL_PATH if 'BEST_MODEL_PATH' in globals() else None,\n",
    "    \"train_curves_png\": TRAIN_CURVES_PNG if 'TRAIN_CURVES_PNG' in globals() else None,\n",
    "    \"confusion_matrix_png\": CM_PNG if 'CM_PNG' in globals() else None,\n",
    "    \"ablation_csv\": ABLATION_CSV if 'ABLATION_CSV' in globals() else None,\n",
    "}\n",
    "\n",
    "meta_path = os.path.join(RESULTS_DIR, 'run_metadata.json') if 'RESULTS_DIR' in globals() else 'run_metadata.json'\n",
    "with open(meta_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "print(f\"运行元数据已保存到: {meta_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 我学到了什么（反思）\n",
    "\n",
    "在本次 CIFAR-10 任务中，我从一个简洁的 CNN 出发，逐步引入以下因素并进行对比：\n",
    "\n",
    "- 残差与 SE 注意力：缓解退化并提升特征表达；\n",
    "- 更深/更宽的结构：带来容量提升但需要配合正则与调参；\n",
    "- 更强的数据增强（含 Cutout/MixUp/RandomErasing）：有效抑制过拟合；\n",
    "- AdamW 与 Warmup+Cosine：更稳定的优化与更好的最终性能；\n",
    "- Label Smoothing：在类别间相似时可提升泛化表现；\n",
    "\n",
    "结合训练曲线与消融结果，我理解到“配方”需要整体协同：增强强度、epoch 数、正则化与学习率日程彼此影响，不可孤立看待。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
