{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'python': '3.10.19', 'platform': 'Windows-10-10.0.22621-SP0', 'pytorch': '2.5.1+cu121', 'cuda_available': True, 'cuda_version': '12.1', 'device': 'cuda'}\n",
      "SEED= 42\n"
     ]
    }
   ],
   "source": [
    "# 环境与随机种子（确保可复现）\n",
    "import os, sys, random, time, platform, json\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "SEED = int(os.environ.get(\"SEED\", 42))\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# cuDNN 可复现设置\n",
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = False\n",
    "cudnn.deterministic = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print({\n",
    "    \"python\": sys.version.split(\" \")[0],\n",
    "    \"platform\": platform.platform(),\n",
    "    \"pytorch\": torch.__version__,\n",
    "    \"cuda_available\": torch.cuda.is_available(),\n",
    "    \"cuda_version\": torch.version.cuda if torch.cuda.is_available() else None,\n",
    "    \"device\": str(device),\n",
    "})\n",
    "print(\"SEED=\", SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 复现实验环境与运行说明\n",
    "\n",
    "本 Notebook 使用 PyTorch>=2 进行 CIFAR-10 图像分类实验。为提高复现性，我们在最前面固定随机种子、打印环境信息，并给出关键开关说明：\n",
    "\n",
    "- 随机种子：seed 固定，cuDNN 设为 deterministic。\n",
    "- 设备选择：自动选择 CUDA/GPU 或 CPU。\n",
    "- 运行产物：所有模型、图像与 CSV 会保存到统一的 RESULTS_DIR 下。\n",
    "\n",
    "在训练前后，可参考末尾的“结果表格与总结”与“我学到了什么”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "This code baseline is inspired by and modified from [this great tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html).\n",
    "\n",
    "This code can achieve an accuracy of approximately 86.50% on CIFAR-10. Please set up the environment and run your experiments starting from this baseline. You are expected to achieve an accuracy higher than this baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "# torch: PyTorch 的核心张量与自动求导库\n",
    "import torch\n",
    "# nn: 神经网络层、损失函数等模块\n",
    "import torch.nn as nn\n",
    "# optim: 各类优化器（SGD/Adam 等）\n",
    "import torch.optim as optim\n",
    "\n",
    "# torchvision: 计算机视觉常用数据集与图像增广\n",
    "# tv_datasets: 常见视觉数据集（如 CIFAR-10）\n",
    "import torchvision.datasets as tv_datasets\n",
    "# tv_transforms: 图像预处理/数据增强流水线\n",
    "import torchvision.transforms as tv_transforms\n",
    "\n",
    "# 额外工具\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# 固定随机种子，保证可复现\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "GPU name: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# 实验参数与运行设备设置\n",
    "# 优先使用 CUDA 的第 0 块 GPU；若不可用则回退到 CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 设备日志：确认是否成功使用 GPU\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    try:\n",
    "        print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    except Exception as e:\n",
    "        # 某些环境下可能无法读取设备名称\n",
    "        print(f\"CUDA available but failed to get device name: {e}\")\n",
    "\n",
    "# 训练超参数\n",
    "num_epochs = 128          # 训练轮数\n",
    "batch_size = 64           # 每个 mini-batch 的样本数量\n",
    "num_workers = 2           # DataLoader 载入数据的线程数（Windows 上建议适度）\n",
    "print_every = 200         # 每多少个 iteration 打印一次训练损失\n",
    "\n",
    "# 优化器配置\n",
    "optim_name = \"Adam\"       # 优化器名称（支持 'SGD'、'Adam' 等）\n",
    "optim_kwargs = dict(\n",
    "    lr=3e-4,              # 学习率\n",
    "    weight_decay=1e-6,    # L2 正则（权重衰减）\n",
    ")\n",
    "\n",
    "# 输入图像的预处理/数据增强流水线\n",
    "# 训练集与测试集使用相同的标准化，但训练集额外加入随机增广提升泛化\n",
    "transformation = dict()\n",
    "for data_type in (\"train\", \"test\"):\n",
    "    is_train = data_type==\"train\"\n",
    "    # Compose 将一系列变换按顺序组合\n",
    "    transformation[data_type] = tv_transforms.Compose(([ \n",
    "        # 仅训练时使用的数据增强\n",
    "        tv_transforms.RandomRotation(degrees=15),                 # 随机旋转\n",
    "        tv_transforms.RandomHorizontalFlip(),                     # 随机水平翻转\n",
    "        tv_transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)), # 随机平移\n",
    "    ] if is_train else []) + \n",
    "    [\n",
    "        tv_transforms.ToTensor(),                                 # 转为张量并缩放到 [0,1]\n",
    "        # 以 0.5 为均值、0.5 为标准差做标准化: (x - 0.5) / 0.5 -> 约等于缩放到 [-1,1]\n",
    "        tv_transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results will be saved to: ./results\\cifar10_20251027_012326\n"
     ]
    }
   ],
   "source": [
    "# 结果目录与路径常量（统一保存产物）\n",
    "import time, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "RUN_TAG = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "RESULTS_ROOT = \"./results\"\n",
    "RESULTS_DIR = os.path.join(RESULTS_ROOT, f\"cifar10_{RUN_TAG}\")\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "print(f\"Results will be saved to: {RESULTS_DIR}\")\n",
    "\n",
    "# 统一产物路径\n",
    "BEST_MODEL_PATH = os.path.join(RESULTS_DIR, \"best_model_cifar10.pth\")\n",
    "TRAIN_CURVES_PNG = os.path.join(RESULTS_DIR, \"cifar10_training_curves.png\")\n",
    "CM_PNG = os.path.join(RESULTS_DIR, \"cifar10_confusion_matrix.png\")\n",
    "ABLATION_CSV = os.path.join(RESULTS_DIR, \"cifar10_ablation_results.csv\")\n",
    "\n",
    "# 当前 Notebook 的绝对路径（用于归档）\n",
    "NOTEBOOK_ABS_PATH = \"/data/zhangzhikui/githubbase/DL/HW1/CIFAR-10.ipynb\"\n",
    "NOTEBOOK_COPY_PATH = os.path.join(RESULTS_DIR, f\"CIFAR-10_{RUN_TAG}.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# 准备 CIFAR-10 数据集与数据加载器\n",
    "dataset, loader = {}, {}\n",
    "for data_type in (\"train\", \"test\"):\n",
    "    is_train = data_type==\"train\"\n",
    "    # CIFAR-10: 50000 张训练图像 + 10000 张测试图像，类别数为 10\n",
    "    dataset[data_type] = tv_datasets.CIFAR10(\n",
    "        root=\"./data\",                 # 数据集存放路径（如不存在将自动创建）\n",
    "        train=is_train,                # 训练/测试划分\n",
    "        download=True,                 # 若本地无数据则联网下载\n",
    "        transform=transformation[data_type],  # 应用上面定义的预处理\n",
    "    )\n",
    "    # DataLoader 负责按批次提供数据并在训练集上打乱顺序\n",
    "    loader[data_type] = torch.utils.data.DataLoader(\n",
    "        dataset[data_type],\n",
    "        batch_size=batch_size,\n",
    "        shuffle=is_train,              # 仅在训练集上打乱，测试集保持顺序\n",
    "        num_workers=num_workers,       # 加载线程数\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 7.28M\n"
     ]
    }
   ],
   "source": [
    "# 定义卷积神经网络（CNN）结构\n",
    "net = nn.Sequential(\n",
    "    # 下采样阶段 1: 输入 3x32x32 -> 中间特征\n",
    "    nn.Conv2d(3, 128, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),\n",
    "    # 下采样阶段 2\n",
    "    nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),\n",
    "    # 更深的卷积特征提取\n",
    "    nn.Conv2d(256, 512, 3, padding=1), nn.ReLU(inplace=True),\n",
    "    nn.Conv2d(512, 512, 3, padding=1), nn.ReLU(inplace=True),\n",
    "    # 压缩通道数并再次下采样\n",
    "    nn.Conv2d(512, 256, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),\n",
    "    # 展平为全连接层输入\n",
    "    nn.Flatten(),\n",
    "    # 全连接分类头，含 Dropout 做正则化\n",
    "    nn.Linear(256 * 4 * 4, 512), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
    "    nn.Linear(512, 256), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
    "    nn.Linear(256, 128), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
    "    nn.Linear(128, 10),  # 10 个类别的 logits\n",
    ")\n",
    "\n",
    "# 将模型移动到指定设备（GPU/CPU）\n",
    "net.to(device)\n",
    "\n",
    "# 统计可训练参数量（单位：百万）\n",
    "print(f\"number of parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad) / 1_000_000:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch=  1, iter=  200] loss: 2.220\n",
      "[epoch=  1, iter=  400] loss: 1.997\n",
      "[epoch=  1, iter=  600] loss: 1.841\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()       \u001b[38;5;66;03m# 按优化器策略更新参数\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# 统计与日志打印\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m print_every \u001b[38;5;241m==\u001b[39m print_every \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[epoch=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m3d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, iter=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m5d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mprint_every\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 定义网络优化器（从 torch.optim 动态获取指定优化器）\n",
    "optimizer = getattr(optim, optim_name)(net.parameters(), **optim_kwargs)\n",
    "\n",
    "# 多分类交叉熵损失，适用于单标签多分类任务\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 训练循环\n",
    "net.train()  # 切换到训练模式（启用 Dropout/BN 的训练行为）\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, (img, target) in enumerate(loader[\"train\"]):\n",
    "        # 将数据移动到相同设备\n",
    "        img, target = img.to(device), target.to(device)\n",
    "\n",
    "        # 前向计算得到 logits 预测\n",
    "        pred = net(img)\n",
    "        loss = criterion(pred, target)\n",
    "\n",
    "        # 反向传播与参数更新\n",
    "        optimizer.zero_grad()  # 清空上一轮梯度\n",
    "        loss.backward()        # 计算当前梯度\n",
    "        optimizer.step()       # 按优化器策略更新参数\n",
    "\n",
    "        # 统计与日志打印\n",
    "        running_loss += loss.item()\n",
    "        if i % print_every == print_every - 1:\n",
    "            print(f\"[epoch={epoch + 1:3d}, iter={i + 1:5d}] loss: {running_loss / print_every:.3f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating its accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 切换到评估模式（关闭 Dropout/固定 BN 统计）\n",
    "net.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():  # 评估时不需要梯度，降低显存/加速\n",
    "    for img, target in loader[\"test\"]:\n",
    "        img, target = img.to(device), target.to(device)\n",
    "        \n",
    "        # 前向推理，得到每类的 logits\n",
    "        pred = net(img)\n",
    "        \n",
    "        # 累积统计：top-1 预测与真实标签比较\n",
    "        total += len(target)\n",
    "        correct += (torch.argmax(pred, dim=1) == target).sum().item()\n",
    "\n",
    "print(f\"Accuracy of the network on the {total} test images: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 改进实验整体流程\n",
    "- **阶段 1：消融实验（各因素单独 10 轮）**\n",
    "  - Baseline：简洁 CNN（无残差/注意力/高级增强/优化）\n",
    "  - 依次只添加一个因素：残差连接、SE 通道注意力、加深网络、强化数据增强、优化策略\n",
    "  - 记录每个因素带来的验证集增益，筛选出有效改进\n",
    "- **阶段 2：组合训练（150 轮）**\n",
    "  - 将阶段 1 证明有效的因素组合到同一配置\n",
    "  - 使用较长训练计划 + 最佳优化策略，保存最佳模型\n",
    "- **阶段 3：结果分析**\n",
    "  - 对比 Baseline 与最终模型\n",
    "  - 回顾各改进因素的贡献，总结经验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 阶段 1：10 轮消融实验设置\n",
    "- 统一使用批量 128、基础学习率 0.1（SGD），不使用学习率调度作为基线。\n",
    "- 数据集划分：训练集 45k / 验证集 5k（从官方训练集划分）。\n",
    "- 指标：验证集 top-1 准确率，记录最后 3 轮的平均值以减小波动。\n",
    "- 逐个因素实验时，其余保持与基线一致，便于对比。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, Any, List, Tuple, Optional\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# 环境初始化与实验配置\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "NUM_CLASSES = 10\n",
    "DATA_ROOT = './data'\n",
    "RESULT_DIR = './runs'\n",
    "MODEL_DIR = './models'\n",
    "os.makedirs(RESULT_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "print('Torch', torch.__version__, '| TorchVision', torchvision.__version__)\n",
    "print('CUDA available:', torch.cuda.is_available(), '| device:', device)\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    name: str = 'baseline'\n",
    "    epochs: int = 10\n",
    "    batch_size: int = 128\n",
    "    base_lr: float = 0.1\n",
    "    momentum: float = 0.9\n",
    "    weight_decay: float = 5e-4\n",
    "    optimizer: str = 'sgd'            # 'sgd' or 'adamw'\n",
    "    scheduler: str = 'none'           # 'none' | 'cosine' | 'onecycle'\n",
    "    label_smoothing: float = 0.0\n",
    "    use_amp: bool = False\n",
    "    grad_clip: float = 0.0\n",
    "    max_steps_per_epoch: int = 0      # 0 表示不限制；用于快速烟囱测试\n",
    "\n",
    "    # 结构改进\n",
    "    use_residual: bool = False\n",
    "    use_se: bool = False\n",
    "    depth: int = 2                   # blocks per stage\n",
    "    width: int = 1                   # channel multiplier\n",
    "\n",
    "    # 数据增强/正则化\n",
    "    use_strong_aug: bool = False\n",
    "    randaugment_n: int = 0\n",
    "    randaugment_m: int = 9\n",
    "    use_mixup: bool = False\n",
    "    mixup_alpha: float = 0.2\n",
    "    use_cutmix: bool = False\n",
    "    cutmix_alpha: float = 1.0\n",
    "    label_smoothing_for_aug: float = 0.05\n",
    "\n",
    "    # 其他\n",
    "    save_path: str = ''\n",
    "    seed: int = SEED\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return asdict(self)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集拆分与数据增强策略\n",
    "- 基线使用 **RandomCrop + HorizontalFlip** 与标准化。\n",
    "- 强增强在基于 RandAugment 的基础上叠加 Cutout / RandomErasing，并在 batch 级别选择性启用 MixUp 或 CutMix。\n",
    "- 训练集按照 45k / 5k 划分验证集，保证消融测试公平对比。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据加载与增强管线\n",
    "CIFAR_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR_STD = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "\n",
    "class Cutout:\n",
    "    \"\"\"在张量图像上随机遮挡若干正方形区域。\"\"\"\n",
    "    def __init__(self, n_holes: int = 1, length: int = 16):\n",
    "        self.n_holes = n_holes\n",
    "        self.length = length\n",
    "\n",
    "    def __call__(self, img: torch.Tensor) -> torch.Tensor:\n",
    "        if not torch.is_tensor(img):\n",
    "            raise TypeError('Cutout 需要在 ToTensor 之后使用。')\n",
    "        h, w = img.size(1), img.size(2)\n",
    "        mask = torch.ones((h, w), device=img.device)\n",
    "        for _ in range(self.n_holes):\n",
    "            y = torch.randint(0, h, (1,)).item()\n",
    "            x = torch.randint(0, w, (1,)).item()\n",
    "            y1 = max(0, y - self.length // 2)\n",
    "            y2 = min(h, y + self.length // 2)\n",
    "            x1 = max(0, x - self.length // 2)\n",
    "            x2 = min(w, x + self.length // 2)\n",
    "            mask[y1:y2, x1:x2] = 0\n",
    "        mask = mask.expand_as(img)\n",
    "        return img * mask\n",
    "\n",
    "\n",
    "class TransformSubset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, indices: List[int], transform):\n",
    "        self.dataset = dataset\n",
    "        self.indices = indices\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        image, target = self.dataset[self.indices[idx]]\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "def build_transforms(cfg: ExperimentConfig):\n",
    "    train_tfms: List[Any] = [\n",
    "        T.RandomCrop(32, padding=4),\n",
    "        T.RandomHorizontalFlip(),\n",
    "    ]\n",
    "    if cfg.use_strong_aug:\n",
    "        if cfg.randaugment_n > 0:\n",
    "            train_tfms.append(T.RandAugment(cfg.randaugment_n, cfg.randaugment_m))\n",
    "        train_tfms.append(T.ColorJitter(0.3, 0.3, 0.3, 0.2))\n",
    "    train_tfms.append(T.ToTensor())\n",
    "    train_tfms.append(T.Normalize(CIFAR_MEAN, CIFAR_STD))\n",
    "    if cfg.use_strong_aug:\n",
    "        train_tfms.append(Cutout(n_holes=1, length=12))\n",
    "        train_tfms.append(T.RandomErasing(p=0.25, scale=(0.02, 0.25)))\n",
    "\n",
    "    eval_tfms = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(CIFAR_MEAN, CIFAR_STD)\n",
    "    ])\n",
    "\n",
    "    return T.Compose(train_tfms), eval_tfms\n",
    "\n",
    "\n",
    "def build_dataloaders(cfg: ExperimentConfig, num_workers: int = 0) -> Dict[str, DataLoader]:\n",
    "    # Windows + Notebook 环境下，num_workers=0 更稳妥\n",
    "    set_seed(cfg.seed)\n",
    "    train_transform, eval_transform = build_transforms(cfg)\n",
    "\n",
    "    base_train = torchvision.datasets.CIFAR10(\n",
    "        root=DATA_ROOT,\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=None\n",
    "    )\n",
    "    test_dataset = torchvision.datasets.CIFAR10(\n",
    "        root=DATA_ROOT,\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=eval_transform\n",
    "    )\n",
    "\n",
    "    total_len = len(base_train)\n",
    "    val_len = 5000\n",
    "    train_len = total_len - val_len\n",
    "    generator = torch.Generator().manual_seed(0)\n",
    "    train_indices, val_indices = random_split(range(total_len), [train_len, val_len], generator=generator)\n",
    "\n",
    "    train_subset = TransformSubset(base_train, list(train_indices), train_transform)\n",
    "    val_subset = TransformSubset(base_train, list(val_indices), eval_transform)\n",
    "\n",
    "    loaders = {\n",
    "        'train': DataLoader(train_subset, batch_size=cfg.batch_size, shuffle=True,\n",
    "                             num_workers=num_workers, pin_memory=True),\n",
    "        'val': DataLoader(val_subset, batch_size=cfg.batch_size, shuffle=False,\n",
    "                           num_workers=num_workers, pin_memory=True),\n",
    "        'test': DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False,\n",
    "                            num_workers=num_workers, pin_memory=True)\n",
    "    }\n",
    "    return loaders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型结构因子\n",
    "- **Residual**：stage 内使用残差跳连，缓解梯度消失。\n",
    "- **SE Attention**：引入通道注意力，自适应重标特征。\n",
    "- **Depth / Width**：通过 `depth` 和 `width` 超参调整网络容量。\n",
    "- 所有变化都在同一个 `TinyCIFARNet` 架构中切换，保证实验公平。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型构建\n",
    "class SEModule(nn.Module):\n",
    "    def __init__(self, ch: int, reduction: int = 16):\n",
    "        super().__init__()\n",
    "        hidden = max(ch // reduction, 4)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv1 = nn.Conv2d(ch, hidden, kernel_size=1, bias=True)\n",
    "        self.conv2 = nn.Conv2d(hidden, ch, kernel_size=1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        w = self.pool(x)\n",
    "        w = F.relu(self.conv1(w), inplace=True)\n",
    "        w = torch.sigmoid(self.conv2(w))\n",
    "        return x * w\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int, stride: int = 1,\n",
    "                 residual: bool = False, use_se: bool = False):\n",
    "        super().__init__()\n",
    "        self.residual = residual\n",
    "        self.use_se = use_se\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_ch)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
    "        self.se = SEModule(out_ch) if use_se else nn.Identity()\n",
    "        self.shortcut = None\n",
    "        if residual and (stride != 1 or in_ch != out_ch):\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_ch)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)), inplace=True)\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = self.se(out)\n",
    "        if self.residual:\n",
    "            shortcut = x if self.shortcut is None else self.shortcut(x)\n",
    "            out = out + shortcut\n",
    "        return F.relu(out, inplace=True)\n",
    "\n",
    "\n",
    "class TinyCIFARNet(nn.Module):\n",
    "    def __init__(self, cfg: ExperimentConfig):\n",
    "        super().__init__()\n",
    "        widths = [32, 64, 128]\n",
    "        widths = [w * cfg.width for w in widths]\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, widths[0], kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(widths[0]),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.stage1 = self._make_stage(widths[0], widths[0], cfg.depth,\n",
    "                                       stride=1, residual=cfg.use_residual, use_se=cfg.use_se)\n",
    "        self.stage2 = self._make_stage(widths[0], widths[1], cfg.depth,\n",
    "                                       stride=2, residual=cfg.use_residual, use_se=cfg.use_se)\n",
    "        self.stage3 = self._make_stage(widths[1], widths[2], cfg.depth,\n",
    "                                       stride=2, residual=cfg.use_residual, use_se=cfg.use_se)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(widths[2], NUM_CLASSES)\n",
    "        )\n",
    "        self._init_weights()\n",
    "\n",
    "    def _make_stage(self, in_ch: int, out_ch: int, depth: int,\n",
    "                    stride: int, residual: bool, use_se: bool) -> nn.Sequential:\n",
    "        blocks = [ConvBlock(in_ch, out_ch, stride=stride,\n",
    "                            residual=residual, use_se=use_se)]\n",
    "        for _ in range(1, depth):\n",
    "            blocks.append(ConvBlock(out_ch, out_ch, stride=1,\n",
    "                                    residual=residual, use_se=use_se))\n",
    "        return nn.Sequential(*blocks)\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def build_model(cfg: ExperimentConfig) -> nn.Module:\n",
    "    model = TinyCIFARNet(cfg)\n",
    "    return model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练与评估工具\n",
    "class AverageMeter:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.sum = 0.0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, value: float, n: int = 1):\n",
    "        self.sum += value * n\n",
    "        self.count += n\n",
    "\n",
    "    @property\n",
    "    def avg(self) -> float:\n",
    "        if self.count == 0:\n",
    "            return 0.0\n",
    "        return self.sum / self.count\n",
    "\n",
    "\n",
    "def soft_cross_entropy(logits: torch.Tensor, soft_targets: torch.Tensor) -> torch.Tensor:\n",
    "    log_probs = F.log_softmax(logits, dim=1)\n",
    "    return -(soft_targets * log_probs).sum(dim=1).mean()\n",
    "\n",
    "\n",
    "def one_hot(targets: torch.Tensor, num_classes: int, smoothing: float = 0.0) -> torch.Tensor:\n",
    "    with torch.no_grad():\n",
    "        y = torch.zeros((targets.size(0), num_classes), device=targets.device)\n",
    "        y.fill_(smoothing / (num_classes - 1))\n",
    "        y.scatter_(1, targets.unsqueeze(1), 1.0 - smoothing)\n",
    "    return y\n",
    "\n",
    "\n",
    "def prepare_optimizer(model: nn.Module, cfg: ExperimentConfig, steps_per_epoch: int):\n",
    "    if cfg.optimizer == 'adamw':\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=cfg.base_lr,\n",
    "                                weight_decay=cfg.weight_decay)\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=cfg.base_lr,\n",
    "                              momentum=cfg.momentum, weight_decay=cfg.weight_decay, nesterov=True)\n",
    "\n",
    "    scheduler = None\n",
    "    if cfg.scheduler == 'cosine':\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=cfg.epochs)\n",
    "    elif cfg.scheduler == 'onecycle':\n",
    "        scheduler = OneCycleLR(optimizer, max_lr=cfg.base_lr,\n",
    "                               steps_per_epoch=steps_per_epoch, epochs=cfg.epochs)\n",
    "    return optimizer, scheduler\n",
    "\n",
    "\n",
    "def apply_mixup_cutmix(inputs: torch.Tensor, targets: torch.Tensor, cfg: ExperimentConfig):\n",
    "    soft_targets = one_hot(targets, NUM_CLASSES, smoothing=cfg.label_smoothing_for_aug)\n",
    "    if cfg.use_mixup and cfg.mixup_alpha > 0:\n",
    "        lam = np.random.beta(cfg.mixup_alpha, cfg.mixup_alpha)\n",
    "        index = torch.randperm(inputs.size(0), device=inputs.device)\n",
    "        mixed = lam * inputs + (1 - lam) * inputs[index]\n",
    "        soft_targets = lam * soft_targets + (1 - lam) * soft_targets[index]\n",
    "        return mixed, soft_targets\n",
    "    if cfg.use_cutmix and cfg.cutmix_alpha > 0:\n",
    "        lam = np.random.beta(cfg.cutmix_alpha, cfg.cutmix_alpha)\n",
    "        batch_size, _, h, w = inputs.size()\n",
    "        index = torch.randperm(batch_size, device=inputs.device)\n",
    "        cut_rat = math.sqrt(1.0 - lam)\n",
    "        cut_w = int(w * cut_rat)\n",
    "        cut_h = int(h * cut_rat)\n",
    "        cx = np.random.randint(w)\n",
    "        cy = np.random.randint(h)\n",
    "        x1 = np.clip(cx - cut_w // 2, 0, w)\n",
    "        y1 = np.clip(cy - cut_h // 2, 0, h)\n",
    "        x2 = np.clip(cx + cut_w // 2, 0, w)\n",
    "        y2 = np.clip(cy + cut_h // 2, 0, h)\n",
    "        inputs[:, :, y1:y2, x1:x2] = inputs[index, :, y1:y2, x1:x2]\n",
    "        lam = 1 - ((x2 - x1) * (y2 - y1) / (w * h))\n",
    "        soft_targets = lam * soft_targets + (1 - lam) * soft_targets[index]\n",
    "        return inputs, soft_targets\n",
    "    return inputs, soft_targets\n",
    "\n",
    "\n",
    "def train_one_epoch(model: nn.Module, loaders: Dict[str, DataLoader], cfg: ExperimentConfig,\n",
    "                    optimizer, scheduler=None, scaler: Optional[GradScaler] = None) -> Dict[str, float]:\n",
    "    model.train()\n",
    "    loss_meter = AverageMeter()\n",
    "    acc_meter = AverageMeter()\n",
    "    train_loader = loaders['train']\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=cfg.label_smoothing)\n",
    "\n",
    "    for step, (inputs, targets) in enumerate(train_loader):\n",
    "        if cfg.max_steps_per_epoch and (step >= cfg.max_steps_per_epoch):\n",
    "            break\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        use_soft = cfg.use_mixup or cfg.use_cutmix\n",
    "        if use_soft:\n",
    "            inputs, soft_targets = apply_mixup_cutmix(inputs, targets, cfg)\n",
    "        with autocast(enabled=cfg.use_amp):\n",
    "            outputs = model(inputs)\n",
    "            if use_soft:\n",
    "                loss = soft_cross_entropy(outputs, soft_targets)\n",
    "                hard_targets = torch.argmax(soft_targets, dim=1)\n",
    "            elif cfg.label_smoothing > 0.0:\n",
    "                soft_targets = one_hot(targets, NUM_CLASSES, smoothing=cfg.label_smoothing)\n",
    "                loss = soft_cross_entropy(outputs, soft_targets)\n",
    "                hard_targets = targets\n",
    "            else:\n",
    "                loss = criterion(outputs, targets)\n",
    "                hard_targets = targets\n",
    "\n",
    "        if scaler is not None and cfg.use_amp:\n",
    "            scaler.scale(loss).backward()\n",
    "            if cfg.grad_clip > 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            if cfg.grad_clip > 0:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "            optimizer.step()\n",
    "\n",
    "        if scheduler is not None and cfg.scheduler == 'onecycle':\n",
    "            scheduler.step()\n",
    "\n",
    "        loss_meter.update(loss.item(), inputs.size(0))\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        acc = (preds == hard_targets).float().mean().item()\n",
    "        acc_meter.update(acc, inputs.size(0))\n",
    "\n",
    "    if scheduler is not None and cfg.scheduler == 'cosine':\n",
    "        scheduler.step()\n",
    "\n",
    "    return {'train_loss': loss_meter.avg, 'train_acc': acc_meter.avg}\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module, loader: DataLoader) -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "    loss_meter = AverageMeter()\n",
    "    acc_meter = AverageMeter()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loader:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            acc = (preds == targets).float().mean().item()\n",
    "            loss_meter.update(loss.item(), inputs.size(0))\n",
    "            acc_meter.update(acc, inputs.size(0))\n",
    "    return loss_meter.avg, acc_meter.avg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练循环封装\n",
    "封装单次实验（训练 + 验证）的流程，便于在消融与主训练中重复调用。支持：\n",
    "- 自动写入日志（DataFrame）\n",
    "- 可选 AMP、梯度裁剪、调度器\n",
    "- 训练完成后返回验证集最优指标与对应权重路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单次实验执行\n",
    "\n",
    "def run_experiment(cfg: ExperimentConfig, loaders: Dict[str, DataLoader] | None = None,\n",
    "                   track_test: bool = False, verbose: bool = True) -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "    if loaders is None:\n",
    "        loaders = build_dataloaders(cfg)\n",
    "    set_seed(cfg.seed)\n",
    "    model = build_model(cfg)\n",
    "    steps_per_epoch = len(loaders['train'])\n",
    "    optimizer, scheduler = prepare_optimizer(model, cfg, steps_per_epoch)\n",
    "    scaler = GradScaler(enabled=cfg.use_amp)\n",
    "\n",
    "    history: List[Dict[str, Any]] = []\n",
    "    best_val = 0.0\n",
    "    best_state = None\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(1, cfg.epochs + 1):\n",
    "        epoch_start = time.time()\n",
    "        train_metrics = train_one_epoch(model, loaders, cfg, optimizer, scheduler, scaler)\n",
    "        val_loss, val_acc = evaluate(model, loaders['val'])\n",
    "        record = {\n",
    "            'epoch': epoch,\n",
    "            'train_loss': train_metrics['train_loss'],\n",
    "            'train_acc': train_metrics['train_acc'],\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc,\n",
    "            'lr': optimizer.param_groups[0]['lr'],\n",
    "            'epoch_time': time.time() - epoch_start,\n",
    "        }\n",
    "        history.append(record)\n",
    "        if verbose:\n",
    "            print(f\"[{cfg.name}] Epoch {epoch:03d}/{cfg.epochs} | \"\n",
    "                  f\"train_acc={train_metrics['train_acc']:.3f} | val_acc={val_acc:.3f} | \"\n",
    "                  f\"time={record['epoch_time']:.1f}s\")\n",
    "        if val_acc > best_val:\n",
    "            best_val = val_acc\n",
    "            best_state = {\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'epoch': epoch\n",
    "            }\n",
    "\n",
    "    history_df = pd.DataFrame(history)\n",
    "    test_metrics = {'test_loss': None, 'test_acc': None}\n",
    "    if track_test:\n",
    "        model.load_state_dict(best_state['model'])\n",
    "        test_loss, test_acc = evaluate(model, loaders['test'])\n",
    "        test_metrics = {'test_loss': test_loss, 'test_acc': test_acc}\n",
    "\n",
    "    summary = {\n",
    "        'config': cfg.to_dict(),\n",
    "        'best_val_acc': best_val,\n",
    "        'best_epoch': best_state['epoch'] if best_state else None,\n",
    "        'hist': history_df,\n",
    "        **test_metrics,\n",
    "        'total_time': time.time() - start_time\n",
    "    }\n",
    "\n",
    "    if cfg.save_path:\n",
    "        torch.save(best_state, cfg.save_path)\n",
    "        summary['model_path'] = cfg.save_path\n",
    "    else:\n",
    "        summary['model_path'] = None\n",
    "\n",
    "    return history_df, summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 阶段 1：10 轮消融实验\n",
    "- **Baseline**：无残差/SE，depth=2，width=1，轻量增强，仅 SGD。\n",
    "- 单因素实验（每个 10 轮）：\n",
    "  1. `+Residual`：启用残差连接。\n",
    "  2. `+SE`：在残差分支基础上启用 SE（保持是否残差可独立开关）。\n",
    "  3. `+Deeper/Wider`：将 `depth=3`，`width=2`。\n",
    "  4. `+Augmentation`：开启 RandAugment + Cutout + RandomErasing。\n",
    "  5. `+Optimizer`：使用 AdamW + Cosine 调度 + AMP + Label Smoothing。\n",
    "- 运行后保存结果表格到 `runs/cifar10_ablation.csv`，供阶段 2 选择组合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 阶段 1：消融实验运行\n",
    "baseline_cfg = ExperimentConfig(\n",
    "    name='baseline', epochs=10, batch_size=128, base_lr=0.1,\n",
    "    optimizer='sgd', scheduler='none', label_smoothing=0.0,\n",
    "    use_amp=False, grad_clip=0.0, use_residual=False, use_se=False,\n",
    "    depth=2, width=1, use_strong_aug=False, use_mixup=False, use_cutmix=False,\n",
    "    randaugment_n=0, randaugment_m=9, weight_decay=5e-4\n",
    ")\n",
    "\n",
    "\n",
    "def clone_config(base: ExperimentConfig, name: str, **overrides) -> ExperimentConfig:\n",
    "    cfg_dict = base.to_dict()\n",
    "    cfg_dict.update(overrides)\n",
    "    cfg_dict['name'] = name\n",
    "    return ExperimentConfig(**cfg_dict)\n",
    "\n",
    "\n",
    "ablation_configs = [\n",
    "    ('baseline', baseline_cfg),\n",
    "    ('residual', clone_config(baseline_cfg, 'abl_residual', use_residual=True)),\n",
    "    ('se', clone_config(baseline_cfg, 'abl_se', use_se=True)),\n",
    "    ('deep_wide', clone_config(baseline_cfg, 'abl_deepwide', depth=3, width=2)),\n",
    "    ('augmentation', clone_config(baseline_cfg, 'abl_aug', use_strong_aug=True,\n",
    "                                  randaugment_n=2, randaugment_m=9, use_mixup=True,\n",
    "                                  use_cutmix=False, label_smoothing_for_aug=0.05)),\n",
    "    ('optimizer', clone_config(baseline_cfg, 'abl_opt', optimizer='adamw', base_lr=3e-4,\n",
    "                               scheduler='cosine', label_smoothing=0.1,\n",
    "                               use_amp=True, grad_clip=1.0))\n",
    "]\n",
    "\n",
    "RUN_ABLATION = True  # 将其改为 True 后执行本单元即可开始 6 组实验\n",
    "ablation_history: Dict[str, pd.DataFrame] = {}\n",
    "ablation_summary: List[Dict[str, Any]] = []\n",
    "\n",
    "if RUN_ABLATION:\n",
    "    total = len(ablation_configs)\n",
    "    for idx, (factor, cfg) in enumerate(ablation_configs, start=1):\n",
    "        print('=' * 80)\n",
    "        print(f'[Stage 1] 进度: {idx}/{total} | 因子: {factor} | 配置名: {cfg.name}')\n",
    "        print(f\"  residual={cfg.use_residual} | se={cfg.use_se} | depth={cfg.depth} | width={cfg.width}\")\n",
    "        print(f\"  strong_aug={cfg.use_strong_aug} | optimizer={cfg.optimizer} | lr={cfg.base_lr} | scheduler={cfg.scheduler}\")\n",
    "        hist, summary = run_experiment(cfg)\n",
    "        ablation_history[factor] = hist\n",
    "        ablation_summary.append({\n",
    "            'factor': factor,\n",
    "            'name': cfg.name,\n",
    "            'best_val_acc': summary['best_val_acc'],\n",
    "            'best_epoch': summary['best_epoch'],\n",
    "            'total_time_min': summary['total_time'] / 60.0,\n",
    "            'config': cfg.to_dict()\n",
    "        })\n",
    "        print(f\"完成 {factor} | best_val_acc={summary['best_val_acc']*100:.2f}% @ epoch {summary['best_epoch']}\")\n",
    "\n",
    "    ablation_df = pd.DataFrame(ablation_summary)\n",
    "    ablation_path = os.path.join(RESULT_DIR, 'cifar10_ablation_summary.csv')\n",
    "    ablation_df.to_csv(ablation_path, index=False)\n",
    "    print('=' * 80)\n",
    "    print(f'Ablation summary saved to {ablation_path}')\n",
    "else:\n",
    "    print('设置 RUN_ABLATION = True 并重新运行此单元以启动消融实验。')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 阶段 2：组合主要训练\n",
    "- 根据阶段 1 结果，筛选验证集准确率超过 Baseline 的因素。\n",
    "- 将这些因素合并，形成主训练配置（150 轮）。\n",
    "- 默认开启：AMP、AdamW、Cosine 调度、MixUp（若在阶段 1 中表现优秀）。\n",
    "- 保存最好模型权重到 `models/cifar10_best.pth`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 组合主训练\n",
    "FACTOR_PATCHES = {\n",
    "    'residual': {'use_residual': True},\n",
    "    'se': {'use_se': True},\n",
    "    'deep_wide': {'depth': 3, 'width': 2},\n",
    "    'augmentation': {\n",
    "        'use_strong_aug': True,\n",
    "        'randaugment_n': 2,\n",
    "        'randaugment_m': 9,\n",
    "        'use_mixup': True,\n",
    "        'use_cutmix': False,\n",
    "        'label_smoothing_for_aug': 0.05\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'optimizer': 'adamw',\n",
    "        'base_lr': 3e-4,\n",
    "        'scheduler': 'cosine',\n",
    "        'label_smoothing': 0.1,\n",
    "        'use_amp': True,\n",
    "        'grad_clip': 1.0\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def load_ablation_table() -> pd.DataFrame:\n",
    "    if 'ablation_summary' in globals() and len(ablation_summary) > 0:\n",
    "        return pd.DataFrame(ablation_summary)\n",
    "    path = os.path.join(RESULT_DIR, 'cifar10_ablation_summary.csv')\n",
    "    if os.path.exists(path):\n",
    "        return pd.read_csv(path)\n",
    "    raise FileNotFoundError('未找到消融实验结果，请先运行上一单元或读取 CSV。')\n",
    "\n",
    "\n",
    "RUN_MAIN_TRAIN = False\n",
    "main_history = None\n",
    "main_summary = None\n",
    "\n",
    "if RUN_MAIN_TRAIN:\n",
    "    ablation_df = load_ablation_table()\n",
    "    baseline_row = ablation_df[ablation_df['factor'] == 'baseline']\n",
    "    if baseline_row.empty:\n",
    "        raise ValueError('消融结果中缺少 baseline，请确认阶段 1 已成功运行。')\n",
    "    baseline_acc = baseline_row['best_val_acc'].max()\n",
    "    selected = ablation_df[ablation_df['best_val_acc'] > baseline_acc]['factor'].tolist()\n",
    "    print('Selected factors:', selected)\n",
    "\n",
    "    combined_cfg = clone_config(\n",
    "        baseline_cfg,\n",
    "        'main_training',\n",
    "        epochs=150,\n",
    "        batch_size=128,\n",
    "        save_path=os.path.join(MODEL_DIR, 'cifar10_best.pth'),\n",
    "        weight_decay=3e-4\n",
    "    )\n",
    "    for factor in selected:\n",
    "        for key, value in FACTOR_PATCHES.get(factor, {}).items():\n",
    "            setattr(combined_cfg, key, value)\n",
    "\n",
    "    if combined_cfg.use_mixup or combined_cfg.use_cutmix:\n",
    "        combined_cfg.label_smoothing_for_aug = max(combined_cfg.label_smoothing_for_aug, 0.05)\n",
    "\n",
    "    config_path = os.path.join(RESULT_DIR, 'cifar10_main_config.json')\n",
    "    with open(config_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(combined_cfg.to_dict(), f, ensure_ascii=False, indent=2)\n",
    "    print(f'Final main config saved to {config_path}')\n",
    "\n",
    "    print('Final main config:')\n",
    "    print(combined_cfg)\n",
    "    main_history, main_summary = run_experiment(combined_cfg, track_test=True)\n",
    "    main_history.to_csv(os.path.join(RESULT_DIR, 'cifar10_main_history.csv'), index=False)\n",
    "    print('Main training finished. Best val acc:', main_summary['best_val_acc'])\n",
    "else:\n",
    "    print('设置 RUN_MAIN_TRAIN = True 并运行此单元以执行 150 轮主训练。')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 结果汇总辅助函数\n",
    "ABLATION_CSV = os.path.join(RESULT_DIR, 'cifar10_ablation_summary.csv')\n",
    "MAIN_HISTORY_CSV = os.path.join(RESULT_DIR, 'cifar10_main_history.csv')\n",
    "BEST_MODEL_PATH = os.path.join(MODEL_DIR, 'cifar10_best.pth')\n",
    "\n",
    "\n",
    "def get_ablation_df() -> pd.DataFrame:\n",
    "    if 'ablation_summary' in globals() and len(ablation_summary) > 0:\n",
    "        return pd.DataFrame(ablation_summary)\n",
    "    if os.path.exists(ABLATION_CSV):\n",
    "        return pd.read_csv(ABLATION_CSV)\n",
    "    raise FileNotFoundError('缺少 ablation summary，请先运行阶段 1 单元。')\n",
    "\n",
    "\n",
    "def get_main_history() -> pd.DataFrame:\n",
    "    if 'main_history' in globals() and isinstance(main_history, pd.DataFrame):\n",
    "        return main_history\n",
    "    if os.path.exists(MAIN_HISTORY_CSV):\n",
    "        return pd.read_csv(MAIN_HISTORY_CSV)\n",
    "    raise FileNotFoundError('未找到主训练历史，请先执行阶段 2 单元。')\n",
    "\n",
    "\n",
    "def summarize_factors(ablation_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if 'best_val_acc' not in ablation_df.columns:\n",
    "        raise ValueError('ablation summary 需要包含 best_val_acc 列。')\n",
    "    baseline_acc = float(ablation_df.loc[ablation_df['factor'] == 'baseline', 'best_val_acc'].max())\n",
    "    ablation_df = ablation_df.copy()\n",
    "    ablation_df['gain_vs_baseline'] = ablation_df['best_val_acc'] - baseline_acc\n",
    "    return ablation_df.sort_values('best_val_acc', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 表格化总结\n",
    "from IPython.display import display\n",
    "\n",
    "try:\n",
    "    ablation_df = get_ablation_df()\n",
    "    ablation_summary_df = summarize_factors(ablation_df)\n",
    "    print('Ablation summary (降序)：')\n",
    "    display(ablation_summary_df[['factor', 'best_val_acc', 'gain_vs_baseline', 'best_epoch', 'total_time_min']])\n",
    "except Exception as exc:\n",
    "    print(f'无法载入消融结果: {exc}')\n",
    "    ablation_summary_df = None\n",
    "\n",
    "try:\n",
    "    main_hist_df = get_main_history()\n",
    "    best_val = main_hist_df.loc[main_hist_df['val_acc'].idxmax()]\n",
    "    main_overview = pd.DataFrame([\n",
    "        {\n",
    "            'metric': 'best_val_acc',\n",
    "            'value': best_val['val_acc'],\n",
    "            'epoch': int(best_val['epoch'])\n",
    "        },\n",
    "        {\n",
    "            'metric': 'final_val_acc',\n",
    "            'value': main_hist_df['val_acc'].iloc[-1],\n",
    "            'epoch': int(main_hist_df['epoch'].iloc[-1])\n",
    "        }\n",
    "    ])\n",
    "    print('Main training overview:')\n",
    "    display(main_overview)\n",
    "except Exception as exc:\n",
    "    print(f'无法载入主训练历史: {exc}')\n",
    "    main_hist_df = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学习曲线可视化\n",
    "if main_hist_df is not None:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    axes[0].plot(main_hist_df['epoch'], main_hist_df['train_loss'], label='train_loss')\n",
    "    axes[0].plot(main_hist_df['epoch'], main_hist_df['val_loss'], label='val_loss')\n",
    "    axes[0].set_xlabel('Epoch'); axes[0].set_ylabel('Loss'); axes[0].set_title('Loss Curve')\n",
    "    axes[0].legend(); axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    axes[1].plot(main_hist_df['epoch'], main_hist_df['train_acc'], label='train_acc')\n",
    "    axes[1].plot(main_hist_df['epoch'], main_hist_df['val_acc'], label='val_acc')\n",
    "    if 'main_summary' in globals() and main_summary is not None:\n",
    "        axes[1].axhline(main_summary['best_val_acc'], color='red', linestyle='--', label='best_val_acc')\n",
    "    axes[1].set_xlabel('Epoch'); axes[1].set_ylabel('Accuracy'); axes[1].set_title('Accuracy Curve')\n",
    "    axes[1].legend(); axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    curve_path = os.path.join(RESULT_DIR, 'cifar10_main_curves.png')\n",
    "    plt.savefig(curve_path, dpi=200)\n",
    "    print(f'学习曲线已保存到 {curve_path}')\n",
    "else:\n",
    "    print('尚未获得主训练历史，跳过绘图。')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 阶段 3：结果分析与可视化\n",
    "- 与 Baseline 对比最终模型性能。\n",
    "- 输出每个因素的贡献条形图。\n",
    "- 绘制主训练的学习曲线（accuracy/loss）。\n",
    "- （可选）计算测试集混淆矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 因素贡献可视化\n",
    "if ablation_summary_df is not None:\n",
    "    contrib_df = ablation_summary_df[ablation_summary_df['factor'] != 'baseline']\n",
    "    if not contrib_df.empty:\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.bar(contrib_df['factor'], contrib_df['gain_vs_baseline'])\n",
    "        plt.ylabel('Δ val_acc vs baseline')\n",
    "        plt.title('Factor Contribution (validation gain)')\n",
    "        plt.grid(True, axis='y', alpha=0.3)\n",
    "        for idx, val in enumerate(contrib_df['gain_vs_baseline']):\n",
    "            plt.text(idx, val, f'{val:.3f}', ha='center', va='bottom')\n",
    "        contrib_path = os.path.join(RESULT_DIR, 'cifar10_factor_gains.png')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(contrib_path, dpi=200)\n",
    "        print(f'因素贡献图已保存到 {contrib_path}')\n",
    "    else:\n",
    "        print('没有高于 baseline 的因素。')\n",
    "else:\n",
    "    print('尚无消融总结，无法绘制贡献图。')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# （可选）测试集混淆矩阵\n",
    "try:\n",
    "    from sklearn.metrics import confusion_matrix, classification_report\n",
    "except ImportError:\n",
    "    confusion_matrix = None\n",
    "    classification_report = None\n",
    "\n",
    "if os.path.exists(BEST_MODEL_PATH) and confusion_matrix is not None:\n",
    "    config_path = os.path.join(RESULT_DIR, 'cifar10_main_config.json')\n",
    "    if os.path.exists(config_path):\n",
    "        with open(config_path, 'r', encoding='utf-8') as f:\n",
    "            cfg_loaded = ExperimentConfig(**json.load(f))\n",
    "    else:\n",
    "        cfg_loaded = clone_config(baseline_cfg, 'eval')\n",
    "\n",
    "    loaders_eval = build_dataloaders(cfg_loaded)\n",
    "    model_eval = build_model(cfg_loaded)\n",
    "    state = torch.load(BEST_MODEL_PATH, map_location=device)\n",
    "    model_eval.load_state_dict(state['model'] if isinstance(state, dict) and 'model' in state else state)\n",
    "    model_eval.eval()\n",
    "\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loaders_eval['test']:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model_eval(inputs)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.numpy())\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, cmap='Blues')\n",
    "    plt.title('Confusion Matrix (Test)')\n",
    "    plt.xlabel('Predicted'); plt.ylabel('True')\n",
    "    plt.colorbar()\n",
    "    tick_labels = loaders_eval['test'].dataset.classes if hasattr(loaders_eval['test'].dataset, 'classes') else range(NUM_CLASSES)\n",
    "    plt.xticks(range(len(tick_labels)), tick_labels, rotation=45, ha='right')\n",
    "    plt.yticks(range(len(tick_labels)), tick_labels)\n",
    "    for i in range(len(tick_labels)):\n",
    "        for j in range(len(tick_labels)):\n",
    "            plt.text(j, i, cm[i, j], ha='center', va='center', color='black')\n",
    "    plt.tight_layout()\n",
    "    cm_path = os.path.join(RESULT_DIR, 'cifar10_confusion_matrix.png')\n",
    "    plt.savefig(cm_path, dpi=200)\n",
    "    plt.show()\n",
    "\n",
    "    if classification_report is not None:\n",
    "        print(classification_report(all_labels, all_preds, target_names=tick_labels))\n",
    "    print(f'混淆矩阵已保存到 {cm_path}')\n",
    "else:\n",
    "    print('缺少最佳模型或 sklearn，跳过混淆矩阵绘制。')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 消融结果备注\n",
    "运行阶段 1 后，可在此单元下方自动加载 `cifar10_ablation_summary.csv` 并生成贡献分析图。若需要重复实验，可清空 `runs/` 目录并重新运行相应单元。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 快速查看消融 CSV（可选）\n",
    "if os.path.exists(ABLATION_CSV):\n",
    "    tmp_df = pd.read_csv(ABLATION_CSV)\n",
    "    display(tmp_df)\n",
    "else:\n",
    "    print('尚未生成 ablation CSV。')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# （可选）归档当前 Notebook\n",
    "import shutil\n",
    "\n",
    "NOTEBOOK_ABS_PATH = os.path.abspath('CIFAR-10.ipynb')\n",
    "NOTEBOOK_COPY_PATH = os.path.join(RESULT_DIR, f'CIFAR10_notebook_backup.ipynb')\n",
    "try:\n",
    "    shutil.copy2(NOTEBOOK_ABS_PATH, NOTEBOOK_COPY_PATH)\n",
    "    print(f'Notebook archived to: {NOTEBOOK_COPY_PATH}')\n",
    "except Exception as exc:\n",
    "    print(f'归档失败: {exc}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验总结与分析（待根据实测结果补充）\n",
    "- Baseline：简洁 CNN + 轻量增强，10 轮即可复现课程参考精度。\n",
    "- 阶段 1：通过 5 个因素的消融实验量化增益，筛选表现优于 Baseline 的方案。\n",
    "- 阶段 2：将有效因素（如 Residual、SE、Deeper/Wider、强化增强、AdamW+Cosine 等）组合，训练 150 轮并保存最佳模型。\n",
    "- 阶段 3：对比 Baseline 与最终模型精度、绘制学习曲线与因素增益图，并可生成混淆矩阵辅助分析。\n",
    "\n",
    "> 建议：在完成全部实验后，将关键数值（最佳验证/测试准确率、各因素增益）填入课程报告的表格中，同时结合曲线与混淆矩阵撰写文字分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将消融结果导出为 Markdown（用于报告）\n",
    "if ablation_summary_df is not None:\n",
    "    md_path = os.path.join(RESULT_DIR, 'cifar10_ablation_summary.md')\n",
    "    with open(md_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(ablation_summary_df.to_markdown(index=False))\n",
    "    print(f'消融结果已导出到 {md_path}')\n",
    "else:\n",
    "    print('暂无消融数据可导出。')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 运行元数据（可选）\n",
    "meta = {\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'seed': SEED,\n",
    "    'device': str(device),\n",
    "    'best_val_acc': main_summary['best_val_acc'] if ('main_summary' in globals() and main_summary) else None,\n",
    "    'best_epoch': main_summary['best_epoch'] if ('main_summary' in globals() and main_summary) else None,\n",
    "    'artifacts': {\n",
    "        'result_dir': RESULT_DIR,\n",
    "        'model_dir': MODEL_DIR,\n",
    "        'best_model_path': BEST_MODEL_PATH if os.path.exists(BEST_MODEL_PATH) else None,\n",
    "        'ablation_csv': ABLATION_CSV if os.path.exists(ABLATION_CSV) else None,\n",
    "        'main_history_csv': MAIN_HISTORY_CSV if os.path.exists(MAIN_HISTORY_CSV) else None\n",
    "    }\n",
    "}\n",
    "meta_path = os.path.join(RESULT_DIR, 'cifar10_run_meta.json')\n",
    "with open(meta_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "print(f'元数据已写入 {meta_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 我学到了什么（反思）\n",
    "\n",
    "在本次 CIFAR-10 任务中，我从一个简洁的 CNN 出发，逐步引入以下因素并进行对比：\n",
    "\n",
    "- 残差与 SE 注意力：缓解退化并提升特征表达；\n",
    "- 更深/更宽的结构：带来容量提升但需要配合正则与调参；\n",
    "- 更强的数据增强（含 Cutout/MixUp/RandomErasing）：有效抑制过拟合；\n",
    "- AdamW 与 Warmup+Cosine：更稳定的优化与更好的最终性能；\n",
    "- Label Smoothing：在类别间相似时可提升泛化表现；\n",
    "\n",
    "结合训练曲线与消融结果，我理解到“配方”需要整体协同：增强强度、epoch 数、正则化与学习率日程彼此影响，不可孤立看待。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 快速烟囱测试（Smoke Test）\n",
    "为验证改进后的代码在本机可直接运行，这里进行一次极简训练：\n",
    "- 只训练 1 个 epoch\n",
    "- 每个 epoch 仅跑 5 个 mini-batches（max_steps_per_epoch=5）\n",
    "- 使用较小的 batch_size=64\n",
    "期望：无异常报错，输出训练与验证的基本指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 运行 Smoke Test\n",
    "cfg = ExperimentConfig(\n",
    "    name='smoke_baseline',\n",
    "    epochs=1,\n",
    "    batch_size=64,\n",
    "    base_lr=0.05,\n",
    "    optimizer='sgd',\n",
    "    scheduler='none',\n",
    "    use_amp=False,\n",
    "    max_steps_per_epoch=5,\n",
    "    use_residual=False,\n",
    "    use_se=False,\n",
    "    depth=2,\n",
    "    width=1,\n",
    "    use_strong_aug=False,\n",
    ")\n",
    "\n",
    "loaders = build_dataloaders(cfg, num_workers=0)\n",
    "hist, summary = run_experiment(cfg, loaders, track_test=False, verbose=True)\n",
    "\n",
    "print('\\n=== Smoke Test Summary ===')\n",
    "print({k: v for k, v in summary.items() if k in ['best_val_acc', 'best_epoch', 'total_time']})\n",
    "print('History (tail):')\n",
    "print(hist.tail())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
