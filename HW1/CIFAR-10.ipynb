{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results will be saved to: ./results/cifar10_20251027_103538\n",
      "{'python': '3.10.0', 'platform': 'Linux-5.15.0-139-generic-x86_64-with-glibc2.31', 'pytorch': '2.5.1+cu121', 'torchvision': '0.20.1+cu121', 'cuda_available': True, 'cuda_version': '12.1', 'device': 'cuda', 'seed': 42}\n"
     ]
    }
   ],
   "source": [
    "# 1. 环境初始化、高级配置与辅助函数\n",
    "\n",
    "# 核心库导入\n",
    "import os, sys, random, time, platform, json, math\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "\n",
    "# 科学计算与深度学习库\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from IPython.display import display\n",
    "\n",
    "# --- 初始设置 ---\n",
    "SEED = 42\n",
    "def set_seed(seed: int):\n",
    "    \"\"\"设置所有相关库的随机种子以保证可复现性\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        # cuDNN 可复现设置\n",
    "        import torch.backends.cudnn as cudnn\n",
    "        cudnn.benchmark = False\n",
    "        cudnn.deterministic = True\n",
    "\n",
    "set_seed(SEED) # 全局设置一次\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- 统一路径配置 ---\n",
    "RUN_TAG = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "RESULTS_ROOT = \"./results\"\n",
    "RESULTS_DIR = os.path.join(RESULTS_ROOT, f\"cifar10_{RUN_TAG}\")\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "print(f\"Results will be saved to: {RESULTS_DIR}\")\n",
    "\n",
    "# 统一产物路径\n",
    "BEST_MODEL_PATH = os.path.join(RESULTS_DIR, \"best_model_cifar10.pth\")\n",
    "ABLATION_CSV = os.path.join(RESULTS_DIR, \"cifar10_ablation_results.csv\")\n",
    "MAIN_HISTORY_CSV = os.path.join(RESULTS_DIR, 'cifar10_main_history.csv')\n",
    "NOTEBOOK_ABS_PATH = \"/data/zhangzhikui/githubbase/DL/HW1/CIFAR-10.ipynb\" # 请根据实际情况检查此路径\n",
    "NOTEBOOK_COPY_PATH = os.path.join(RESULTS_DIR, f\"CIFAR-10_{RUN_TAG}.ipynb\")\n",
    "NUM_CLASSES = 10\n",
    "DATA_ROOT = './data'\n",
    "\n",
    "# --- 环境信息打印 ---\n",
    "print({\n",
    "    \"python\": sys.version.split(\" \")[0],\n",
    "    \"platform\": platform.platform(),\n",
    "    \"pytorch\": torch.__version__,\n",
    "    \"torchvision\": torchvision.__version__,\n",
    "    \"cuda_available\": torch.cuda.is_available(),\n",
    "    \"cuda_version\": torch.version.cuda if torch.cuda.is_available() else None,\n",
    "    \"device\": str(device),\n",
    "    \"seed\": SEED\n",
    "})\n",
    "\n",
    "\n",
    "# --- 实验配置 ---\n",
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    name: str = 'baseline'\n",
    "    epochs: int = 10\n",
    "    batch_size: int = 128\n",
    "    base_lr: float = 0.1\n",
    "    momentum: float = 0.9\n",
    "    weight_decay: float = 5e-4\n",
    "    optimizer: str = 'sgd'            # 'sgd' or 'adamw'\n",
    "    scheduler: str = 'none'           # 'none' | 'cosine' | 'onecycle'\n",
    "    label_smoothing: float = 0.0\n",
    "    use_amp: bool = False\n",
    "    grad_clip: float = 0.0\n",
    "    max_steps_per_epoch: int = 0      # 0 表示不限制；用于快速烟囱测试\n",
    "\n",
    "    # 结构改进\n",
    "    use_residual: bool = False\n",
    "    use_se: bool = False\n",
    "    depth: int = 2                   # blocks per stage\n",
    "    width: int = 1                   # channel multiplier\n",
    "\n",
    "    # 数据增强/正则化\n",
    "    use_strong_aug: bool = False\n",
    "    randaugment_n: int = 0\n",
    "    randaugment_m: int = 9\n",
    "    use_mixup: bool = False\n",
    "    mixup_alpha: float = 0.2\n",
    "    use_cutmix: bool = False\n",
    "    cutmix_alpha: float = 1.0\n",
    "    label_smoothing_for_aug: float = 0.05\n",
    "\n",
    "    # 其他\n",
    "    save_path: str = ''\n",
    "    seed: int = SEED\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return asdict(self)\n",
    "\n",
    "# --- 辅助函数 ---\n",
    "def clone_config(base: ExperimentConfig, name: str, **overrides) -> ExperimentConfig:\n",
    "    \"\"\"克隆并覆盖配置\"\"\"\n",
    "    cfg_dict = base.to_dict()\n",
    "    cfg_dict.update(overrides)\n",
    "    cfg_dict['name'] = name\n",
    "    return ExperimentConfig(**cfg_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 数据集拆分与数据增强策略\n",
    "\n",
    "# 数据加载与增强管线\n",
    "CIFAR_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR_STD = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "\n",
    "class Cutout:\n",
    "    \"\"\"在张量图像上随机遮挡若干正方形区域。\"\"\"\n",
    "    def __init__(self, n_holes: int = 1, length: int = 16):\n",
    "        self.n_holes = n_holes\n",
    "        self.length = length\n",
    "\n",
    "    def __call__(self, img: torch.Tensor) -> torch.Tensor:\n",
    "        if not torch.is_tensor(img):\n",
    "            raise TypeError('Cutout 需要在 ToTensor 之后使用。')\n",
    "        h, w = img.size(1), img.size(2)\n",
    "        mask = torch.ones((h, w), device=img.device)\n",
    "        for _ in range(self.n_holes):\n",
    "            y = torch.randint(0, h, (1,)).item()\n",
    "            x = torch.randint(0, w, (1,)).item()\n",
    "            y1 = max(0, y - self.length // 2)\n",
    "            y2 = min(h, y + self.length // 2)\n",
    "            x1 = max(0, x - self.length // 2)\n",
    "            x2 = min(w, x + self.length // 2)\n",
    "            mask[y1:y2, x1:x2] = 0\n",
    "        mask = mask.expand_as(img)\n",
    "        return img * mask\n",
    "\n",
    "\n",
    "class TransformSubset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, indices: List[int], transform):\n",
    "        self.dataset = dataset\n",
    "        self.indices = indices\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        image, target = self.dataset[self.indices[idx]]\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "def build_transforms(cfg: ExperimentConfig):\n",
    "    train_tfms: List[Any] = [\n",
    "        T.RandomCrop(32, padding=4),\n",
    "        T.RandomHorizontalFlip(),\n",
    "    ]\n",
    "    if cfg.use_strong_aug:\n",
    "        if cfg.randaugment_n > 0:\n",
    "            train_tfms.append(T.RandAugment(cfg.randaugment_n, cfg.randaugment_m))\n",
    "        train_tfms.append(T.ColorJitter(0.3, 0.3, 0.3, 0.2))\n",
    "    train_tfms.append(T.ToTensor())\n",
    "    train_tfms.append(T.Normalize(CIFAR_MEAN, CIFAR_STD))\n",
    "    if cfg.use_strong_aug:\n",
    "        train_tfms.append(Cutout(n_holes=1, length=12))\n",
    "        train_tfms.append(T.RandomErasing(p=0.25, scale=(0.02, 0.25)))\n",
    "\n",
    "    eval_tfms = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(CIFAR_MEAN, CIFAR_STD)\n",
    "    ])\n",
    "\n",
    "    return T.Compose(train_tfms), eval_tfms\n",
    "\n",
    "\n",
    "def build_dataloaders(cfg: ExperimentConfig, num_workers: int = 0) -> Dict[str, DataLoader]:\n",
    "    # Windows + Notebook 环境下，num_workers=0 更稳妥\n",
    "    torch.manual_seed(cfg.seed) # 保证数据集划分一致\n",
    "    train_transform, eval_transform = build_transforms(cfg)\n",
    "\n",
    "    base_train = torchvision.datasets.CIFAR10(\n",
    "        root=DATA_ROOT,\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=None\n",
    "    )\n",
    "    test_dataset = torchvision.datasets.CIFAR10(\n",
    "        root=DATA_ROOT,\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=eval_transform\n",
    "    )\n",
    "\n",
    "    total_len = len(base_train)\n",
    "    val_len = 5000\n",
    "    train_len = total_len - val_len\n",
    "    generator = torch.Generator().manual_seed(0) # 固定划分\n",
    "    train_indices, val_indices = random_split(range(total_len), [train_len, val_len], generator=generator)\n",
    "\n",
    "    train_subset = TransformSubset(base_train, list(train_indices), train_transform)\n",
    "    val_subset = TransformSubset(base_train, list(val_indices), eval_transform)\n",
    "\n",
    "    loaders = {\n",
    "        'train': DataLoader(train_subset, batch_size=cfg.batch_size, shuffle=True,\n",
    "                             num_workers=num_workers, pin_memory=True),\n",
    "        'val': DataLoader(val_subset, batch_size=cfg.batch_size, shuffle=False,\n",
    "                           num_workers=num_workers, pin_memory=True),\n",
    "        'test': DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False,\n",
    "                            num_workers=num_workers, pin_memory=True)\n",
    "    }\n",
    "    print(f\"DataLoaders created: train={len(train_subset)}, val={len(val_subset)}, test={len(test_dataset)}\")\n",
    "    return loaders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 模型结构\n",
    "- **Residual**：stage 内使用残差跳连，缓解梯度消失。\n",
    "- **SE Attention**：引入通道注意力，自适应重标特征。\n",
    "- **Depth / Width**：通过 `depth` 和 `width` 超参调整网络容量。\n",
    "- 所有变化都在同一个 `TinyCIFARNet` 架构中切换，保证实验公平。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型构建\n",
    "class SEModule(nn.Module):\n",
    "    def __init__(self, ch: int, reduction: int = 16):\n",
    "        super().__init__()\n",
    "        hidden = max(ch // reduction, 4)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv1 = nn.Conv2d(ch, hidden, kernel_size=1, bias=True)\n",
    "        self.conv2 = nn.Conv2d(hidden, ch, kernel_size=1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        w = self.pool(x)\n",
    "        w = F.relu(self.conv1(w), inplace=True)\n",
    "        w = torch.sigmoid(self.conv2(w))\n",
    "        return x * w\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int, stride: int = 1,\n",
    "                 residual: bool = False, use_se: bool = False):\n",
    "        super().__init__()\n",
    "        self.residual = residual\n",
    "        self.use_se = use_se\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_ch)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
    "        self.se = SEModule(out_ch) if use_se else nn.Identity()\n",
    "        self.shortcut = None\n",
    "        if residual and (stride != 1 or in_ch != out_ch):\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_ch)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)), inplace=True)\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = self.se(out)\n",
    "        if self.residual:\n",
    "            shortcut = x if self.shortcut is None else self.shortcut(x)\n",
    "            out = out + shortcut\n",
    "        return F.relu(out, inplace=True)\n",
    "\n",
    "\n",
    "class TinyCIFARNet(nn.Module):\n",
    "    def __init__(self, cfg: ExperimentConfig):\n",
    "        super().__init__()\n",
    "        widths = [32, 64, 128]\n",
    "        widths = [w * cfg.width for w in widths]\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, widths[0], kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(widths[0]),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.stage1 = self._make_stage(widths[0], widths[0], cfg.depth,\n",
    "                                       stride=1, residual=cfg.use_residual, use_se=cfg.use_se)\n",
    "        self.stage2 = self._make_stage(widths[0], widths[1], cfg.depth,\n",
    "                                       stride=2, residual=cfg.use_residual, use_se=cfg.use_se)\n",
    "        self.stage3 = self._make_stage(widths[1], widths[2], cfg.depth,\n",
    "                                       stride=2, residual=cfg.use_residual, use_se=cfg.use_se)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(widths[2], NUM_CLASSES)\n",
    "        )\n",
    "        self._init_weights()\n",
    "\n",
    "    def _make_stage(self, in_ch: int, out_ch: int, depth: int,\n",
    "                    stride: int, residual: bool, use_se: bool) -> nn.Sequential:\n",
    "        blocks = [ConvBlock(in_ch, out_ch, stride=stride,\n",
    "                            residual=residual, use_se=use_se)]\n",
    "        for _ in range(1, depth):\n",
    "            blocks.append(ConvBlock(out_ch, out_ch, stride=1,\n",
    "                                    residual=residual, use_se=use_se))\n",
    "        return nn.Sequential(*blocks)\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def build_model(cfg: ExperimentConfig) -> nn.Module:\n",
    "    model = TinyCIFARNet(cfg)\n",
    "    return model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 训练与评估工具\n",
    "\n",
    "class AverageMeter:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.sum = 0.0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, value: float, n: int = 1):\n",
    "        self.sum += value * n\n",
    "        self.count += n\n",
    "\n",
    "    @property\n",
    "    def avg(self) -> float:\n",
    "        if self.count == 0:\n",
    "            return 0.0\n",
    "        return self.sum / self.count\n",
    "\n",
    "\n",
    "def soft_cross_entropy(logits: torch.Tensor, soft_targets: torch.Tensor) -> torch.Tensor:\n",
    "    log_probs = F.log_softmax(logits, dim=1)\n",
    "    return -(soft_targets * log_probs).sum(dim=1).mean()\n",
    "\n",
    "\n",
    "def one_hot(targets: torch.Tensor, num_classes: int, smoothing: float = 0.0) -> torch.Tensor:\n",
    "    with torch.no_grad():\n",
    "        y = torch.zeros((targets.size(0), num_classes), device=targets.device)\n",
    "        y.fill_(smoothing / (num_classes - 1))\n",
    "        y.scatter_(1, targets.unsqueeze(1), 1.0 - smoothing)\n",
    "    return y\n",
    "\n",
    "\n",
    "def prepare_optimizer(model: nn.Module, cfg: ExperimentConfig, steps_per_epoch: int):\n",
    "    if cfg.optimizer == 'adamw':\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=cfg.base_lr,\n",
    "                                weight_decay=cfg.weight_decay)\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=cfg.base_lr,\n",
    "                              momentum=cfg.momentum, weight_decay=cfg.weight_decay, nesterov=True)\n",
    "\n",
    "    scheduler = None\n",
    "    if cfg.scheduler == 'cosine':\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=cfg.epochs)\n",
    "    elif cfg.scheduler == 'onecycle':\n",
    "        scheduler = OneCycleLR(optimizer, max_lr=cfg.base_lr,\n",
    "                               steps_per_epoch=steps_per_epoch, epochs=cfg.epochs)\n",
    "    return optimizer, scheduler\n",
    "\n",
    "\n",
    "def apply_mixup_cutmix(inputs: torch.Tensor, targets: torch.Tensor, cfg: ExperimentConfig):\n",
    "    soft_targets = one_hot(targets, NUM_CLASSES, smoothing=cfg.label_smoothing_for_aug)\n",
    "    if cfg.use_mixup and cfg.mixup_alpha > 0:\n",
    "        lam = np.random.beta(cfg.mixup_alpha, cfg.mixup_alpha)\n",
    "        index = torch.randperm(inputs.size(0), device=inputs.device)\n",
    "        mixed = lam * inputs + (1 - lam) * inputs[index]\n",
    "        soft_targets = lam * soft_targets + (1 - lam) * soft_targets[index]\n",
    "        return mixed, soft_targets\n",
    "    if cfg.use_cutmix and cfg.cutmix_alpha > 0:\n",
    "        lam = np.random.beta(cfg.cutmix_alpha, cfg.cutmix_alpha)\n",
    "        batch_size, _, h, w = inputs.size()\n",
    "        index = torch.randperm(batch_size, device=inputs.device)\n",
    "        cut_rat = math.sqrt(1.0 - lam)\n",
    "        cut_w = int(w * cut_rat)\n",
    "        cut_h = int(h * cut_rat)\n",
    "        cx = np.random.randint(w)\n",
    "        cy = np.random.randint(h)\n",
    "        x1 = np.clip(cx - cut_w // 2, 0, w)\n",
    "        y1 = np.clip(cy - cut_h // 2, 0, h)\n",
    "        x2 = np.clip(cx + cut_w // 2, 0, w)\n",
    "        y2 = np.clip(cy + cut_h // 2, 0, h)\n",
    "        inputs[:, :, y1:y2, x1:x2] = inputs[index, :, y1:y2, x1:x2]\n",
    "        lam = 1 - ((x2 - x1) * (y2 - y1) / (w * h))\n",
    "        soft_targets = lam * soft_targets + (1 - lam) * soft_targets[index]\n",
    "        return inputs, soft_targets\n",
    "    return inputs, soft_targets\n",
    "\n",
    "\n",
    "def train_one_epoch(model: nn.Module, loaders: Dict[str, DataLoader], cfg: ExperimentConfig,\n",
    "                    optimizer, scheduler=None, scaler: Optional[GradScaler] = None) -> Dict[str, float]:\n",
    "    model.train()\n",
    "    loss_meter = AverageMeter()\n",
    "    acc_meter = AverageMeter()\n",
    "    train_loader = loaders['train']\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=cfg.label_smoothing)\n",
    "\n",
    "    for step, (inputs, targets) in enumerate(train_loader):\n",
    "        if cfg.max_steps_per_epoch and (step >= cfg.max_steps_per_epoch):\n",
    "            break\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        use_soft = cfg.use_mixup or cfg.use_cutmix\n",
    "        if use_soft:\n",
    "            inputs, soft_targets = apply_mixup_cutmix(inputs, targets, cfg)\n",
    "        with autocast(enabled=cfg.use_amp):\n",
    "            outputs = model(inputs)\n",
    "            if use_soft:\n",
    "                loss = soft_cross_entropy(outputs, soft_targets)\n",
    "                hard_targets = torch.argmax(soft_targets, dim=1)\n",
    "            elif cfg.label_smoothing > 0.0:\n",
    "                soft_targets = one_hot(targets, NUM_CLASSES, smoothing=cfg.label_smoothing)\n",
    "                loss = soft_cross_entropy(outputs, soft_targets)\n",
    "                hard_targets = targets\n",
    "            else:\n",
    "                loss = criterion(outputs, targets)\n",
    "                hard_targets = targets\n",
    "\n",
    "        if scaler is not None and cfg.use_amp:\n",
    "            scaler.scale(loss).backward()\n",
    "            if cfg.grad_clip > 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            if cfg.grad_clip > 0:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "            optimizer.step()\n",
    "\n",
    "        if scheduler is not None and cfg.scheduler == 'onecycle':\n",
    "            scheduler.step()\n",
    "\n",
    "        loss_meter.update(loss.item(), inputs.size(0))\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        acc = (preds == hard_targets).float().mean().item()\n",
    "        acc_meter.update(acc, inputs.size(0))\n",
    "\n",
    "    if scheduler is not None and cfg.scheduler == 'cosine':\n",
    "        scheduler.step()\n",
    "\n",
    "    return {'train_loss': loss_meter.avg, 'train_acc': acc_meter.avg}\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module, loader: DataLoader) -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "    loss_meter = AverageMeter()\n",
    "    acc_meter = AverageMeter()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loader:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            acc = (preds == targets).float().mean().item()\n",
    "            loss_meter.update(loss.item(), inputs.size(0))\n",
    "            acc_meter.update(acc, inputs.size(0))\n",
    "    return loss_meter.avg, acc_meter.avg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 训练循环封装\n",
    "封装单次实验（训练 + 验证）的流程，便于在消融与主训练中重复调用。支持：\n",
    "- 自动写入日志（DataFrame）\n",
    "- 可选 AMP、梯度裁剪、调度器\n",
    "- 训练完成后返回验证集最优指标与对应权重路径\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单次实验执行\n",
    "\n",
    "def run_experiment(cfg: ExperimentConfig, loaders: Dict[str, DataLoader] | None = None,\n",
    "                   track_test: bool = False, verbose: bool = True) -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "    if loaders is None:\n",
    "        loaders = build_dataloaders(cfg)\n",
    "    set_seed(cfg.seed)\n",
    "    model = build_model(cfg)\n",
    "    steps_per_epoch = len(loaders['train'])\n",
    "    optimizer, scheduler = prepare_optimizer(model, cfg, steps_per_epoch)\n",
    "    scaler = GradScaler(enabled=cfg.use_amp)\n",
    "\n",
    "    history: List[Dict[str, Any]] = []\n",
    "    best_val = 0.0\n",
    "    best_state = None\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(1, cfg.epochs + 1):\n",
    "        epoch_start = time.time()\n",
    "        train_metrics = train_one_epoch(model, loaders, cfg, optimizer, scheduler, scaler)\n",
    "        val_loss, val_acc = evaluate(model, loaders['val'])\n",
    "        record = {\n",
    "            'epoch': epoch,\n",
    "            'train_loss': train_metrics['train_loss'],\n",
    "            'train_acc': train_metrics['train_acc'],\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc,\n",
    "            'lr': optimizer.param_groups[0]['lr'],\n",
    "            'epoch_time': time.time() - epoch_start,\n",
    "        }\n",
    "        history.append(record)\n",
    "        if verbose:\n",
    "            print(f\"[{cfg.name}] Epoch {epoch:03d}/{cfg.epochs} | \"\n",
    "                  f\"train_acc={train_metrics['train_acc']:.3f} | val_acc={val_acc:.3f} | \"\n",
    "                  f\"time={record['epoch_time']:.1f}s\")\n",
    "        if val_acc > best_val:\n",
    "            best_val = val_acc\n",
    "            best_state = {\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'epoch': epoch\n",
    "            }\n",
    "\n",
    "    history_df = pd.DataFrame(history)\n",
    "    test_metrics = {'test_loss': None, 'test_acc': None}\n",
    "    if track_test:\n",
    "        model.load_state_dict(best_state['model'])\n",
    "        test_loss, test_acc = evaluate(model, loaders['test'])\n",
    "        test_metrics = {'test_loss': test_loss, 'test_acc': test_acc}\n",
    "\n",
    "    summary = {\n",
    "        'config': cfg.to_dict(),\n",
    "        'best_val_acc': best_val,\n",
    "        'best_epoch': best_state['epoch'] if best_state else None,\n",
    "        'hist': history_df,\n",
    "        **test_metrics,\n",
    "        'total_time': time.time() - start_time\n",
    "    }\n",
    "\n",
    "    if cfg.save_path:\n",
    "        torch.save(best_state, cfg.save_path)\n",
    "        summary['model_path'] = cfg.save_path\n",
    "    else:\n",
    "        summary['model_path'] = None\n",
    "\n",
    "    return history_df, summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 阶段 1：消融实验\n",
    "- **Baseline**：无残差/SE，depth=2，width=1，轻量增强，仅 SGD。\n",
    "- 单因素实验（每个 10 轮）：\n",
    "  1. `+Residual`：启用残差连接。\n",
    "  2. `+SE`：在残差分支基础上启用 SE（保持是否残差可独立开关）。\n",
    "  3. `+Deeper/Wider`：将 `depth=3`，`width=2`。\n",
    "  4. `+Augmentation`：开启 RandAugment + Cutout + RandomErasing。\n",
    "  5. `+Optimizer`：使用 AdamW + Cosine 调度 + AMP + Label Smoothing。\n",
    "- 运行后保存结果表格到 `results/cifar10_.../cifar10_ablation_summary.csv`，供阶段 2 选择组合。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "[Stage 1] 进度: 1/6 | 因子: baseline | 配置名: baseline\n",
      "  residual=False | se=False | depth=2 | width=1\n",
      "  strong_aug=False | optimizer=sgd | lr=0.1 | scheduler=none\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "DataLoaders created: train=45000, val=5000, test=10000\n",
      "DataLoaders created: train=45000, val=5000, test=10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3410959/1658897150.py:11: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=cfg.use_amp)\n",
      "/tmp/ipykernel_3410959/589816504.py:98: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=cfg.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[baseline] Epoch 001/10 | train_acc=0.425 | val_acc=0.449 | time=26.2s\n",
      "[baseline] Epoch 002/10 | train_acc=0.641 | val_acc=0.625 | time=25.0s\n",
      "[baseline] Epoch 002/10 | train_acc=0.641 | val_acc=0.625 | time=25.0s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  residual=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg\u001b[38;5;241m.\u001b[39muse_residual\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | se=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg\u001b[38;5;241m.\u001b[39muse_se\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | depth=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg\u001b[38;5;241m.\u001b[39mdepth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | width=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg\u001b[38;5;241m.\u001b[39mwidth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  strong_aug=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg\u001b[38;5;241m.\u001b[39muse_strong_aug\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | optimizer=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | lr=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg\u001b[38;5;241m.\u001b[39mbase_lr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | scheduler=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m hist, summary \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m ablation_history[factor] \u001b[38;5;241m=\u001b[39m hist\n\u001b[1;32m     39\u001b[0m ablation_summary\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfactor\u001b[39m\u001b[38;5;124m'\u001b[39m: factor,\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: cfg\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m'\u001b[39m: cfg\u001b[38;5;241m.\u001b[39mto_dict()\n\u001b[1;32m     46\u001b[0m })\n",
      "Cell \u001b[0;32mIn[23], line 20\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(cfg, loaders, track_test, verbose)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, cfg\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     19\u001b[0m     epoch_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 20\u001b[0m     train_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m evaluate(model, loaders[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     22\u001b[0m     record \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch,\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: train_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch_time\u001b[39m\u001b[38;5;124m'\u001b[39m: time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m epoch_start,\n\u001b[1;32m     30\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[22], line 119\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, loaders, cfg, optimizer, scheduler, scaler)\u001b[0m\n\u001b[1;32m    117\u001b[0m     scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 119\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mgrad_clip \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    121\u001b[0m         nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), cfg\u001b[38;5;241m.\u001b[39mgrad_clip)\n",
      "File \u001b[0;32m~/enter/envs/dl/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/enter/envs/dl/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/enter/envs/dl/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 阶段 1：消融实验运行\n",
    "baseline_cfg = ExperimentConfig(\n",
    "    name='baseline', epochs=50, batch_size=128, base_lr=0.1,\n",
    "    optimizer='sgd', scheduler='none', label_smoothing=0.0,\n",
    "    use_amp=False, grad_clip=0.0, use_residual=False, use_se=False,\n",
    "    depth=2, width=1, use_strong_aug=False, use_mixup=False, use_cutmix=False,\n",
    "    randaugment_n=0, randaugment_m=9, weight_decay=5e-4\n",
    ")\n",
    "\n",
    "ablation_configs = [\n",
    "    ('baseline', baseline_cfg),\n",
    "    ('residual', clone_config(baseline_cfg, 'abl_residual', use_residual=True)),\n",
    "    ('se', clone_config(baseline_cfg, 'abl_se', use_se=True)),\n",
    "    ('deep_wide', clone_config(baseline_cfg, 'abl_deepwide', depth=3, width=2)),\n",
    "    ('augmentation', clone_config(baseline_cfg, 'abl_aug', use_strong_aug=True,\n",
    "                                  randaugment_n=2, randaugment_m=9, use_mixup=True,\n",
    "                                  use_cutmix=False, label_smoothing_for_aug=0.05)),\n",
    "    ('optimizer', clone_config(baseline_cfg, 'abl_opt', optimizer='adamw', base_lr=3e-4,\n",
    "                               scheduler='cosine', label_smoothing=0.1,\n",
    "                               use_amp=True, grad_clip=1.0))\n",
    "]\n",
    "\n",
    "# --- 控制开关 ---\n",
    "RUN_ABLATION = True  # 将其改为 True 后执行本单元即可开始 6 组实验\n",
    "# ---\n",
    "\n",
    "ablation_history: Dict[str, pd.DataFrame] = {}\n",
    "ablation_summary: List[Dict[str, Any]] = []\n",
    "\n",
    "if RUN_ABLATION:\n",
    "    total = len(ablation_configs)\n",
    "    for idx, (factor, cfg) in enumerate(ablation_configs, start=1):\n",
    "        print('=' * 80)\n",
    "        print(f'[Stage 1] 进度: {idx}/{total} | 因子: {factor} | 配置名: {cfg.name}')\n",
    "        print(f\"  residual={cfg.use_residual} | se={cfg.use_se} | depth={cfg.depth} | width={cfg.width}\")\n",
    "        print(f\"  strong_aug={cfg.use_strong_aug} | optimizer={cfg.optimizer} | lr={cfg.base_lr} | scheduler={cfg.scheduler}\")\n",
    "        hist, summary = run_experiment(cfg)\n",
    "        ablation_history[factor] = hist\n",
    "        ablation_summary.append({\n",
    "            'factor': factor,\n",
    "            'name': cfg.name,\n",
    "            'best_val_acc': summary['best_val_acc'],\n",
    "            'best_epoch': summary['best_epoch'],\n",
    "            'total_time_min': summary['total_time'] / 60.0,\n",
    "            'config': cfg.to_dict()\n",
    "        })\n",
    "        print(f\"完成 {factor} | best_val_acc={summary['best_val_acc']*100:.2f}% @ epoch {summary['best_epoch']}\")\n",
    "\n",
    "    ablation_df = pd.DataFrame(ablation_summary)\n",
    "    ablation_df.to_csv(ABLATION_CSV, index=False)\n",
    "    print('=' * 80)\n",
    "    print(f'Ablation summary saved to {ABLATION_CSV}')\n",
    "else:\n",
    "    print('设置 RUN_ABLATION = True 并重新运行此单元以启动消融实验。')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 阶段 2：组合主要训练\n",
    "- 根据阶段 1 结果，筛选验证集准确率超过 Baseline 的因素。\n",
    "- 将这些因素合并，形成主训练配置（150 轮）。\n",
    "- 默认开启：AMP、AdamW、Cosine 调度、MixUp（若在阶段 1 中表现优秀）。\n",
    "- 保存最好模型权重到 `results/.../best_model_cifar10.pth`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 组合主训练\n",
    "FACTOR_PATCHES = {\n",
    "    'residual': {'use_residual': True},\n",
    "    'se': {'use_se': True},\n",
    "    'deep_wide': {'depth': 3, 'width': 2},\n",
    "    'augmentation': {\n",
    "        'use_strong_aug': True,\n",
    "        'randaugment_n': 2,\n",
    "        'randaugment_m': 9,\n",
    "        'use_mixup': True,\n",
    "        'use_cutmix': False,\n",
    "        'label_smoothing_for_aug': 0.05\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'optimizer': 'adamw',\n",
    "        'base_lr': 3e-4,\n",
    "        'scheduler': 'cosine',\n",
    "        'label_smoothing': 0.1,\n",
    "        'use_amp': True,\n",
    "        'grad_clip': 1.0\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def load_ablation_table() -> pd.DataFrame:\n",
    "    if 'ablation_summary' in globals() and len(ablation_summary) > 0:\n",
    "        return pd.DataFrame(ablation_summary)\n",
    "    if os.path.exists(ABLATION_CSV):\n",
    "        return pd.read_csv(ABLATION_CSV)\n",
    "    raise FileNotFoundError(f'未找到消融实验结果 {ABLATION_CSV}，请先运行上一单元或读取 CSV。')\n",
    "\n",
    "# --- 控制开关 ---\n",
    "RUN_MAIN_TRAIN = True\n",
    "# ---\n",
    "\n",
    "main_history = None\n",
    "main_summary = None\n",
    "\n",
    "if RUN_MAIN_TRAIN:\n",
    "    ablation_df = load_ablation_table()\n",
    "    baseline_row = ablation_df[ablation_df['factor'] == 'baseline']\n",
    "    if baseline_row.empty:\n",
    "        raise ValueError('消融结果中缺少 baseline，请确认阶段 1 已成功运行。')\n",
    "    baseline_acc = baseline_row['best_val_acc'].max()\n",
    "    selected = ablation_df[ablation_df['best_val_acc'] > baseline_acc]['factor'].tolist()\n",
    "    print('Selected factors with gain > baseline:', selected)\n",
    "\n",
    "    combined_cfg = clone_config(\n",
    "        baseline_cfg,\n",
    "        'main_training',\n",
    "        epochs=150,\n",
    "        batch_size=128,\n",
    "        save_path=BEST_MODEL_PATH,\n",
    "        weight_decay=3e-4\n",
    "    )\n",
    "    for factor in selected:\n",
    "        for key, value in FACTOR_PATCHES.get(factor, {}).items():\n",
    "            setattr(combined_cfg, key, value)\n",
    "\n",
    "    if combined_cfg.use_mixup or combined_cfg.use_cutmix:\n",
    "        combined_cfg.label_smoothing_for_aug = max(combined_cfg.label_smoothing_for_aug, 0.05)\n",
    "\n",
    "    config_path = os.path.join(RESULTS_DIR, 'cifar10_main_config.json')\n",
    "    with open(config_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(combined_cfg.to_dict(), f, ensure_ascii=False, indent=2)\n",
    "    print(f'Final main config saved to {config_path}')\n",
    "\n",
    "    print('Final main config:')\n",
    "    print(combined_cfg)\n",
    "    main_history, main_summary = run_experiment(combined_cfg, track_test=True)\n",
    "    main_history.to_csv(MAIN_HISTORY_CSV, index=False)\n",
    "    print('Main training finished. Best val acc:', main_summary['best_val_acc'])\n",
    "else:\n",
    "    print('设置 RUN_MAIN_TRAIN = True 并运行此单元以执行 150 轮主训练。')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. 结果分析与可视化\n",
    "import matplotlib.pyplot as plt\n",
    "# 结果汇总辅助函数\n",
    "def get_ablation_df() -> pd.DataFrame:\n",
    "    if 'ablation_summary' in globals() and len(ablation_summary) > 0:\n",
    "        return pd.DataFrame(ablation_summary)\n",
    "    if os.path.exists(ABLATION_CSV):\n",
    "        return pd.read_csv(ABLATION_CSV)\n",
    "    raise FileNotFoundError(f'缺少 ablation summary {ABLATION_CSV}，请先运行阶段 1 单元。')\n",
    "\n",
    "\n",
    "def get_main_history() -> pd.DataFrame:\n",
    "    if 'main_history' in globals() and isinstance(main_history, pd.DataFrame):\n",
    "        return main_history\n",
    "    if os.path.exists(MAIN_HISTORY_CSV):\n",
    "        return pd.read_csv(MAIN_HISTORY_CSV)\n",
    "    raise FileNotFoundError(f'未找到主训练历史 {MAIN_HISTORY_CSV}，请先执行阶段 2 单元。')\n",
    "\n",
    "\n",
    "def summarize_factors(ablation_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if 'best_val_acc' not in ablation_df.columns:\n",
    "        raise ValueError('ablation summary 需要包含 best_val_acc 列。')\n",
    "    baseline_acc = float(ablation_df.loc[ablation_df['factor'] == 'baseline', 'best_val_acc'].max())\n",
    "    ablation_df = ablation_df.copy()\n",
    "    ablation_df['gain_vs_baseline'] = ablation_df['best_val_acc'] - baseline_acc\n",
    "    return ablation_df.sort_values('best_val_acc', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 表格化总结\n",
    "from IPython.display import display\n",
    "\n",
    "try:\n",
    "    ablation_df = get_ablation_df()\n",
    "    ablation_summary_df = summarize_factors(ablation_df)\n",
    "    print('Ablation summary (降序)：')\n",
    "    display(ablation_summary_df[['factor', 'best_val_acc', 'gain_vs_baseline', 'best_epoch', 'total_time_min']])\n",
    "except Exception as exc:\n",
    "    print(f'无法载入消融结果: {exc}')\n",
    "    ablation_summary_df = None\n",
    "\n",
    "try:\n",
    "    main_hist_df = get_main_history()\n",
    "    best_val = main_hist_df.loc[main_hist_df['val_acc'].idxmax()]\n",
    "    main_overview = pd.DataFrame([\n",
    "        {\n",
    "            'metric': 'best_val_acc',\n",
    "            'value': best_val['val_acc'],\n",
    "            'epoch': int(best_val['epoch'])\n",
    "        },\n",
    "        {\n",
    "            'metric': 'final_val_acc',\n",
    "            'value': main_hist_df['val_acc'].iloc[-1],\n",
    "            'epoch': int(main_hist_df['epoch'].iloc[-1])\n",
    "        }\n",
    "    ])\n",
    "    print('Main training overview:')\n",
    "    display(main_overview)\n",
    "except Exception as exc:\n",
    "    print(f'无法载入主训练历史: {exc}')\n",
    "    main_hist_df = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学习曲线可视化\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 尝试加载数据，使该单元格可以独立运行\n",
    "try:\n",
    "    main_hist_df = get_main_history()\n",
    "except FileNotFoundError:\n",
    "    main_hist_df = None\n",
    "\n",
    "if main_hist_df is not None:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    axes[0].plot(main_hist_df['epoch'], main_hist_df['train_loss'], label='train_loss')\n",
    "    axes[0].plot(main_hist_df['epoch'], main_hist_df['val_loss'], label='val_loss')\n",
    "    axes[0].set_xlabel('Epoch'); axes[0].set_ylabel('Loss'); axes[0].set_title('Loss Curve')\n",
    "    axes[0].legend(); axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    axes[1].plot(main_hist_df['epoch'], main_hist_df['train_acc'], label='train_acc')\n",
    "    axes[1].plot(main_hist_df['epoch'], main_hist_df['val_acc'], label='val_acc')\n",
    "    \n",
    "    # 尝试获取 main_summary 以绘制最佳准确率线\n",
    "    try:\n",
    "        if 'main_summary' in globals() and main_summary is not None:\n",
    "             axes[1].axhline(main_summary['best_val_acc'], color='red', linestyle='--', label=f\"best_val_acc ({main_summary['best_val_acc']:.3f})\")\n",
    "    except Exception:\n",
    "        pass # 如果 main_summary 不可用，则不绘制\n",
    "\n",
    "    axes[1].set_xlabel('Epoch'); axes[1].set_ylabel('Accuracy'); axes[1].set_title('Accuracy Curve')\n",
    "    axes[1].legend(); axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    curve_path = os.path.join(RESULTS_DIR, 'cifar10_main_curves.png')\n",
    "    plt.savefig(curve_path, dpi=200)\n",
    "    print(f'学习曲线已保存到 {curve_path}')\n",
    "else:\n",
    "    print('尚未获得主训练历史 (main_history.csv)，跳过绘图。请先运行阶段2的训练。')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 因素贡献可视化\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "try:\n",
    "    ablation_df_plot = get_ablation_df()\n",
    "    contrib_df = summarize_factors(ablation_df_plot)\n",
    "    contrib_df = contrib_df[contrib_df['factor'] != 'baseline']\n",
    "    \n",
    "    if not contrib_df.empty:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        sns.barplot(x='factor', y='gain_vs_baseline', data=contrib_df, palette='viridis')\n",
    "        plt.ylabel('Δ val_acc vs baseline')\n",
    "        plt.title('Factor Contribution (Validation Gain)')\n",
    "        plt.grid(True, axis='y', alpha=0.3)\n",
    "        for index, row in contrib_df.iterrows():\n",
    "            plt.text(row.name, row.gain_vs_baseline, f'{row.gain_vs_baseline:.3f}', \n",
    "                     color='black', ha=\"center\", va='bottom')\n",
    "        \n",
    "        contrib_path = os.path.join(RESULTS_DIR, 'cifar10_factor_gains.png')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(contrib_path, dpi=200)\n",
    "        print(f'因素贡献图已保存到 {contrib_path}')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('没有高于 baseline 的因素。')\n",
    "except Exception as e:\n",
    "    print(f'尚无消融总结，无法绘制贡献图: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# （可选）测试集混淆矩阵\n",
    "try:\n",
    "    from sklearn.metrics import confusion_matrix, classification_report\n",
    "except ImportError:\n",
    "    confusion_matrix = None\n",
    "    classification_report = None\n",
    "\n",
    "if os.path.exists(BEST_MODEL_PATH) and confusion_matrix is not None:\n",
    "    config_path = os.path.join(RESULTS_DIR, 'cifar10_main_config.json')\n",
    "    if os.path.exists(config_path):\n",
    "        with open(config_path, 'r', encoding='utf-8') as f:\n",
    "            cfg_loaded = ExperimentConfig(**json.load(f))\n",
    "    else:\n",
    "        # 如果找不到配置文件，用一个合理的默认值来加载模型结构\n",
    "        cfg_loaded = clone_config(baseline_cfg, 'eval', use_residual=True, use_se=True, depth=3, width=2)\n",
    "\n",
    "    loaders_eval = build_dataloaders(cfg_loaded, num_workers=0)\n",
    "    model_eval = build_model(cfg_loaded)\n",
    "    state = torch.load(BEST_MODEL_PATH, map_location=device)\n",
    "    model_eval.load_state_dict(state['model'] if isinstance(state, dict) and 'model' in state else state)\n",
    "    model_eval.eval()\n",
    "\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loaders_eval['test']:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model_eval(inputs)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.numpy())\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    tick_labels = loaders_eval['test'].dataset.classes if hasattr(loaders_eval['test'].dataset, 'classes') else range(NUM_CLASSES)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=tick_labels, yticklabels=tick_labels)\n",
    "    plt.title('Confusion Matrix (Test Set)')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    \n",
    "    cm_path = os.path.join(RESULTS_DIR, 'cifar10_confusion_matrix.png')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(cm_path, dpi=200)\n",
    "    plt.show()\n",
    "\n",
    "    if classification_report is not None:\n",
    "        print(classification_report(all_labels, all_preds, target_names=tick_labels))\n",
    "    print(f'混淆矩阵已保存到 {cm_path}')\n",
    "else:\n",
    "    print(f'缺少最佳模型 ({BEST_MODEL_PATH}) 或 sklearn，跳过混淆矩阵绘制。')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. 快速检查与归档\n",
    "\n",
    "# 快速查看消融 CSV（可选）\n",
    "if os.path.exists(ABLATION_CSV):\n",
    "    tmp_df = pd.read_csv(ABLATION_CSV)\n",
    "    display(tmp_df)\n",
    "else:\n",
    "    print('尚未生成 ablation CSV。')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# （可选）归档当前 Notebook\n",
    "import shutil\n",
    "\n",
    "try:\n",
    "    shutil.copy2(NOTEBOOK_ABS_PATH, NOTEBOOK_COPY_PATH)\n",
    "    print(f'Notebook archived to: {NOTEBOOK_COPY_PATH}')\n",
    "except Exception as exc:\n",
    "    print(f'归档失败: {exc}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 实验总结与分析（待根据实测结果补充）\n",
    "- **阶段 1：消融实验** 通过 5 个因素的消融实验量化增益，筛选表现优于 Baseline 的方案。\n",
    "- **阶段 2：组合训练** 将有效因素（如 Residual、SE、Deeper/Wider、强化增强、AdamW+Cosine 等）组合，训练 150 轮并保存最佳模型。\n",
    "- **阶段 3：结果分析** 对比 Baseline 与最终模型精度、绘制学习曲线与因素增益图，并可生成混淆矩阵辅助分析。\n",
    "\n",
    "> 建议：在完成全部实验后，将关键数值（最佳验证/测试准确率、各因素增益）填入课程报告的表格中，同时结合曲线与混淆矩阵撰写文字分析。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将消融结果导出为 Markdown（用于报告）\n",
    "if ablation_summary_df is not None:\n",
    "    md_path = os.path.join(RESULTS_DIR, 'cifar10_ablation_summary.md')\n",
    "    with open(md_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(ablation_summary_df.to_markdown(index=False))\n",
    "    print(f'消融结果已导出到 {md_path}')\n",
    "else:\n",
    "    print('暂无消融数据可导出。')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 运行元数据（可选）\n",
    "meta = {\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'seed': SEED,\n",
    "    'device': str(device),\n",
    "    'best_val_acc': main_summary['best_val_acc'] if ('main_summary' in globals() and main_summary) else None,\n",
    "    'best_epoch': main_summary['best_epoch'] if ('main_summary' in globals() and main_summary) else None,\n",
    "    'artifacts': {\n",
    "        'result_dir': RESULTS_DIR,\n",
    "        'best_model_path': BEST_MODEL_PATH if os.path.exists(BEST_MODEL_PATH) else None,\n",
    "        'ablation_csv': ABLATION_CSV if os.path.exists(ABLATION_CSV) else None,\n",
    "        'main_history_csv': MAIN_HISTORY_CSV if os.path.exists(MAIN_HISTORY_CSV) else None\n",
    "    }\n",
    "}\n",
    "meta_path = os.path.join(RESULTS_DIR, 'cifar10_run_meta.json')\n",
    "with open(meta_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "print(f'元数据已写入 {meta_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 我学到了什么（反思）\n",
    "\n",
    "在本次 CIFAR-10 任务中，我从一个简洁的 CNN 出发，逐步引入以下因素并进行对比：\n",
    "\n",
    "- 残差与 SE 注意力：缓解退化并提升特征表达；\n",
    "- 更深/更宽的结构：带来容量提升但需要配合正则与调参；\n",
    "- 更强的数据增强（含 Cutout/MixUp/RandomErasing）：有效抑制过拟合；\n",
    "- AdamW 与 Warmup+Cosine：更稳定的优化与更好的最终性能；\n",
    "- Label Smoothing：在类别间相似时可提升泛化表现；\n",
    "\n",
    "结合训练曲线与消融结果，我理解到“配方”需要整体协同：增强强度、epoch 数、正则化与学习率日程彼此影响，不可孤立看待。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 快速烟囱测试（Smoke Test）\n",
    "为验证改进后的代码在本机可直接运行，这里进行一次极简训练：\n",
    "- 只训练 1 个 epoch\n",
    "- 每个 epoch 仅跑 5 个 mini-batches（max_steps_per_epoch=5）\n",
    "- 使用较小的 batch_size=64\n",
    "期望：无异常报错，输出训练与验证的基本指标。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 运行 Smoke Test\n",
    "cfg = ExperimentConfig(\n",
    "    name='smoke_baseline',\n",
    "    epochs=1,\n",
    "    batch_size=64,\n",
    "    base_lr=0.05,\n",
    "    optimizer='sgd',\n",
    "    scheduler='none',\n",
    "    use_amp=False,\n",
    "    max_steps_per_epoch=5,\n",
    "    use_residual=False,\n",
    "    use_se=False,\n",
    "    depth=2,\n",
    "    width=1,\n",
    "    use_strong_aug=False,\n",
    ")\n",
    "\n",
    "loaders = build_dataloaders(cfg, num_workers=0)\n",
    "hist, summary = run_experiment(cfg, loaders, track_test=False, verbose=True)\n",
    "\n",
    "print('\\n=== Smoke Test Summary ===')\n",
    "print({k: v for k, v in summary.items() if k in ['best_val_acc', 'best_epoch', 'total_time']})\n",
    "print('History (tail):')\n",
    "print(hist.tail())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
