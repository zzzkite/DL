#!/usr/bin/env python3
"""
ControlNetè®­ç»ƒåæ¨ç†è„šæœ¬
ç»“åˆæœ¬åœ°Stable Diffusionç”Ÿæˆå›¾åƒå¹¶ä¸åŸå§‹æ•°æ®å¯¹æ¯”
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import cv2
import numpy as np
from pathlib import Path
import matplotlib.pyplot as plt
import sys
import os
import json
import pandas as pd
from PIL import Image
from skimage.metrics import structural_similarity as ssim
from skimage.metrics import peak_signal_noise_ratio as psnr

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.append(str(project_root))

try:
    from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler
    from transformers import CLIPTokenizer, CLIPTextModel
    print("âœ… æˆåŠŸå¯¼å…¥Diffuserså’ŒTransformersæ¨¡å—")
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·å®‰è£…: pip install diffusers transformers")
    sys.exit(1)

# ==================== æ—¶åºç‰¹å¾æå–å™¨ï¼ˆä¸è®­ç»ƒæ—¶ç›¸åŒï¼‰ ====================

class EnhancedTemporalFeatureExtractor(nn.Module):
    """å¢å¼ºç‰ˆæ—¶åºç‰¹å¾æå–å™¨ - ä¸“é—¨å¤„ç†20å¸§â†’25å¸§é¢„æµ‹"""
    
    def __init__(self, input_channels=3, feature_channels=32):
        super().__init__()
        
        # è¿åŠ¨è½¨è¿¹é¢„æµ‹ç½‘ç»œ
        self.trajectory_predictor = nn.Sequential(
            nn.Conv3d(input_channels, 16, (5, 3, 3), padding=(2, 1, 1)),
            nn.ReLU(),
            nn.AdaptiveAvgPool3d((8, None, None)),
            nn.Conv3d(16, 8, (3, 3, 3), padding=(1, 1, 1)),
            nn.ReLU(),
            nn.Conv3d(8, 4, (3, 3, 3), padding=(1, 1, 1)),
            nn.ReLU()
        )
        
        # å…‰æµç‰¹å¾æå–
        self.flow_processor = nn.Sequential(
            nn.Conv2d(2, 16, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(16, 8, 3, padding=1),
            nn.ReLU()
        )
        
        # ç‰¹å¾èåˆç½‘ç»œ
        self.feature_fusion = nn.Sequential(
            nn.Conv2d(12, 32, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 16, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(16, 3, 3, padding=1),
            nn.Tanh()
        )
        
    def compute_optical_flow_sequence(self, frames):
        """è®¡ç®—è¿ç»­å…‰æµåºåˆ—"""
        batch_size, num_frames, C, H, W = frames.shape
        flow_sequences = []
        
        for b in range(batch_size):
            frame_flows = []
            for t in range(num_frames - 1):
                prev_frame = (frames[b, t].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)
                curr_frame = (frames[b, t+1].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)
                
                prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_RGB2GRAY)
                curr_gray = cv2.cvtColor(curr_frame, cv2.COLOR_RGB2GRAY)
                
                # è®¡ç®—å…‰æµ
                flow = cv2.calcOpticalFlowFarneback(
                    prev_gray, curr_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0
                )
                
                # è½¬æ¢ä¸ºå¼ é‡
                flow_tensor = torch.from_numpy(flow).permute(2, 0, 1).float()
                frame_flows.append(flow_tensor)
            
            if frame_flows:
                flow_sequence = torch.stack(frame_flows)
            else:
                flow_sequence = torch.zeros(num_frames-1, 2, H, W)
            flow_sequences.append(flow_sequence)
        
        return torch.stack(flow_sequences).to(frames.device)
    
    def predict_future_motion(self, flows):
        """é¢„æµ‹æœªæ¥è¿åŠ¨è¶‹åŠ¿"""
        batch_size, seq_len, C, H, W = flows.shape
        
        # ä½¿ç”¨æœ€è¿‘å‡ å¸§çš„è¿åŠ¨æ¥é¢„æµ‹æœªæ¥è¶‹åŠ¿
        recent_flows = flows[:, -4:]
        
        # è®¡ç®—è¿åŠ¨åŠ é€Ÿåº¦
        if recent_flows.shape[1] >= 3:
            flow_acceleration = recent_flows[:, -1] - recent_flows[:, -2]
        else:
            flow_acceleration = torch.zeros_like(recent_flows[:, -1])
        
        # é¢„æµ‹æœªæ¥è¿åŠ¨
        last_flow = recent_flows[:, -1]
        predicted_flow = last_flow + flow_acceleration * 1.2
        
        return predicted_flow
    
    def forward(self, frames):
        """æå–æ—¶åºç‰¹å¾"""
        batch_size, num_frames, C, H, W = frames.shape
        
        # 1. è®¡ç®—å…‰æµåºåˆ—
        flow_sequence = self.compute_optical_flow_sequence(frames)
        
        # 2. é¢„æµ‹æœªæ¥è¿åŠ¨
        predicted_flow = self.predict_future_motion(flow_sequence)
        
        # 3. å¤„ç†å…‰æµç‰¹å¾
        flow_features = self.flow_processor(predicted_flow)
        
        # 4. è½¨è¿¹ç‰¹å¾æå–
        trajectory_input = frames.permute(0, 2, 1, 3, 4)
        trajectory_features = self.trajectory_predictor(trajectory_input)
        trajectory_features = trajectory_features.mean(dim=2)
        
        # 5. ç‰¹å¾èåˆ
        combined_features = torch.cat([flow_features, trajectory_features], dim=1)
        enhanced_control = self.feature_fusion(combined_features)
        
        return enhanced_control

# ==================== æ•°æ®åŠ è½½å™¨ ====================

class FramePredictionDataset:
    """ç®€åŒ–ç‰ˆæ•°æ®åŠ è½½å™¨ï¼Œç”¨äºåŠ è½½æµ‹è¯•æ•°æ®"""
    
    def __init__(self, metadata_file, task_name=None):
        self.metadata = pd.read_csv(metadata_file)
        
        if task_name and task_name != 'all':
            self.metadata = self.metadata[self.metadata['category'] == task_name]
        
        print(f"âœ… åŠ è½½æ¨ç†æ•°æ®é›†: {len(self.metadata)} ä¸ªæ ·æœ¬")
    
    def __len__(self):
        return len(self.metadata)
    
    def __getitem__(self, idx):
        sample = self.metadata.iloc[idx]
        
        try:
            # åŠ è½½è¾“å…¥å¸§ï¼ˆå‰20å¸§ï¼‰
            input_frames = np.load(sample['input_frames_path'])
            # åŠ è½½ç›®æ ‡å¸§ï¼ˆç¬¬25å¸§ï¼‰
            target_frame = np.load(sample['target_frame_path'])
            
            # è½¬æ¢ä¸ºPyTorchå¼ é‡
            input_frames = torch.from_numpy(input_frames).float()
            target_frame = torch.from_numpy(target_frame).float()
            
            # è°ƒæ•´ç»´åº¦é¡ºåº
            input_frames = input_frames.permute(0, 3, 1, 2)
            target_frame = target_frame.permute(2, 0, 1)
            
            # å½’ä¸€åŒ–
            input_frames = input_frames / 255.0
            target_frame = target_frame / 255.0
            
            return {
                'input_frames': input_frames,
                'target_frame': target_frame,
                'category': sample['category'],
                'video_id': sample['video_id'],
                'template': sample['template'],
                'label_text': sample['label']
            }
            
        except Exception as e:
            print(f"âŒ åŠ è½½æ ·æœ¬å¤±è´¥ {sample['video_id']}: {e}")
            return None

# ==================== ControlNetæ¨ç†å™¨ ====================

class ControlNetSDInference:
    """ControlNet + Stable Diffusion æ¨ç†å™¨"""
    
    def __init__(self, sd_model_path, controlnet_model_path, task_name, device="cuda"):
        self.device = device
        self.task_name = task_name
        
        print(f"ğŸ¯ åˆå§‹åŒ– {task_name} ä»»åŠ¡æ¨ç†ç®¡é“...")
        
        # 1. åŠ è½½è®­ç»ƒå¥½çš„ControlNet
        print("ğŸ“¦ åŠ è½½è®­ç»ƒå¥½çš„ControlNet...")
        checkpoint = torch.load(controlnet_model_path, map_location='cpu')
        
        # 2. åŠ è½½æœ¬åœ°Stable Diffusion 1.5
        print("ğŸ“¦ åŠ è½½æœ¬åœ°Stable Diffusion 1.5...")
        self.pipe = StableDiffusionControlNetPipeline.from_pretrained(
            sd_model_path,
            torch_dtype=torch.float16,
            safety_checker=None,
            requires_safety_checker=False,
            local_files_only=True
        )
        
        # 3. åˆ›å»ºå¹¶åŠ è½½ControlNetæƒé‡
        controlnet = ControlNetModel.from_unet(self.pipe.unet)
        controlnet.load_state_dict(checkpoint['controlnet_state_dict'])
        
        # 4. å°†ControlNetæ·»åŠ åˆ°ç®¡é“
        self.pipe.controlnet = controlnet
        self.pipe.scheduler = UniPCMultistepScheduler.from_config(self.pipe.scheduler.config)
        self.pipe = self.pipe.to(device)
        
        # 5. è·å–è®­ç»ƒå‚æ•°
        self.conditioning_scale = checkpoint.get('current_scale', 1.0)
        self.training_config = checkpoint.get('config', {})
        
        # 6. åŠ è½½æ—¶åºç‰¹å¾æå–å™¨
        self.temporal_extractor = EnhancedTemporalFeatureExtractor().to(device)
        self.temporal_extractor.eval()
        
        print(f"âœ… {task_name} æ¨ç†ç®¡é“åˆå§‹åŒ–å®Œæˆ")
        print(f"   æ¡ä»¶ç¼©æ”¾ç³»æ•°: {self.conditioning_scale}")
        print(f"   è®­ç»ƒé…ç½®: {self.training_config.get('conditioning_strategy', 'unknown')}")
    
    def prepare_control_image(self, input_frames):
        """å‡†å¤‡æ§åˆ¶å›¾åƒï¼ˆä¸è®­ç»ƒæ—¶ç›¸åŒï¼‰"""
        with torch.no_grad():
            if len(input_frames.shape) == 4:  # (B, T, C, H, W)
                control_images = self.temporal_extractor(input_frames)
            else:  # (T, C, H, W)
                control_images = self.temporal_extractor(input_frames.unsqueeze(0))
                control_images = control_images.squeeze(0)
            
            control_images = torch.clamp(control_images, -1.0, 1.0)
            control_images = (control_images + 1.0) / 2.0
            return control_images
    
    def predict_frame(self, input_frames, text_prompt, num_inference_steps=20, guidance_scale=7.5):
        """
        é¢„æµ‹ç¬¬25å¸§
        
        Args:
            input_frames: (20, 3, H, W) å‰20å¸§åºåˆ—
            text_prompt: æ–‡æœ¬æè¿°
        """
        # å‡†å¤‡æ§åˆ¶å›¾åƒ
        input_batch = input_frames.unsqueeze(0).to(self.device)  # (1, 20, 3, H, W)
        control_image = self.prepare_control_image(input_batch)  # (1, 3, H, W)
        
        # è°ƒæ•´æ§åˆ¶å›¾åƒå°ºå¯¸ä»¥åŒ¹é…Stable Diffusion
        control_image_pil = self.tensor_to_pil(control_image.squeeze(0))
        
        # ä½¿ç”¨ControlNetç”Ÿæˆå›¾åƒ
        with torch.no_grad():
            result = self.pipe(
                prompt=text_prompt,
                image=control_image_pil,
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
                controlnet_conditioning_scale=self.conditioning_scale,
                generator=torch.manual_seed(42),
                height=96,
                width=96
            )
        
        return result.images[0]
    
    def tensor_to_pil(self, tensor):
        """å°†å¼ é‡è½¬æ¢ä¸ºPILå›¾åƒ"""
        tensor = tensor.cpu().squeeze(0)
        if tensor.dim() == 3:
            tensor = tensor.permute(1, 2, 0)
        tensor = (tensor * 255).numpy().astype(np.uint8)
        return Image.fromarray(tensor)
    
    def pil_to_tensor(self, pil_image):
        """å°†PILå›¾åƒè½¬æ¢ä¸ºå¼ é‡"""
        np_image = np.array(pil_image).astype(np.float32) / 255.0
        tensor = torch.from_numpy(np_image).permute(2, 0, 1)
        return tensor

# ==================== å¯¹æ¯”è¯„ä¼°ç³»ç»Ÿ ====================

class ComparisonEvaluator:
    """å¯¹æ¯”è¯„ä¼°ç³»ç»Ÿ"""
    
    def __init__(self, output_dir="inference_results"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
    
    def compute_metrics(self, predicted, ground_truth):
        """è®¡ç®—å›¾åƒè´¨é‡æŒ‡æ ‡"""
        pred_np = np.array(predicted)
        gt_np = np.array(ground_truth)
        
        # ç¡®ä¿å›¾åƒå°ºå¯¸ç›¸åŒ
        if pred_np.shape != gt_np.shape:
            pred_np = cv2.resize(pred_np, (gt_np.shape[1], gt_np.shape[0]))
        
        # è®¡ç®—æŒ‡æ ‡
        ssim_value = ssim(gt_np, pred_np, multichannel=True, channel_axis=2)
        psnr_value = psnr(gt_np, pred_np)
        
        return {
            'ssim': ssim_value,
            'psnr': psnr_value
        }
    
    def create_comparison_image(self, input_frames, predicted_frame, ground_truth, metrics, save_path):
        """åˆ›å»ºå¯¹æ¯”å›¾åƒ"""
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        
        # è¾“å…¥å¸§ç¤ºä¾‹ï¼ˆç¬¬ä¸€å¸§ã€ä¸­é—´å¸§ã€æœ€åä¸€å¸§ï¼‰
        input_indices = [0, 10, 19]
        for i, idx in enumerate(input_indices):
            frame = input_frames[idx].permute(1, 2, 0).cpu().numpy()
            axes[0, i].imshow(frame)
            axes[0, i].set_title(f'è¾“å…¥å¸§ {idx+1}')
            axes[0, i].axis('off')
        
        # é¢„æµ‹ç»“æœ
        axes[1, 0].imshow(predicted_frame)
        axes[1, 0].set_title('é¢„æµ‹å¸§ (ç¬¬25å¸§)')
        axes[1, 0].axis('off')
        
        # çœŸå®ç»“æœ
        if ground_truth is not None:
            gt_frame = ground_truth.permute(1, 2, 0).cpu().numpy()
            axes[1, 1].imshow(gt_frame)
            axes[1, 1].set_title('çœŸå®å¸§ (ç¬¬25å¸§)')
            axes[1, 1].axis('off')
            
            # å·®å¼‚å›¾
            pred_np = np.array(predicted_frame)
            if pred_np.shape != gt_np.shape:
                pred_np = cv2.resize(pred_np, (gt_np.shape[1], gt_np.shape[0]))
            
            diff = np.abs(pred_np.astype(float) - gt_np.astype(float))
            diff_normalized = (diff / diff.max() * 255).astype(np.uint8)
            
            axes[1, 2].imshow(diff_normalized, cmap='hot')
            axes[1, 2].set_title('å·®å¼‚å›¾')
            axes[1, 2].axis('off')
            
            # æ·»åŠ æŒ‡æ ‡æ–‡æœ¬
            metrics_text = f"SSIM: {metrics['ssim']:.4f}\nPSNR: {metrics['psnr']:.2f} dB"
            fig.text(0.5, 0.01, metrics_text, ha='center', fontsize=12, 
                    bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray"))
        else:
            # å¦‚æœæ²¡æœ‰çœŸå®å¸§ï¼Œåªæ˜¾ç¤ºé¢„æµ‹ç»“æœ
            axes[1, 1].axis('off')
            axes[1, 2].axis('off')
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.close()

# ==================== ä¸»æ‰§è¡Œå‡½æ•° ====================

def load_all_predictors(sd_model_path, tasks_config, device="cuda"):
    """åŠ è½½æ‰€æœ‰ä»»åŠ¡çš„é¢„æµ‹å™¨"""
    predictors = {}
    for task_name, model_path in tasks_config.items():
        if Path(model_path).exists():
            try:
                predictors[task_name] = ControlNetSDInference(
                    sd_model_path, model_path, task_name, device
                )
                print(f"âœ… æˆåŠŸåŠ è½½ {task_name} é¢„æµ‹å™¨")
            except Exception as e:
                print(f"âŒ åŠ è½½ {task_name} é¢„æµ‹å™¨å¤±è´¥: {e}")
        else:
            print(f"âš ï¸  æ‰¾ä¸åˆ° {task_name} çš„æ¨¡å‹æ–‡ä»¶: {model_path}")
    
    return predictors

def single_sample_inference(predictors, task_name, input_frames, text_prompt, output_dir="single_results"):
    """å•æ ·æœ¬æ¨ç†"""
    output_dir = Path(output_dir)
    output_dir.mkdir(exist_ok=True)
    
    if task_name not in predictors:
        print(f"âŒ æ‰¾ä¸åˆ° {task_name} çš„é¢„æµ‹å™¨")
        return
    
    predictor = predictors[task_name]
    evaluator = ComparisonEvaluator(output_dir)
    
    print(f"ğŸ¯ å¼€å§‹å•æ ·æœ¬æ¨ç† ({task_name})...")
    
    # é¢„æµ‹å¸§
    predicted_frame = predictor.predict_frame(input_frames, text_prompt)
    
    # ä¿å­˜é¢„æµ‹ç»“æœ
    save_path = output_dir / f"{task_name}_predicted.png"
    predicted_frame.save(save_path)
    print(f"âœ… ä¿å­˜é¢„æµ‹ç»“æœ: {save_path}")
    
    # åˆ›å»ºå¯¹æ¯”å›¾åƒï¼ˆä¸åŒ…å«çœŸå®å¸§ï¼‰
    comparison_path = output_dir / f"{task_name}_comparison.png"
    evaluator.create_comparison_image(input_frames, predicted_frame, None, {}, comparison_path)
    print(f"âœ… ä¿å­˜å¯¹æ¯”å›¾åƒ: {comparison_path}")
    
    return predicted_frame

def batch_evaluation(predictors, data_loader, task_name, output_dir="batch_results", num_samples=10):
    """æ‰¹é‡è¯„ä¼°"""
    output_dir = Path(output_dir) / task_name
    output_dir.mkdir(exist_ok=True)
    
    if task_name not in predictors:
        print(f"âŒ æ‰¾ä¸åˆ° {task_name} çš„é¢„æµ‹å™¨")
        return
    
    predictor = predictors[task_name]
    evaluator = ComparisonEvaluator(output_dir)
    
    all_metrics = []
    
    print(f"ğŸ¯ å¼€å§‹æ‰¹é‡è¯„ä¼° {task_name} ä»»åŠ¡...")
    
    for batch_idx, batch in enumerate(data_loader):
        if batch is None:
            continue
        
        if batch_idx * batch['input_frames'].shape[0] >= num_samples:
            break
        
        input_frames_batch = batch['input_frames']
        target_frames_batch = batch['target_frame']
        text_descriptions = batch.get('label_text', [''] * len(input_frames_batch))
        video_ids = batch.get('video_id', [f'batch_{batch_idx}_{i}' for i in range(len(input_frames_batch))])
        
        for i in range(len(input_frames_batch)):
            if len(all_metrics) >= num_samples:
                break
            
            try:
                # é¢„æµ‹
                predicted_frame = predictor.predict_frame(
                    input_frames_batch[i], 
                    text_descriptions[i]
                )
                
                # è®¡ç®—æŒ‡æ ‡
                metrics = evaluator.compute_metrics(predicted_frame, target_frames_batch[i])
                all_metrics.append(metrics)
                
                # ä¿å­˜å¯¹æ¯”å›¾åƒ
                save_path = output_dir / f"{video_ids[i]}_comparison.png"
                evaluator.create_comparison_image(
                    input_frames_batch[i], predicted_frame, target_frames_batch[i], 
                    metrics, save_path
                )
                
                # å•ç‹¬ä¿å­˜é¢„æµ‹ç»“æœ
                pred_save_path = output_dir / f"{video_ids[i]}_predicted.png"
                predicted_frame.save(pred_save_path)
                
                print(f"âœ… {video_ids[i]} - SSIM: {metrics['ssim']:.4f}, PSNR: {metrics['psnr']:.2f}")
                
            except Exception as e:
                print(f"âŒ è¯„ä¼°å¤±è´¥ {video_ids[i]}: {e}")
    
    # æ±‡æ€»ç»Ÿè®¡
    if all_metrics:
        ssim_values = [m['ssim'] for m in all_metrics]
        psnr_values = [m['psnr'] for m in all_metrics]
        
        summary = {
            'task': task_name,
            'num_samples': len(all_metrics),
            'ssim_mean': np.mean(ssim_values),
            'ssim_std': np.std(ssim_values),
            'psnr_mean': np.mean(psnr_values),
            'psnr_std': np.std(psnr_values),
            'ssim_min': np.min(ssim_values),
            'ssim_max': np.max(ssim_values),
            'psnr_min': np.min(psnr_values),
            'psnr_max': np.max(psnr_values)
        }
        
        # ä¿å­˜æ±‡æ€»ç»“æœ
        summary_path = output_dir / "evaluation_summary.json"
        with open(summary_path, 'w') as f:
            json.dump(summary, f, indent=2)
        
        # æ‰“å°æ±‡æ€»
        print(f"\nğŸ“Š {task_name} è¯„ä¼°æ±‡æ€»:")
        print(f"   æ ·æœ¬æ•°é‡: {summary['num_samples']}")
        print(f"   SSIM: {summary['ssim_mean']:.4f} Â± {summary['ssim_std']:.4f}")
        print(f"   PSNR: {summary['psnr_mean']:.2f} Â± {summary['psnr_std']:.2f} dB")
        
        return summary
    else:
        print(f"âŒ {task_name} æ²¡æœ‰æˆåŠŸè¯„ä¼°çš„æ ·æœ¬")
        return None

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ControlNetè®­ç»ƒåæ¨ç†")
    parser.add_argument("--sd_path", type=str, required=True, 
                       help="æœ¬åœ°Stable Diffusionæ¨¡å‹è·¯å¾„")
    parser.add_argument("--data_path", type=str, default="processed_data",
                       help="å¤„ç†æ•°æ®è·¯å¾„")
    parser.add_argument("--output_dir", type=str, default="inference_results",
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--mode", type=str, default="batch", choices=["single", "batch"],
                       help="æ¨ç†æ¨¡å¼: single(å•æ ·æœ¬) æˆ– batch(æ‰¹é‡)")
    parser.add_argument("--task", type=str, default="all",
                       choices=['move_object', 'drop_object', 'cover_object', 'all'],
                       help="æŒ‡å®šä»»åŠ¡")
    parser.add_argument("--num_samples", type=int, default=10,
                       help="æ‰¹é‡è¯„ä¼°çš„æ ·æœ¬æ•°é‡")
    
    args = parser.parse_args()
    
    # ä»»åŠ¡é…ç½®
    tasks_config = {
        'move_object': 'training_results_move_object/controlnet_move_object_best.pth',
        'drop_object': 'training_results_drop_object/controlnet_drop_object_best.pth',
        'cover_object': 'training_results_cover_object/controlnet_cover_object_best.pth'
    }
    
    # è®¾å¤‡è®¾ç½®
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½é¢„æµ‹å™¨
    print("ğŸ“¦ åˆå§‹åŒ–é¢„æµ‹å™¨...")
    predictors = load_all_predictors(args.sd_path, tasks_config, device)
    
    if not predictors:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„é¢„æµ‹å™¨ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶è·¯å¾„")
        return
    
    if args.mode == "single":
        # å•æ ·æœ¬æ¨ç†æ¨¡å¼
        print("\nğŸ¯ å•æ ·æœ¬æ¨ç†æ¨¡å¼")
        
        # è¿™é‡Œå¯ä»¥æ‰‹åŠ¨æŒ‡å®šè¾“å…¥å¸§å’Œæ–‡æœ¬æç¤º
        # ç¤ºä¾‹ï¼šåˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•æ•°æ®
        print("âš ï¸  å•æ ·æœ¬æ¨¡å¼éœ€è¦æ‰‹åŠ¨æä¾›è¾“å…¥æ•°æ®")
        print("è¯·ä¿®æ”¹ä»£ç ä¸­çš„ input_frames å’Œ text_prompt")
        
        # ç¤ºä¾‹ä»£ç ï¼š
        # input_frames = torch.rand(20, 3, 96, 96)  # æ›¿æ¢ä¸ºçœŸå®æ•°æ®
        # text_prompt = "Moving something to the right"
        # single_sample_inference(predictors, args.task, input_frames, text_prompt, args.output_dir)
        
    else:
        # æ‰¹é‡è¯„ä¼°æ¨¡å¼
        print("\nğŸ¯ æ‰¹é‡è¯„ä¼°æ¨¡å¼")
        
        if args.task == 'all':
            tasks_to_evaluate = list(predictors.keys())
        else:
            tasks_to_evaluate = [args.task]
        
        all_summaries = {}
        
        for task_name in tasks_to_evaluate:
            print(f"\n{'='*50}")
            print(f"ğŸ¯ è¯„ä¼°ä»»åŠ¡: {task_name}")
            print(f"{'='*50}")
            
            # åŠ è½½æµ‹è¯•æ•°æ®
            test_dataset = FramePredictionDataset(
                f"{args.data_path}/metadata/test_samples.csv",
                task_name=task_name
            )
            
            # åˆ›å»ºç®€å•æ•°æ®åŠ è½½å™¨
            class SimpleDataLoader:
                def __init__(self, dataset, batch_size=2):
                    self.dataset = dataset
                    self.batch_size = batch_size
                
                def __iter__(self):
                    self.idx = 0
                    return self
                
                def __next__(self):
                    if self.idx >= len(self.dataset):
                        raise StopIteration
                    
                    batch = []
                    for i in range(self.batch_size):
                        if self.idx >= len(self.dataset):
                            break
                        sample = self.dataset[self.idx]
                        if sample is not None:
                            batch.append(sample)
                        self.idx += 1
                    
                    if not batch:
                        raise StopIteration
                    
                    # åˆå¹¶æ‰¹æ¬¡
                    return {
                        'input_frames': torch.stack([item['input_frames'] for item in batch]),
                        'target_frame': torch.stack([item['target_frame'] for item in batch]),
                        'label_text': [item['label_text'] for item in batch],
                        'video_id': [item['video_id'] for item in batch]
                    }
            
            data_loader = SimpleDataLoader(test_dataset, batch_size=2)
            
            # æ‰§è¡Œè¯„ä¼°
            summary = batch_evaluation(
                predictors, data_loader, task_name, args.output_dir, args.num_samples
            )
            
            if summary:
                all_summaries[task_name] = summary
        
        # ç”Ÿæˆæ€»ä½“æŠ¥å‘Š
        if all_summaries:
            generate_final_report(all_summaries, args.output_dir)
    
    print(f"\nğŸŠ æ¨ç†å®Œæˆ! ç»“æœä¿å­˜åœ¨: {args.output_dir}")

def generate_final_report(summaries, output_dir):
    """ç”Ÿæˆæœ€ç»ˆè¯„ä¼°æŠ¥å‘Š"""
    report = {
        'overall': {},
        'tasks': summaries
    }
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = Path(output_dir) / "final_evaluation_report.json"
    with open(report_path, 'w') as f:
        json.dump(report, f, indent=2)
    
    # æ‰“å°æŠ¥å‘Š
    print(f"\nğŸ“‹ æœ€ç»ˆè¯„ä¼°æŠ¥å‘Š:")
    print(f"{'='*60}")
    for task_name, summary in summaries.items():
        print(f"{task_name}:")
        print(f"  SSIM: {summary['ssim_mean']:.4f} (Â±{summary['ssim_std']:.4f})")
        print(f"  PSNR: {summary['psnr_mean']:.2f} dB (Â±{summary['psnr_std']:.2f})")
        print(f"  æ ·æœ¬æ•°: {summary['num_samples']}")
        print()

if __name__ == "__main__":
    main()