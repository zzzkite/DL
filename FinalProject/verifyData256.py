#!/usr/bin/env python3
"""
éªŒè¯æå–çš„æ•°æ®æ˜¯å¦æ­£ç¡® - é€‚é…å‰20å¸§â†’ç¬¬25å¸§è®¾ç½®ï¼Œ256x256ç‰ˆæœ¬
"""

import numpy as np
import pandas as pd
from pathlib import Path
import cv2
import json

def verify_data():
    """éªŒè¯æ•°æ®å®Œæ•´æ€§"""
    print("å¼€å§‹éªŒè¯æ•°æ®...")
    print("å¸§è®¾ç½®: è¾“å…¥å‰20å¸§ â†’ é¢„æµ‹ç¬¬25å¸§ï¼ˆè·³è¿‡4å¸§ï¼‰")
    print("åˆ†è¾¨ç‡: 256x256")
    
    # åŠ è½½å…ƒæ•°æ®
    metadata_dir = Path("processed_data_256/metadata")
    
    # æ£€æŸ¥æ‰€æœ‰æ•°æ®é›†æ–‡ä»¶
    dataset_files = {
        "æ‰€æœ‰æ ·æœ¬": "all_samples.csv",
        "è®­ç»ƒé›†": "train_samples.csv", 
        "éªŒè¯é›†": "val_samples.csv",
        "æµ‹è¯•é›†": "test_samples.csv"
    }
    
    for name, filename in dataset_files.items():
        file_path = metadata_dir / filename
        if file_path.exists():
            df = pd.read_csv(file_path)
            print(f"âœ… {name}: {len(df)} ä¸ªæ ·æœ¬")
        else:
            print(f"âŒ {name}æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    # åŠ è½½å®Œæ•´æ•°æ®é›†ä¿¡æ¯
    all_samples = pd.read_csv(metadata_dir / "all_samples.csv")
    
    print(f"\næ€»æ ·æœ¬æ•°: {len(all_samples)}")
    
    # æ£€æŸ¥æ•°æ®é›†åˆ’åˆ†æ¯”ä¾‹
    train_samples = pd.read_csv(metadata_dir / "train_samples.csv")
    val_samples = pd.read_csv(metadata_dir / "val_samples.csv")
    test_samples = pd.read_csv(metadata_dir / "test_samples.csv")
    
    total_actual = len(train_samples) + len(val_samples) + len(test_samples)
    print(f"è®­ç»ƒé›†: {len(train_samples)} ä¸ªæ ·æœ¬ ({len(train_samples)/total_actual*100:.1f}%)")
    print(f"éªŒè¯é›†: {len(val_samples)} ä¸ªæ ·æœ¬ ({len(val_samples)/total_actual*100:.1f}%)")
    print(f"æµ‹è¯•é›†: {len(test_samples)} ä¸ªæ ·æœ¬ ({len(test_samples)/total_actual*100:.1f}%)")
    
    # æ£€æŸ¥æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬
    categories = all_samples['category'].unique()
    
    print(f"\n=== è¯¦ç»†æ ·æœ¬æ£€æŸ¥ ===")
    for category in categories:
        category_samples = all_samples[all_samples['category'] == category]
        print(f"\n{category}: {len(category_samples)} ä¸ªæ ·æœ¬")
        
        # éšæœºæ£€æŸ¥å‡ ä¸ªæ ·æœ¬
        for i, sample in category_samples.head(2).iterrows():
            print(f"  æ£€æŸ¥æ ·æœ¬ {sample['video_id']}:")
            
            # æ£€æŸ¥è¾“å…¥å¸§
            try:
                input_frames = np.load(sample['input_frames_path'])
                expected_shape = (20, 256, 256, 3)  # æ›´æ–°æœŸæœ›å½¢çŠ¶ä¸º256x256
                actual_shape = input_frames.shape
                print(f"    âœ… è¾“å…¥å¸§: {actual_shape} (æœŸæœ›: {expected_shape})")
                
                if actual_shape != expected_shape:
                    print(f"    âš ï¸  è¾“å…¥å¸§å½¢çŠ¶ä¸åŒ¹é…!")
                
                # æ£€æŸ¥ç›®æ ‡å¸§
                target_frame = np.load(sample['target_frame_path'])
                expected_target_shape = (256, 256, 3)  # æ›´æ–°æœŸæœ›å½¢çŠ¶ä¸º256x256
                actual_target_shape = target_frame.shape
                print(f"    âœ… ç›®æ ‡å¸§: {actual_target_shape} (æœŸæœ›: {expected_target_shape})")
                
                if actual_target_shape != expected_target_shape:
                    print(f"    âš ï¸  ç›®æ ‡å¸§å½¢çŠ¶ä¸åŒ¹é…!")
                
                # æ£€æŸ¥æ•°æ®èŒƒå›´
                print(f"    âœ… è¾“å…¥å¸§èŒƒå›´: [{input_frames.min():.1f}, {input_frames.max():.1f}]")
                print(f"    âœ… ç›®æ ‡å¸§èŒƒå›´: [{target_frame.min():.1f}, {target_frame.max():.1f}]")
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆå›¾åƒæ•°æ®
                if input_frames.max() <= 1.0:
                    print(f"    âš ï¸  è¾“å…¥å¸§å¯èƒ½å·²ç»å½’ä¸€åŒ– (æœ€å¤§å€¼: {input_frames.max():.3f})")
                else:
                    print(f"    âœ… è¾“å…¥å¸§æ•°æ®èŒƒå›´æ­£å¸¸ (0-255)")
                    
                if target_frame.max() <= 1.0:
                    print(f"    âš ï¸  ç›®æ ‡å¸§å¯èƒ½å·²ç»å½’ä¸€åŒ– (æœ€å¤§å€¼: {target_frame.max():.3f})")
                else:
                    print(f"    âœ… ç›®æ ‡å¸§æ•°æ®èŒƒå›´æ­£å¸¸ (0-255)")
                
                # æ£€æŸ¥å›¾åƒè´¨é‡ï¼ˆå¯é€‰ï¼‰
                if input_frames.shape[1] == 256 and input_frames.shape[2] == 256:
                    # è®¡ç®—å›¾åƒé”åº¦ï¼ˆæ‹‰æ™®æ‹‰æ–¯æ–¹å·®ï¼‰
                    laplacian_var = cv2.Laplacian(input_frames[0].astype(np.uint8), cv2.CV_64F).var()
                    print(f"    ğŸ“Š ç¬¬ä¸€å¸§é”åº¦: {laplacian_var:.1f}")
                    
                    # è°ƒæ•´é”åº¦é˜ˆå€¼ä»¥é€‚åº”256x256åˆ†è¾¨ç‡
                    if laplacian_var < 50:
                        print(f"    âš ï¸  å›¾åƒå¯èƒ½æ¨¡ç³Š (é”åº¦: {laplacian_var:.1f})")
                    else:
                        print(f"    âœ… å›¾åƒæ¸…æ™°åº¦è‰¯å¥½")
                
            except Exception as e:
                print(f"    âŒ åŠ è½½å¤±è´¥: {e}")
    
    # æ£€æŸ¥ç¤ºä¾‹å›¾åƒ
    print(f"\n=== ç¤ºä¾‹å›¾åƒæ£€æŸ¥ ===")
    samples_dir = Path("processed_data_256/samples")
    for category in categories:
        category_dir = samples_dir / category
        if category_dir.exists():
            image_files = list(category_dir.glob("*.jpg"))
            print(f"  {category}: {len(image_files)} ä¸ªç¤ºä¾‹å›¾åƒ")
            for img_file in image_files[:2]:  # æ˜¾ç¤ºå‰2ä¸ªæ–‡ä»¶
                # æ£€æŸ¥ç¤ºä¾‹å›¾åƒçš„åˆ†è¾¨ç‡
                img = cv2.imread(str(img_file))
                if img is not None:
                    print(f"    - {img_file.name} ({img.shape[1]}x{img.shape[0]})")
                else:
                    print(f"    - {img_file.name} (åŠ è½½å¤±è´¥)")
        else:
            print(f"  {category}: ç¤ºä¾‹ç›®å½•ä¸å­˜åœ¨")
    
    # æ£€æŸ¥æ•°æ®é›†ä¿¡æ¯æ–‡ä»¶
    print(f"\n=== æ•°æ®é›†ä¿¡æ¯æ£€æŸ¥ ===")
    info_file = metadata_dir / "dataset_info.json"
    if info_file.exists():
        with open(info_file, 'r') as f:
            dataset_info = json.load(f)
        print(f"æ•°æ®é›†åç§°: {dataset_info.get('name', 'N/A')}")
        print(f"æ€»æ ·æœ¬æ•°: {dataset_info.get('total_samples', 'N/A')}")
        print(f"è®­ç»ƒé›†: {dataset_info.get('train_samples', 'N/A')}")
        print(f"éªŒè¯é›†: {dataset_info.get('val_samples', 'N/A')}")
        print(f"æµ‹è¯•é›†: {dataset_info.get('test_samples', 'N/A')}")
        print(f"åˆ’åˆ†æ¯”ä¾‹: {dataset_info.get('split_ratio', 'N/A')}")
        
        frame_info = dataset_info.get('frame_info', {})
        print(f"è¾“å…¥å¸§æ•°: {frame_info.get('input_frames', 'N/A')}")
        print(f"ç›®æ ‡å¸§æ•°: {frame_info.get('target_frame', 'N/A')}")
        print(f"è·³è¿‡å¸§æ•°: {frame_info.get('skip_frames', 'N/A')}")
        resolution = frame_info.get('resolution', 'N/A')
        print(f"åˆ†è¾¨ç‡: {resolution}")
        
        # éªŒè¯åˆ†è¾¨ç‡ä¿¡æ¯
        if resolution != "256x256":
            print(f"âš ï¸  åˆ†è¾¨ç‡ä¸åŒ¹é…: æœŸæœ›256x256ï¼Œå®é™…{resolution}")
        else:
            print(f"âœ… åˆ†è¾¨ç‡éªŒè¯é€šè¿‡")
    else:
        print("âŒ æ•°æ®é›†ä¿¡æ¯æ–‡ä»¶ä¸å­˜åœ¨")
    
    # æ£€æŸ¥æ ·æœ¬ä¸­çš„åˆ†è¾¨ç‡å­—æ®µ
    print(f"\n=== æ ·æœ¬åˆ†è¾¨ç‡æ£€æŸ¥ ===")
    resolution_counts = all_samples['resolution'].value_counts()
    for res, count in resolution_counts.items():
        print(f"åˆ†è¾¨ç‡ {res}: {count} ä¸ªæ ·æœ¬")
        if res != "256x256":
            print(f"âš ï¸  å‘ç°ä¸åŒ¹é…çš„åˆ†è¾¨ç‡: {res}")
    
    # æ–‡ä»¶å¤§å°ç»Ÿè®¡
    print(f"\n=== æ–‡ä»¶å¤§å°ç»Ÿè®¡ ===")
    total_input_size = 0
    total_target_size = 0
    sample_count = min(10, len(all_samples))  # æ£€æŸ¥å‰10ä¸ªæ ·æœ¬çš„æ–‡ä»¶å¤§å°
    
    for i, sample in all_samples.head(sample_count).iterrows():
        try:
            input_path = Path(sample['input_frames_path'])
            target_path = Path(sample['target_frame_path'])
            
            if input_path.exists():
                input_size = input_path.stat().st_size / (1024 * 1024)  # MB
                total_input_size += input_size
                print(f"  {input_path.name}: {input_size:.2f} MB")
                
            if target_path.exists():
                target_size = target_path.stat().st_size / (1024 * 1024)  # MB
                total_target_size += target_size
                print(f"  {target_path.name}: {target_size:.2f} MB")
                
        except Exception as e:
            print(f"  æ£€æŸ¥æ–‡ä»¶å¤§å°å¤±è´¥: {e}")
    
    if sample_count > 0:
        avg_input_size = total_input_size / sample_count
        avg_target_size = total_target_size / sample_count
        print(f"\nå¹³å‡æ–‡ä»¶å¤§å°:")
        print(f"  è¾“å…¥å¸§: {avg_input_size:.2f} MB")
        print(f"  ç›®æ ‡å¸§: {avg_target_size:.2f} MB")
        
        # ä¼°ç®—æ€»å­˜å‚¨éœ€æ±‚
        total_samples = len(all_samples)
        estimated_total_size = (avg_input_size + avg_target_size) * total_samples
        print(f"ä¼°ç®—æ€»å­˜å‚¨éœ€æ±‚: {estimated_total_size:.2f} MB ({estimated_total_size/1024:.2f} GB)")
        
        # ä¸512x512ç‰ˆæœ¬çš„æ¯”è¾ƒ
        print(f"\nğŸ’¡ å­˜å‚¨æ•ˆç‡å¯¹æ¯”:")
        print(f"  ç›¸æ¯”512x512ç‰ˆæœ¬ï¼Œ256x256ç‰ˆæœ¬å­˜å‚¨éœ€æ±‚é™ä½çº¦75%")
    
    # æ€§èƒ½é¢„ä¼°
    print(f"\n=== æ€§èƒ½é¢„ä¼° ===")
    print(f"ğŸ“Š 256x256åˆ†è¾¨ç‡ä¼˜åŠ¿:")
    print(f"  âœ… è®­ç»ƒé€Ÿåº¦: æ¯”512x512å¿«çº¦4å€")
    print(f"  âœ… æ˜¾å­˜éœ€æ±‚: æ¯”512x512å‡å°‘çº¦75%")
    print(f"  âœ… æ‰¹æ¬¡å¤§å°: å¯æ”¯æŒæ›´å¤§çš„æ‰¹æ¬¡")
    print(f"  âœ… æ”¶æ•›é€Ÿåº¦: é€šå¸¸æ›´å¿«æ”¶æ•›")
    
    print(f"\nğŸ‰ æ•°æ®éªŒè¯å®Œæˆ!")
    print(f"å¸§è®¾ç½®ç¡®è®¤: è¾“å…¥å‰20å¸§ â†’ é¢„æµ‹ç¬¬25å¸§ï¼ˆè·³è¿‡4å¸§ï¼‰")
    print(f"åˆ†è¾¨ç‡ç¡®è®¤: 256x256")

if __name__ == "__main__":
    verify_data()