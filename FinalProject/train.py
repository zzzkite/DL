#!/usr/bin/env python3
"""
ControlNet 1.1å¤šå¸§æ—¶åºè®­ç»ƒ - å‰20å¸§é¢„æµ‹ç¬¬25å¸§ç‰ˆæœ¬ï¼ˆå¸¦åŠ¨æ€æ¡ä»¶ç¼©æ”¾ï¼‰
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
import torch.optim as optim
import cv2
import numpy as np
from pathlib import Path
import matplotlib.pyplot as plt
import sys
import os
import math
from collections import deque
import gc
import time

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.append(str(project_root))

try:
    from diffusers import DDPMScheduler, AutoencoderKL, UNet2DConditionModel
    from transformers import CLIPTokenizer, CLIPTextModel
    from loaderData import create_task_specific_loaders
    
    # å°è¯•å¯¼å…¥ControlNet 1.1çš„æ¨¡å—
    try:
        from cldm.model import create_model, load_state_dict
        CONTROLNET_AVAILABLE = True
        print("âœ… æˆåŠŸå¯¼å…¥ControlNetæ¨¡å—")
    except ImportError as e:
        print(f"âš ï¸  æ— æ³•å¯¼å…¥cldm: {e}")
        CONTROLNET_AVAILABLE = False
        
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

class EnhancedTemporalFeatureExtractor(nn.Module):
    """å¢å¼ºç‰ˆæ—¶åºç‰¹å¾æå–å™¨ - ä¸“é—¨å¤„ç†20å¸§â†’25å¸§é¢„æµ‹"""
    
    def __init__(self, input_channels=3, feature_channels=32):
        super().__init__()
        
        # è¿åŠ¨è½¨è¿¹é¢„æµ‹ç½‘ç»œ
        self.trajectory_predictor = nn.Sequential(
            nn.Conv3d(input_channels, 16, (5, 3, 3), padding=(2, 1, 1)),  # æ—¶é—´ç»´åº¦å·ç§¯
            nn.ReLU(),
            nn.AdaptiveAvgPool3d((8, None, None)),  # å‹ç¼©æ—¶é—´ç»´åº¦
            nn.Conv3d(16, 8, (3, 3, 3), padding=(1, 1, 1)),
            nn.ReLU(),
            nn.Conv3d(8, 4, (3, 3, 3), padding=(1, 1, 1)),
            nn.ReLU()
        )
        
        # å…‰æµç‰¹å¾æå–
        self.flow_processor = nn.Sequential(
            nn.Conv2d(2, 16, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(16, 8, 3, padding=1),
            nn.ReLU()
        )
        
        # ç‰¹å¾èåˆç½‘ç»œ
        self.feature_fusion = nn.Sequential(
            nn.Conv2d(12, 32, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 16, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(16, 3, 3, padding=1),  # è¾“å‡º3é€šé“æ§åˆ¶å›¾åƒ
            nn.Tanh()
        )
        
    def compute_optical_flow_sequence(self, frames):
        """è®¡ç®—è¿ç»­å…‰æµåºåˆ—"""
        batch_size, num_frames, C, H, W = frames.shape
        flow_sequences = []
        
        for b in range(batch_size):
            frame_flows = []
            for t in range(num_frames - 1):
                prev_frame = (frames[b, t].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)
                curr_frame = (frames[b, t+1].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)
                
                prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_RGB2GRAY)
                curr_gray = cv2.cvtColor(curr_frame, cv2.COLOR_RGB2GRAY)
                
                # è®¡ç®—å…‰æµ
                flow = cv2.calcOpticalFlowFarneback(
                    prev_gray, curr_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0
                )
                
                # è½¬æ¢ä¸ºå¼ é‡
                flow_tensor = torch.from_numpy(flow).permute(2, 0, 1).float()  # (2, H, W)
                frame_flows.append(flow_tensor)
            
            if frame_flows:
                flow_sequence = torch.stack(frame_flows)  # (T-1, 2, H, W)
                flow_sequences.append(flow_sequence)
            else:
                # å¦‚æœæ²¡æœ‰å…‰æµï¼Œåˆ›å»ºé›¶å¼ é‡
                flow_sequences.append(torch.zeros(num_frames-1, 2, H, W))
        
        return torch.stack(flow_sequences).to(frames.device)  # (B, T-1, 2, H, W)
    
    def predict_future_motion(self, flows):
        """é¢„æµ‹æœªæ¥è¿åŠ¨è¶‹åŠ¿"""
        batch_size, seq_len, C, H, W = flows.shape
        
        # ä½¿ç”¨æœ€è¿‘å‡ å¸§çš„è¿åŠ¨æ¥é¢„æµ‹æœªæ¥è¶‹åŠ¿
        recent_flows = flows[:, -4:]  # å–æœ€å4å¸§å…‰æµ (B, 4, 2, H, W)
        
        # è®¡ç®—è¿åŠ¨åŠ é€Ÿåº¦ï¼ˆå…‰æµçš„å˜åŒ–ï¼‰
        if recent_flows.shape[1] >= 3:
            flow_acceleration = recent_flows[:, -1] - recent_flows[:, -2]  # æœ€æ–°è¿åŠ¨å˜åŒ–
        else:
            flow_acceleration = torch.zeros_like(recent_flows[:, -1])
        
        # é¢„æµ‹æœªæ¥è¿åŠ¨ï¼ˆç®€å•çº¿æ€§å¤–æ¨ï¼‰
        last_flow = recent_flows[:, -1]  # æœ€åä¸€å¸§å…‰æµ
        predicted_flow = last_flow + flow_acceleration * 1.2  # ä¹˜ä»¥ç³»æ•°é¢„æµ‹æœªæ¥
        
        return predicted_flow  # (B, 2, H, W)
    
    def forward(self, frames):
        """
        æå–æ—¶åºç‰¹å¾ - ä¸“é—¨ä¸º20å¸§â†’25å¸§é¢„æµ‹è®¾è®¡
        frames: (B, 20, 3, H, W)
        è¿”å›: (B, 3, H, W) å¢å¼ºçš„æ§åˆ¶å›¾åƒ
        """
        batch_size, num_frames, C, H, W = frames.shape
        
        # 1. è®¡ç®—å…‰æµåºåˆ—
        flow_sequence = self.compute_optical_flow_sequence(frames)  # (B, 19, 2, H, W)
        
        # 2. é¢„æµ‹æœªæ¥è¿åŠ¨ï¼ˆç¬¬20å¸§â†’ç¬¬25å¸§ï¼‰
        predicted_flow = self.predict_future_motion(flow_sequence)  # (B, 2, H, W)
        
        # 3. å¤„ç†å…‰æµç‰¹å¾
        flow_features = self.flow_processor(predicted_flow)  # (B, 8, H, W)
        
        # 4. è½¨è¿¹ç‰¹å¾æå–
        trajectory_input = frames.permute(0, 2, 1, 3, 4)  # (B, 3, 20, H, W)
        trajectory_features = self.trajectory_predictor(trajectory_input)  # (B, 4, 8, H, W)
        trajectory_features = trajectory_features.mean(dim=2)  # (B, 4, H, W) å¹³å‡æ—¶é—´ç»´åº¦
        
        # 5. ç‰¹å¾èåˆ
        combined_features = torch.cat([flow_features, trajectory_features], dim=1)  # (B, 12, H, W)
        enhanced_control = self.feature_fusion(combined_features)  # (B, 3, H, W)
        
        return enhanced_control

class DynamicConditioningControlNetTrainer:
    def __init__(self, config):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.task_name = config.get('task_name', 'unknown_task')
        
        print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"ğŸ¯ ä»»åŠ¡: {self.task_name} - å‰20å¸§ â†’ é¢„æµ‹ç¬¬25å¸§")
        
        # ğŸ¯ åŠ¨æ€æ¡ä»¶ç¼©æ”¾å‚æ•°
        self.conditioning_strategy = config.get('conditioning_strategy', 'adaptive')
        self.initial_scale = config.get('initial_conditioning_scale', 0.8)
        self.final_scale = config.get('final_conditioning_scale', 1.2)
        self.current_scale = self.initial_scale
        
        # ç­–ç•¥ç‰¹å®šå‚æ•°
        self.adaptive_threshold = config.get('adaptive_threshold', 0.15)
        self.scale_step = config.get('scale_step', 0.05)
        self.patience_counter = 0
        self.best_val_loss = float('inf')
        
        print(f"ğŸ¯ åŠ¨æ€æ¡ä»¶ç¼©æ”¾ç­–ç•¥: {self.conditioning_strategy}")
        print(f"   åˆå§‹ç¼©æ”¾: {self.initial_scale} â†’ æœ€ç»ˆç¼©æ”¾: {self.final_scale}")
        
        # æ¢¯åº¦ç¼©æ”¾å™¨
        if self.device.type == 'cuda':
            self.scaler = torch.amp.GradScaler(device='cuda')
        else:
            self.scaler = None

        # æ˜¾å­˜æ¸…ç†ä¸ç›‘æ§è®¾ç½®
        self.cleanup_steps = config.get('cleanup_steps', 20)  # æ¯å¤šå°‘ä¸ªbatchæ‰§è¡Œä¸€æ¬¡æ˜¾å­˜æ¸…ç†
        self.mem_log_steps = config.get('mem_log_steps', 10)  # æ¯å¤šå°‘ä¸ªbatchæ‰“å°ä¸€æ¬¡æ˜¾å­˜ä¿¡æ¯
        self.enable_mem_logging = config.get('enable_mem_logging', True)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.setup_models()
        self.setup_optimizers()
        
    def update_conditioning_scale(self, epoch, total_epochs, train_loss=None, val_loss=None):
        """æ”¹è¿›çš„æ¡ä»¶ç¼©æ”¾æ›´æ–°ç­–ç•¥"""
        if self.conditioning_strategy == 'fixed':
            self.current_scale = self.initial_scale
            
        elif self.conditioning_strategy == 'linear_increase':
            # æ›´å¹³æ»‘çš„çº¿æ€§å¢åŠ 
            progress = min(1.0, epoch / (total_epochs * 0.8))  # å‰80%çº¿æ€§å¢åŠ 
            self.current_scale = self.initial_scale + progress * (self.final_scale - self.initial_scale)
            
        elif self.conditioning_strategy == 'adaptive_improved':
            # æ”¹è¿›çš„è‡ªé€‚åº”ç­–ç•¥
            if epoch > 5 and val_loss is not None and train_loss is not None:
                # ä½¿ç”¨ç§»åŠ¨å¹³å‡æ¥å¹³æ»‘åˆ¤æ–­
                if not hasattr(self, 'val_loss_history'):
                    self.val_loss_history = []
                
                self.val_loss_history.append(val_loss)
                if len(self.val_loss_history) > 5:
                    self.val_loss_history.pop(0)
                
                avg_val_loss = np.mean(self.val_loss_history)
                
                # æ›´ä¿å®ˆçš„æ¡ä»¶è°ƒæ•´
                if val_loss > train_loss * 1.2 and avg_val_loss > self.best_val_loss * 1.1:
                    # æ˜æ˜¾è¿‡æ‹Ÿåˆæ—¶æ‰å¢åŠ æ¡ä»¶æ§åˆ¶
                    self.current_scale = min(self.final_scale, self.current_scale + self.scale_step * 0.5)
                    print(f"  ğŸ”¼ æ£€æµ‹åˆ°è¿‡æ‹Ÿåˆï¼Œæ¸©å’Œå¢åŠ æ¡ä»¶ç¼©æ”¾è‡³: {self.current_scale:.3f}")
                elif train_loss > self.adaptive_threshold * 2:
                    # è®­ç»ƒéå¸¸å›°éš¾æ—¶æ‰å‡å°‘æ¡ä»¶æ§åˆ¶
                    self.current_scale = max(self.initial_scale, self.current_scale - self.scale_step * 0.5)
                    print(f"  ğŸ”½ è®­ç»ƒå›°éš¾ï¼Œæ¸©å’Œå‡å°‘æ¡ä»¶ç¼©æ”¾è‡³: {self.current_scale:.3f}")
        
        # ç¡®ä¿åœ¨åˆç†èŒƒå›´å†…
        self.current_scale = max(0.1, min(2.0, self.current_scale))
        
        return self.current_scale
    
    def apply_conditioning_scale(self, down_block_res_samples, mid_block_res_sample):
        """åº”ç”¨æ¡ä»¶ç¼©æ”¾åˆ°ControlNetè¾“å‡º"""
        if self.current_scale != 1.0:
            down_block_res_samples = [
                sample * self.current_scale for sample in down_block_res_samples
            ]
            mid_block_res_sample = mid_block_res_sample * self.current_scale
        
        return down_block_res_samples, mid_block_res_sample

    def setup_models(self):
        """å†…å­˜ä¼˜åŒ–çš„æ¨¡å‹åˆå§‹åŒ–"""
        print("ğŸ“¦ åˆå§‹åŒ–æ¨¡å‹ï¼ˆå†…å­˜ä¼˜åŒ–ç‰ˆï¼‰...")
        
        try:
            # 1. é¦–å…ˆæ¸…ç†GPUå†…å­˜
            torch.cuda.empty_cache()
            
            # 2. ä½¿ç”¨æ›´èŠ‚çœå†…å­˜çš„æ–¹å¼åŠ è½½æ¨¡å‹
            self.tokenizer = CLIPTokenizer.from_pretrained(
                "stable-diffusion-v1-5/tokenizer",
                local_files_only=True
            )
            
            # 3. æ–‡æœ¬ç¼–ç å™¨ - ä½¿ç”¨fp16
            self.text_encoder = CLIPTextModel.from_pretrained(
                "stable-diffusion-v1-5/text_encoder", 
                local_files_only=True,
                torch_dtype=torch.float16  # æ·»åŠ fp16
            )
            
            # 4. VAE - ä½¿ç”¨fp16å¹¶ä¸”è®¾ç½®ä¸ç¼“å­˜
            self.vae = AutoencoderKL.from_pretrained(
                "stable-diffusion-v1-5/vae",
                local_files_only=True,
                torch_dtype=torch.float16
            )
            
            # 5. UNet - ä½¿ç”¨fp16
            self.unet = UNet2DConditionModel.from_pretrained(
                "stable-diffusion-v1-5/unet",
                local_files_only=True, 
                torch_dtype=torch.float16
            )
            
            # 6. å™ªå£°è°ƒåº¦å™¨
            self.noise_scheduler = DDPMScheduler.from_pretrained(
                "stable-diffusion-v1-5/scheduler",
                local_files_only=True
            )
            
            # 7. åŠ è½½ControlNet - ä½¿ç”¨fp16
            self.controlnet = self.load_controlnet()
            
            # 8. æ—¶åºç‰¹å¾æå–å™¨ - ä½¿ç”¨fp16
            print("åˆå§‹åŒ–å¢å¼ºæ—¶åºç‰¹å¾æå–å™¨...")
            self.temporal_extractor = EnhancedTemporalFeatureExtractor().to(self.device)
            
            # 9. ç§»åŠ¨åˆ°è®¾å¤‡å¹¶å†»ç»“å‚æ•°
            self.text_encoder = self.text_encoder.to(self.device)
            self.vae = self.vae.to(self.device) 
            self.unet = self.unet.to(self.device)
            self.controlnet = self.controlnet.to(self.device)
            
            # å†»ç»“ä¸éœ€è¦è®­ç»ƒçš„ç»„ä»¶
            self.text_encoder.requires_grad_(False)
            self.vae.requires_grad_(False)
            self.unet.requires_grad_(False)
            self.temporal_extractor.requires_grad_(True) # å…è®¸è®­ç»ƒæ—¶åºæå–å™¨
            
            # åªè®­ç»ƒControlNet
            self.controlnet.requires_grad_(True)
            
            # 10. å†æ¬¡æ¸…ç†å†…å­˜
            torch.cuda.empty_cache()
            
            trainable_params = sum(p.numel() for p in self.controlnet.parameters() if p.requires_grad)
            total_params = sum(p.numel() for p in self.controlnet.parameters())
            print(f"âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
            print(f"   ControlNetå‚æ•°: {trainable_params:,} å¯è®­ç»ƒ / {total_params:,} æ€»è®¡")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            # æ¸…ç†å†…å­˜åé‡æ–°æŠ›å‡ºå¼‚å¸¸
            torch.cuda.empty_cache()
            raise

    
    def load_controlnet(self):
        """åŠ è½½ControlNetæ¨¡å‹"""
        try:
            if CONTROLNET_AVAILABLE:
                # ä½¿ç”¨ControlNet 1.1å®˜æ–¹åŠ è½½
                controlnet_dir = Path("ControlNet-v1-1")
                model_path = controlnet_dir / "control_sd15_canny.pth"
                config_path = controlnet_dir / "cldm_v15.yaml"
                
                model = create_model(str(config_path)).to(self.device)
                model.load_state_dict(load_state_dict(str(model_path), location='cpu'))
                return model.control_model
            else:
                # ä½¿ç”¨diffusersçš„ControlNet
                from diffusers import ControlNetModel
                controlnet = ControlNetModel.from_unet(
                    self.unet,
                    conditioning_channels=3
                )
                print("âœ… ä½¿ç”¨diffusers ControlNet")
                return controlnet
                
        except Exception as e:
            print(f"âŒ åŠ è½½ControlNetå¤±è´¥: {e}")
            raise
    
    def setup_optimizers(self):
        """è®¾ç½®ä¼˜åŒ–å™¨ - ä¿®å¤ç‰ˆæœ¬"""
        # ä½¿ç”¨æ›´å¤§çš„å­¦ä¹ ç‡
        self.optimizer = optim.AdamW(
            self.controlnet.parameters(),
            lr=self.config.get('learning_rate', 5e-5),  # å¢å¤§å­¦ä¹ ç‡
            weight_decay=self.config.get('weight_decay', 1e-3),  # å‡å°‘æƒé‡è¡°å‡
            betas=(0.9, 0.999),
            eps=1e-8  # æ·»åŠ epsé˜²æ­¢é™¤é›¶
        )
        
        # æ”¹è¿›çš„å­¦ä¹ ç‡è°ƒåº¦å™¨
        if self.config.get('lr_scheduler') == 'cosine_with_warmup':
            # å¸¦warmupçš„ä½™å¼¦é€€ç«
            from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts
            self.lr_scheduler = CosineAnnealingWarmRestarts(
                self.optimizer,
                T_0=10,  # é‡å¯å‘¨æœŸ
                T_mult=2,
                eta_min=self.config.get('min_learning_rate', 1e-6)
            )
        else:
            # ä¿æŒåŸæ¥çš„è°ƒåº¦å™¨
            self.lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer, 
                T_max=self.config['num_epochs'],
                eta_min=self.config.get('min_learning_rate', 1e-6)
            )
        
        print("âœ… ä¼˜åŒ–å™¨è®¾ç½®å®Œæˆ")

    # ---- GPU å†…å­˜ç›‘æ§/å·¥å…·æ–¹æ³• ----
    def _log_gpu_memory(self, label: str = ""):
        if self.device.type != 'cuda' or not self.enable_mem_logging:
            return
        try:
            allocated = torch.cuda.memory_allocated() / 1024**2
            reserved = torch.cuda.memory_reserved() / 1024**2
            max_alloc = torch.cuda.max_memory_allocated() / 1024**2
            print(f"[GPU MEM] {label} allocated={allocated:.1f}MB reserved={reserved:.1f}MB max_alloc={max_alloc:.1f}MB")
        except Exception:
            pass

    def _optimizer_state_cpu(self):
        """Return a CPU copy of optimizer.state_dict() to avoid storing GPU tensors when saving."""
        state = self.optimizer.state_dict()
        state_cpu = {'state': {}, 'param_groups': state.get('param_groups', [])}
        for k, v in state.get('state', {}).items():
            state_cpu['state'][k] = {}
            for kk, vv in v.items():
                try:
                    if isinstance(vv, torch.Tensor):
                        state_cpu['state'][k][kk] = vv.cpu()
                    else:
                        state_cpu['state'][k][kk] = vv
                except Exception:
                    state_cpu['state'][k][kk] = vv
        return state_cpu

    def _cleanup_temps(self, *tensors, force_gc: bool = False):
        """Delete provided tensor references and optionally run GC + empty_cache."""
        for t in tensors:
            try:
                del t
            except Exception:
                pass
        if force_gc:
            torch.cuda.synchronize()
            torch.cuda.empty_cache()
            gc.collect()
    
    def prepare_control_image(self, input_frames):
        """
        å‡†å¤‡æ§åˆ¶å›¾åƒ - ä½¿ç”¨å¢å¼ºçš„æ—¶åºç‰¹å¾
        input_frames: (B, 20, 3, H, W)
        """
        try:
            # ä½¿ç”¨å¢å¼ºæ—¶åºç‰¹å¾æå–å™¨
            control_images = self.temporal_extractor(input_frames)
            
            # ç¡®ä¿è¾“å‡ºåœ¨åˆç†èŒƒå›´å†…
            control_images = torch.clamp(control_images, -1.0, 1.0)
            control_images = (control_images + 1.0) / 2.0  # è½¬æ¢åˆ° [0, 1]
            
            return control_images
            
        except Exception as e:
            print(f"âš ï¸ æ—¶åºç‰¹å¾æå–å¤±è´¥: {e}, ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ")
            return self.prepare_backup_control_image(input_frames)
    
    def prepare_backup_control_image(self, input_frames):
        """å¤‡ç”¨æ§åˆ¶å›¾åƒç”Ÿæˆ"""
        batch_size = input_frames.shape[0]
        control_images = []
        
        for i in range(batch_size):
            # ä½¿ç”¨æœ€åä¸€å¸§çš„Cannyè¾¹ç¼˜
            last_frame = input_frames[i, -1]
            img_np = (last_frame.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)
            img_gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)
            
            median_intensity = np.median(img_gray)
            lower_threshold = int(max(0, 0.66 * median_intensity))
            upper_threshold = int(min(255, 1.33 * median_intensity))
            
            edges = cv2.Canny(img_gray, lower_threshold, upper_threshold)
            edges = np.repeat(edges[:, :, None], 3, axis=2)
            control_tensor = torch.from_numpy(edges).permute(2, 0, 1).float() / 255.0
            control_images.append(control_tensor)
        
        return torch.stack(control_images).to(self.device)
    
    def encode_images(self, images):
        """å°†å›¾åƒç¼–ç åˆ°æ½œåœ¨ç©ºé—´"""
        images = images * 2.0 - 1.0  # è½¬æ¢åˆ° [-1, 1]
        
        with torch.no_grad():
            if self.device.type == 'cuda' and images.dtype != torch.float16:
                with torch.cuda.amp.autocast():
                    latents = self.vae.encode(images.to(torch.float16)).latent_dist.sample()
            else:
                latents = self.vae.encode(images).latent_dist.sample()
                
            latents = latents * self.vae.config.scaling_factor
            
        return latents
    
    def encode_text(self, text_list):
        """ç¼–ç æ–‡æœ¬æè¿°"""
        if not text_list or all(text == '' for text in text_list):
            text_list = ['a person interacting with an object'] * len(text_list) if text_list else ['interaction']
            
        inputs = self.tokenizer(
            text_list, 
            padding="max_length", 
            max_length=77, 
            truncation=True, 
            return_tensors="pt"
        )
        
        with torch.no_grad():
            text_embeddings = self.text_encoder(inputs.input_ids.to(self.device))[0]
            
        return text_embeddings
    
    def train_epoch(self, train_loader, epoch):
        """å†…å­˜ä¼˜åŒ–çš„è®­ç»ƒepoch"""
        self.controlnet.train()
        total_loss = 0
        num_batches = 0
        
        # æ‰‹åŠ¨æ¸…ç†å†…å­˜
        torch.cuda.empty_cache()
        
        for batch_idx, batch in enumerate(train_loader):
            if batch is None:
                continue
                
            try:
                # å‡†å¤‡æ•°æ®
                input_frames = batch['input_frames'].to(self.device)
                target_frames = batch['target_frame'].to(self.device)
                text_descriptions = batch.get('label_text', ['moving object'] * len(input_frames))
                
                # ä½¿ç”¨æ›´èŠ‚çœå†…å­˜çš„æ··åˆç²¾åº¦
                with torch.amp.autocast('cuda', enabled=True):  # ä¿®å¤çš„API
                    # ç¼–ç ç›®æ ‡å›¾åƒåˆ°æ½œåœ¨ç©ºé—´
                    target_latents = self.encode_images(target_frames)
                    
                    # ç¼–ç æ–‡æœ¬
                    text_embeddings = self.encode_text(text_descriptions)
                    
                    # å‡†å¤‡æ§åˆ¶å›¾åƒ
                    control_images = self.prepare_control_image(input_frames)
                    
                    # æ·»åŠ å™ªå£°
                    noise = torch.randn_like(target_latents)
                    timesteps = torch.randint(
                        0, self.noise_scheduler.config.num_train_timesteps, 
                        (target_latents.shape[0],), device=self.device
                    ).long()
                    
                    noisy_latents = self.noise_scheduler.add_noise(target_latents, noise, timesteps)
                    
                    # ControlNetå‰å‘ä¼ æ’­
                    down_block_res_samples, mid_block_res_sample = self.controlnet(
                        noisy_latents,
                        timesteps,
                        encoder_hidden_states=text_embeddings,
                        controlnet_cond=control_images,
                        return_dict=False,
                    )
                    
                    # åº”ç”¨æ¡ä»¶ç¼©æ”¾
                    down_block_res_samples, mid_block_res_sample = self.apply_conditioning_scale(
                        down_block_res_samples, mid_block_res_sample
                    )
                    
                    # UNetå‰å‘ä¼ æ’­
                    noise_pred = self.unet(
                        noisy_latents,
                        timesteps,
                        encoder_hidden_states=text_embeddings,
                        down_block_additional_residuals=down_block_res_samples,
                        mid_block_additional_residual=mid_block_res_sample,
                    ).sample
                    
                    # è®¡ç®—æŸå¤±
                    loss = F.mse_loss(noise_pred, noise)
                
                # åå‘ä¼ æ’­
                self.optimizer.zero_grad()
                
                if self.scaler:
                    self.scaler.scale(loss).backward()
                    
                    # æ¢¯åº¦è£å‰ª
                    if self.config.get('grad_clip', 0) > 0:
                        self.scaler.unscale_(self.optimizer)
                        torch.nn.utils.clip_grad_norm_(
                            self.controlnet.parameters(), 
                            self.config['grad_clip']
                        )
                    
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    loss.backward()
                    if self.config.get('grad_clip', 0) > 0:
                        torch.nn.utils.clip_grad_norm_(
                            self.controlnet.parameters(), 
                            self.config['grad_clip']
                        )
                    self.optimizer.step()
                
                total_loss += loss.item()
                num_batches += 1
                
                # å®šæœŸæ‰“å°æ˜¾å­˜ä¿¡æ¯å’Œæ¸…ç†ä¸´æ—¶å˜é‡
                if batch_idx % self.mem_log_steps == 0:
                    self._log_gpu_memory(f"Epoch{epoch} Batch{batch_idx} post-step")

                # åˆ é™¤å¤§å¼ é‡å¼•ç”¨ä»¥ä¾¿å›æ”¶
                try:
                    del target_latents, text_embeddings, control_images, noise, timesteps, noisy_latents
                    del down_block_res_samples, mid_block_res_sample, noise_pred
                except Exception:
                    pass

                # æ¯éš” cleanup_steps è§¦å‘æ˜¾å­˜æ¸…ç†å’Œ GC
                if (batch_idx + 1) % self.cleanup_steps == 0:
                    torch.cuda.synchronize()
                    torch.cuda.empty_cache()
                    gc.collect()

                if batch_idx % 10 == 0:
                    current_lr = self.optimizer.param_groups[0]['lr']
                    print(f"  Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}, "
                        f"LR: {current_lr:.2e}, Scale: {self.current_scale:.3f}")
                        
            except RuntimeError as e:
                if "out of memory" in str(e):
                    print(f"âš ï¸  æ‰¹æ¬¡ {batch_idx} å†…å­˜ä¸è¶³ï¼Œè·³è¿‡")
                    torch.cuda.empty_cache()
                    gc.collect()
                    continue
                else:
                    raise
        
        return total_loss / num_batches if num_batches > 0 else 0.0
    
    def validate(self, val_loader):
        """éªŒè¯æ¨¡å‹ - åªä½¿ç”¨éªŒè¯é›†"""
        self.controlnet.eval()
        total_loss = 0
        num_batches = 0
        
        with torch.no_grad():
            for batch in val_loader:
                if batch is None:
                    continue
                    
                input_frames = batch['input_frames'].to(self.device)
                target_frames = batch['target_frame'].to(self.device)
                text_descriptions = batch.get('label_text', ['moving object'] * len(input_frames))
                
                with torch.cuda.amp.autocast(enabled=self.device.type == 'cuda'):
                    target_latents = self.encode_images(target_frames)
                    text_embeddings = self.encode_text(text_descriptions)
                    control_images = self.prepare_control_image(input_frames)
                    
                    noise = torch.randn_like(target_latents)
                    timesteps = torch.randint(
                        0, self.noise_scheduler.config.num_train_timesteps, 
                        (target_latents.shape[0],), device=self.device
                    ).long()
                    
                    noisy_latents = self.noise_scheduler.add_noise(target_latents, noise, timesteps)
                    
                    down_block_res_samples, mid_block_res_sample = self.controlnet(
                        noisy_latents,
                        timesteps,
                        encoder_hidden_states=text_embeddings,
                        controlnet_cond=control_images,
                        return_dict=False,
                    )
                    
                    # ğŸ¯ éªŒè¯æ—¶ä¹Ÿåº”ç”¨ç›¸åŒçš„æ¡ä»¶ç¼©æ”¾
                    down_block_res_samples, mid_block_res_sample = self.apply_conditioning_scale(
                        down_block_res_samples, mid_block_res_sample
                    )
                    
                    noise_pred = self.unet(
                        noisy_latents,
                        timesteps,
                        encoder_hidden_states=text_embeddings,
                        down_block_additional_residuals=down_block_res_samples,
                        mid_block_additional_residual=mid_block_res_sample,
                    ).sample
                    
                    loss = F.mse_loss(noise_pred, noise)
                
                total_loss += loss.item()
                num_batches += 1

                # éªŒè¯é˜¶æ®µä¹Ÿå®šæœŸæ¸…ç†å’Œæ‰“å°æ˜¾å­˜
                if num_batches % self.mem_log_steps == 0:
                    self._log_gpu_memory(f"Validate Batch {num_batches}")

                try:
                    del target_latents, text_embeddings, control_images, noise, timesteps, noisy_latents
                    del down_block_res_samples, mid_block_res_sample, noise_pred
                except Exception:
                    pass

                if num_batches % self.cleanup_steps == 0:
                    torch.cuda.synchronize()
                    torch.cuda.empty_cache()
                    gc.collect()
        
        return total_loss / num_batches if num_batches > 0 else 0.0
    
    def train(self, train_loader, val_loader):
        """ä¸»è®­ç»ƒå¾ªç¯ - åªä½¿ç”¨è®­ç»ƒé›†å’ŒéªŒè¯é›†"""
        print("ğŸš€ å¼€å§‹è®­ç»ƒå¾ªç¯...")
        print("ğŸ“Š ä»…ä½¿ç”¨è®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼Œæµ‹è¯•é›†ä¿ç•™ç”¨äºæœ€ç»ˆè¯„ä¼°")
        
        best_val_loss = float('inf')
        train_losses = []
        val_losses = []
        scale_history = []
        
        for epoch in range(1, self.config['num_epochs'] + 1):
            print(f"\n=== Epoch {epoch}/{self.config['num_epochs']} ===")
            
            # ğŸ¯ åœ¨æ¯ä¸ªepochå¼€å§‹æ—¶æ›´æ–°æ¡ä»¶ç¼©æ”¾
            if epoch > 1:
                self.update_conditioning_scale(
                    epoch, self.config['num_epochs'], 
                    train_losses[-1] if train_losses else None,
                    val_losses[-1] if val_losses else None
                )
            
            # è®­ç»ƒï¼ˆè®­ç»ƒé›†ï¼‰
            train_loss = self.train_epoch(train_loader, epoch)
            train_losses.append(train_loss)
            
            # éªŒè¯ï¼ˆéªŒè¯é›†ï¼‰
            val_loss = self.validate(val_loader)
            val_losses.append(val_loss)
            scale_history.append(self.current_scale)
            
            # æ›´æ–°å­¦ä¹ ç‡
            if self.lr_scheduler:
                self.lr_scheduler.step()
            
            print(f"âœ… Epoch {epoch} å®Œæˆ")
            print(f"   è®­ç»ƒæŸå¤±: {train_loss:.4f}")
            print(f"   éªŒè¯æŸå¤±: {val_loss:.4f}")
            print(f"   æ¡ä»¶ç¼©æ”¾: {self.current_scale:.3f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                self.save_checkpoint(epoch, val_loss, is_best=True)
                print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯æŸå¤±: {val_loss:.4f}")
            
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if epoch % self.config['save_interval'] == 0:
                self.save_checkpoint(epoch, val_loss)
        
        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿å’Œç¼©æ”¾å†å²
        self.plot_training_metrics(train_losses, val_losses, scale_history)
        print("ğŸ‰ è®­ç»ƒå®Œæˆ!")
    
    def save_checkpoint(self, epoch, val_loss, is_best=False):
        """ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹"""
        # ä¸ºäº†å‡å°‘åœ¨ä¿å­˜æ—¶çš„GPUå†…å­˜å ç”¨ï¼ŒæŠŠ optimizer state è½¬åˆ° CPU
        optimizer_state_cpu = None
        try:
            optimizer_state_cpu = self._optimizer_state_cpu()
        except Exception:
            optimizer_state_cpu = self.optimizer.state_dict()

        checkpoint = {
            'epoch': epoch,
            'controlnet_state_dict': self.controlnet.state_dict(),
            'optimizer_state_dict': optimizer_state_cpu,
            'lr_scheduler_state_dict': self.lr_scheduler.state_dict() if self.lr_scheduler else None,
            'val_loss': val_loss,
            'config': self.config,
            'task_name': self.task_name,
            'current_scale': self.current_scale,  # ğŸ†• ä¿å­˜å½“å‰ç¼©æ”¾å€¼
            'conditioning_strategy': self.conditioning_strategy  # ğŸ†• ä¿å­˜ç­–ç•¥
        }
        
        output_dir = Path(self.config['output_dir'])
        output_dir.mkdir(exist_ok=True)
        
        if is_best:
            filename = f"controlnet_{self.task_name}_best.pth"
        else:
            filename = f"controlnet_{self.task_name}_epoch_{epoch}.pth"
        
        save_path = output_dir / filename
        torch.save(checkpoint, save_path)
        print(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: {save_path}")
    
    def plot_training_metrics(self, train_losses, val_losses, scale_history):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿å’Œç¼©æ”¾å†å²"""
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))
        
        # æŸå¤±æ›²çº¿
        ax1.plot(train_losses, label='Training Loss', linewidth=2, color='blue')
        ax1.plot(val_losses, label='Validation Loss', linewidth=2, color='red')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.set_title(f'ControlNet Training - {self.task_name}\n20 frames â†’ 25th frame prediction')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # ç¼©æ”¾å†å²
        ax2.plot(scale_history, label='Conditioning Scale', linewidth=2, color='green')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Scale Value')
        ax2.set_title('Dynamic Conditioning Scale History')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        output_dir = Path(self.config['output_dir'])
        plt.savefig(output_dir / f'training_metrics_{self.task_name}.png', dpi=300, bbox_inches='tight')
        plt.close()

def main():
    # ğŸ¯ åŸºç¡€è®­ç»ƒé…ç½®
    base_config = {
        # åŸºç¡€è®­ç»ƒå‚æ•°
        'learning_rate': 1e-4,
        'min_learning_rate': 1e-6,
        'weight_decay': 1e-3,
        'num_epochs': 50,
        'batch_size': 2,
        'save_interval': 10,

        # æ·»åŠ æ¢¯åº¦ç´¯ç§¯
        'gradient_accumulation_steps': 4,  # æ–°çš„å‚æ•°

        # ä¼˜åŒ–å™¨å‚æ•°
        'lr_scheduler': 'cosine',
        'lr_step_size': 20,
        'lr_gamma': 0.5,
        'warmup_steps': 500,                  # æ–°çš„å‚æ•°
        
        # è®­ç»ƒç­–ç•¥å‚æ•°
        'grad_clip': 1.0,
        
        # ğŸ¯ åŠ¨æ€æ¡ä»¶ç¼©æ”¾å‚æ•°
        'conditioning_strategy': 'linear_increase',  # 'fixed', 'linear_increase', 'adaptive', 'stepwise', 'cosine'
        'initial_conditioning_scale': 0.6,
        'final_conditioning_scale': 1.0,
        'adaptive_threshold': 0.15,
        'scale_step': 0.05,
    }
    
    # ğŸ¯ å®šä¹‰ä¸‰ä¸ªä»»åŠ¡
    tasks = [
        {
            'name': 'move_object',
            'display_name': 'ç§»åŠ¨ç‰©ä½“',
            'config_override': {
                'learning_rate': 1e-4,
                'num_epochs': 50,
                'conditioning_strategy': 'adaptive',
                'initial_conditioning_scale': 0.2,  # ç§»åŠ¨ä»»åŠ¡å¯ä»¥æ›´å®½æ¾
                'final_conditioning_scale': 1.0,
                'gtad_clip': 0.5,
                'weight_decay':1e-2,
            }
        },
        {
            'name': 'drop_object', 
            'display_name': 'æ‰è½ç‰©ä½“',
            'config_override': {
                'learning_rate': 1e-4,
                'num_epochs': 50,
                'conditioning_strategy': 'linear_increase',  # æ‰è½ä»»åŠ¡éœ€è¦é€æ¸åŠ å¼ºæ§åˆ¶
                'initial_conditioning_scale': 0.8,
                'final_conditioning_scale': 1.3,
            }
        },
        {
            'name': 'cover_object',
            'display_name': 'è¦†ç›–ç‰©ä½“', 
            'config_override': {
                'learning_rate': 8e-6,
                'num_epochs': 50,
                'conditioning_strategy': 'stepwise',  # è¦†ç›–ä»»åŠ¡åˆ†é˜¶æ®µæ§åˆ¶
                'initial_conditioning_scale': 0.8,
                'final_conditioning_scale': 1.2,
            }
        }
    ]
    
    print("=" * 60)
    print("ğŸ¯ ControlNet 1.1 å¤šä»»åŠ¡åˆ†åˆ«è®­ç»ƒï¼ˆåŠ¨æ€æ¡ä»¶ç¼©æ”¾ï¼‰")
    print("ğŸ“Š ä»»åŠ¡: å‰20å¸§ â†’ é¢„æµ‹ç¬¬25å¸§")
    print("=" * 60)
    
    # ğŸ”„ åˆ†åˆ«è®­ç»ƒæ¯ä¸ªä»»åŠ¡
    for task_info in tasks:
        task_name = task_info['name']
        task_display = task_info['display_name']
        
        print(f"\n{'='*50}")
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒä»»åŠ¡: {task_display} ({task_name})")
        print(f"{'='*50}")
        
        # åˆå¹¶é…ç½®
        task_config = {**base_config, **task_info['config_override']}
        task_config['output_dir'] = f'training_results_{task_name}'
        task_config['task_name'] = task_name
        
        # æ‰“å°ä»»åŠ¡ç‰¹å®šé…ç½®
        print(f"ä»»åŠ¡é…ç½®:")
        for key, value in task_config.items():
            if key in task_info['config_override']:
                print(f"  {key}: {value} ğŸ¯")
            else:
                print(f"  {key}: {value}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path(task_config['output_dir'])
        output_dir.mkdir(exist_ok=True)
        
        # ğŸ—‚ï¸ åŠ è½½ä»»åŠ¡ç‰¹å®šæ•°æ®
        print(f"\nğŸ“Š åŠ è½½ {task_display} æ•°æ®...")
        try:
            train_loader, val_loader, test_loader = create_task_specific_loaders(
                task_name=task_name,
                batch_size=task_config['batch_size'],
                data_path="processed_data"
            )
            
            # æ£€æŸ¥æ•°æ®æ˜¯å¦è¶³å¤Ÿ
            if len(train_loader.dataset) == 0:
                print(f"âŒ ä»»åŠ¡ {task_name} æ²¡æœ‰è®­ç»ƒæ•°æ®ï¼Œè·³è¿‡")
                continue
                
            print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ")
            print(f"   è®­ç»ƒé›†: {len(train_loader.dataset)} æ ·æœ¬")
            print(f"   éªŒè¯é›†: {len(val_loader.dataset)} æ ·æœ¬") 
            print(f"   æµ‹è¯•é›†: {len(test_loader.dataset)} æ ·æœ¬ - ä¿ç•™ç”¨äºæœ€ç»ˆè¯„ä¼°")
            
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            continue
        
        # ğŸ¤– åˆå§‹åŒ–è®­ç»ƒå™¨
        try:
            trainer = DynamicConditioningControlNetTrainer(task_config)
        except Exception as e:
            print(f"âŒ è®­ç»ƒå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            continue
        
        # ğŸ‹ï¸ å¼€å§‹è®­ç»ƒ
        try:
            trainer.train(train_loader, val_loader)
            print(f"ğŸ‰ ä»»åŠ¡ {task_display} è®­ç»ƒå®Œæˆ!")
            
        except KeyboardInterrupt:
            print(f"\nâš ï¸ ä»»åŠ¡ {task_display} è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        except Exception as e:
            print(f"\nâŒ ä»»åŠ¡ {task_display} è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    print(f"\n{'='*60}")
    print("ğŸŠ æ‰€æœ‰ä»»åŠ¡è®­ç»ƒå®Œæˆ!")
    print("ğŸ“ æ¯ä¸ªä»»åŠ¡çš„æ¨¡å‹ä¿å­˜åœ¨å„è‡ªçš„ç›®å½•ä¸­:")
    for task_info in tasks:
        print(f"   {task_info['display_name']}: training_results_{task_info['name']}/")
    print(f"{'='*60}")

# å¯é€‰ï¼šä¿ç•™åŸæ¥çš„mainå‡½æ•°ç”¨äºå•ä¸ªä»»åŠ¡è®­ç»ƒ
def main_single_task():
    """å•ä¸ªä»»åŠ¡è®­ç»ƒï¼ˆç”¨äºè°ƒè¯•æˆ–ç‰¹å®šä»»åŠ¡ï¼‰"""
    config = {
        'learning_rate': 1e-5,
        'num_epochs': 50,
        'batch_size': 2,
        'save_interval': 10,
        'output_dir': 'training_results_single',
        'task_name': 'drop_object',  # æŒ‡å®šå•ä¸ªä»»åŠ¡
        
        # åŠ¨æ€æ¡ä»¶ç¼©æ”¾å‚æ•°
        'conditioning_strategy': 'linear_increase',
        'initial_conditioning_scale': 0.8,
        'final_conditioning_scale': 1.3,
        'adaptive_threshold': 0.15,
        'scale_step': 0.05,

        'min_learning_rate': 1e-6,
        'weight_decay': 1e-3,

        # æ·»åŠ æ¢¯åº¦ç´¯ç§¯
        'gradient_accumulation_steps': 4,  # æ–°çš„å‚æ•°

        # ä¼˜åŒ–å™¨å‚æ•°
        'lr_scheduler': 'cosine',
        'lr_step_size': 20,
        'lr_gamma': 0.5,
        'warmup_steps': 500,                  # æ–°çš„å‚æ•°
        
        # è®­ç»ƒç­–ç•¥å‚æ•°
        'grad_clip': 1.0,
    }
    
    train_loader, val_loader, _ = create_task_specific_loaders(
        task_name=config['task_name'],
        batch_size=config['batch_size']
    )
    
    trainer = DynamicConditioningControlNetTrainer(config)
    trainer.train(train_loader, val_loader)

if __name__ == "__main__":
    #main()  # ä½¿ç”¨å¤šä»»åŠ¡è®­ç»ƒ
    main_single_task()  # æˆ–è€…ä½¿ç”¨å•ä»»åŠ¡è®­ç»ƒ