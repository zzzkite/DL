#!/usr/bin/env python3
"""
éªŒè¯æå–çš„æ•°æ®æ˜¯å¦æ­£ç¡® - é€‚é…å‰20å¸§â†’ç¬¬25å¸§è®¾ç½®
"""

import numpy as np
import pandas as pd
from pathlib import Path
import cv2

def verify_data():
    """éªŒè¯æ•°æ®å®Œæ•´æ€§"""
    print("å¼€å§‹éªŒè¯æ•°æ®...")
    print("å¸§è®¾ç½®: è¾“å…¥å‰20å¸§ â†’ é¢„æµ‹ç¬¬25å¸§ï¼ˆè·³è¿‡4å¸§ï¼‰")
    
    # åŠ è½½å…ƒæ•°æ®
    metadata_dir = Path("processed_data/metadata")
    
    # æ£€æŸ¥æ‰€æœ‰æ•°æ®é›†æ–‡ä»¶
    dataset_files = {
        "æ‰€æœ‰æ ·æœ¬": "all_samples.csv",
        "è®­ç»ƒé›†": "train_samples.csv", 
        "éªŒè¯é›†": "val_samples.csv",
        "æµ‹è¯•é›†": "test_samples.csv"
    }
    
    for name, filename in dataset_files.items():
        file_path = metadata_dir / filename
        if file_path.exists():
            df = pd.read_csv(file_path)
            print(f"âœ… {name}: {len(df)} ä¸ªæ ·æœ¬")
        else:
            print(f"âŒ {name}æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    # åŠ è½½å®Œæ•´æ•°æ®é›†ä¿¡æ¯
    all_samples = pd.read_csv(metadata_dir / "all_samples.csv")
    
    print(f"\næ€»æ ·æœ¬æ•°: {len(all_samples)}")
    
    # æ£€æŸ¥æ•°æ®é›†åˆ’åˆ†æ¯”ä¾‹
    train_samples = pd.read_csv(metadata_dir / "train_samples.csv")
    val_samples = pd.read_csv(metadata_dir / "val_samples.csv")
    test_samples = pd.read_csv(metadata_dir / "test_samples.csv")
    
    total_actual = len(train_samples) + len(val_samples) + len(test_samples)
    print(f"è®­ç»ƒé›†: {len(train_samples)} ä¸ªæ ·æœ¬ ({len(train_samples)/total_actual*100:.1f}%)")
    print(f"éªŒè¯é›†: {len(val_samples)} ä¸ªæ ·æœ¬ ({len(val_samples)/total_actual*100:.1f}%)")
    print(f"æµ‹è¯•é›†: {len(test_samples)} ä¸ªæ ·æœ¬ ({len(test_samples)/total_actual*100:.1f}%)")
    
    # æ£€æŸ¥æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬
    categories = all_samples['category'].unique()
    
    print(f"\n=== è¯¦ç»†æ ·æœ¬æ£€æŸ¥ ===")
    for category in categories:
        category_samples = all_samples[all_samples['category'] == category]
        print(f"\n{category}: {len(category_samples)} ä¸ªæ ·æœ¬")
        
        # éšæœºæ£€æŸ¥å‡ ä¸ªæ ·æœ¬
        for i, sample in category_samples.head(2).iterrows():
            print(f"  æ£€æŸ¥æ ·æœ¬ {sample['video_id']}:")
            
            # æ£€æŸ¥è¾“å…¥å¸§
            try:
                input_frames = np.load(sample['input_frames_path'])
                expected_shape = (20, 96, 96, 3)  # æœŸæœ›çš„å½¢çŠ¶
                actual_shape = input_frames.shape
                print(f"    âœ… è¾“å…¥å¸§: {actual_shape} (æœŸæœ›: {expected_shape})")
                
                if actual_shape != expected_shape:
                    print(f"    âš ï¸  è¾“å…¥å¸§å½¢çŠ¶ä¸åŒ¹é…!")
                
                # æ£€æŸ¥ç›®æ ‡å¸§
                target_frame = np.load(sample['target_frame_path'])
                expected_target_shape = (96, 96, 3)
                actual_target_shape = target_frame.shape
                print(f"    âœ… ç›®æ ‡å¸§: {actual_target_shape} (æœŸæœ›: {expected_target_shape})")
                
                if actual_target_shape != expected_target_shape:
                    print(f"    âš ï¸  ç›®æ ‡å¸§å½¢çŠ¶ä¸åŒ¹é…!")
                
                # æ£€æŸ¥æ•°æ®èŒƒå›´
                print(f"    âœ… è¾“å…¥å¸§èŒƒå›´: [{input_frames.min():.1f}, {input_frames.max():.1f}]")
                print(f"    âœ… ç›®æ ‡å¸§èŒƒå›´: [{target_frame.min():.1f}, {target_frame.max():.1f}]")
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆå›¾åƒæ•°æ®
                if input_frames.max() <= 1.0:
                    print(f"    âš ï¸  è¾“å…¥å¸§å¯èƒ½å·²ç»å½’ä¸€åŒ– (æœ€å¤§å€¼: {input_frames.max():.3f})")
                if target_frame.max() <= 1.0:
                    print(f"    âš ï¸  ç›®æ ‡å¸§å¯èƒ½å·²ç»å½’ä¸€åŒ– (æœ€å¤§å€¼: {target_frame.max():.3f})")
                
            except Exception as e:
                print(f"    âŒ åŠ è½½å¤±è´¥: {e}")
    
    # æ£€æŸ¥ç¤ºä¾‹å›¾åƒ
    print(f"\n=== ç¤ºä¾‹å›¾åƒæ£€æŸ¥ ===")
    samples_dir = Path("processed_data/samples")
    for category in categories:
        category_dir = samples_dir / category
        if category_dir.exists():
            image_files = list(category_dir.glob("*.jpg"))
            print(f"  {category}: {len(image_files)} ä¸ªç¤ºä¾‹å›¾åƒ")
            for img_file in image_files[:2]:  # æ˜¾ç¤ºå‰2ä¸ªæ–‡ä»¶
                print(f"    - {img_file.name}")
        else:
            print(f"  {category}: ç¤ºä¾‹ç›®å½•ä¸å­˜åœ¨")
    
    # æ£€æŸ¥æ•°æ®é›†ä¿¡æ¯æ–‡ä»¶
    print(f"\n=== æ•°æ®é›†ä¿¡æ¯æ£€æŸ¥ ===")
    info_file = metadata_dir / "dataset_info.json"
    if info_file.exists():
        import json
        with open(info_file, 'r') as f:
            dataset_info = json.load(f)
        print(f"æ•°æ®é›†åç§°: {dataset_info.get('name', 'N/A')}")
        print(f"æ€»æ ·æœ¬æ•°: {dataset_info.get('total_samples', 'N/A')}")
        print(f"è®­ç»ƒé›†: {dataset_info.get('train_samples', 'N/A')}")
        print(f"éªŒè¯é›†: {dataset_info.get('val_samples', 'N/A')}")
        print(f"æµ‹è¯•é›†: {dataset_info.get('test_samples', 'N/A')}")
        print(f"åˆ’åˆ†æ¯”ä¾‹: {dataset_info.get('split_ratio', 'N/A')}")
        
        frame_info = dataset_info.get('frame_info', {})
        print(f"è¾“å…¥å¸§æ•°: {frame_info.get('input_frames', 'N/A')}")
        print(f"ç›®æ ‡å¸§æ•°: {frame_info.get('target_frame', 'N/A')}")
        print(f"è·³è¿‡å¸§æ•°: {frame_info.get('skip_frames', 'N/A')}")
        print(f"åˆ†è¾¨ç‡: {frame_info.get('resolution', 'N/A')}")
    else:
        print("âŒ æ•°æ®é›†ä¿¡æ¯æ–‡ä»¶ä¸å­˜åœ¨")
    
    print(f"\nğŸ‰ æ•°æ®éªŒè¯å®Œæˆ!")
    print(f"å¸§è®¾ç½®ç¡®è®¤: è¾“å…¥å‰20å¸§ â†’ é¢„æµ‹ç¬¬25å¸§ï¼ˆè·³è¿‡4å¸§ï¼‰")

if __name__ == "__main__":
    verify_data()