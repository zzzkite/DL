#!/usr/bin/env python3
"""
PyTorchæ•°æ®åŠ è½½å™¨ - æ”¯æŒä»»åŠ¡ç‰¹å®šè®­ç»ƒ
"""

import torch
from torch.utils.data import Dataset, DataLoader
import numpy as np
import pandas as pd
from pathlib import Path
import torchvision.transforms as transforms

class TaskSpecificDataset(Dataset):
    def __init__(self, metadata_file, task_name=None, transform=None):
        """
        ä»»åŠ¡ç‰¹å®šæ•°æ®é›†ç±» - å¯ç­›é€‰ç‰¹å®šä»»åŠ¡
        
        Args:
            metadata_file: å…ƒæ•°æ®CSVæ–‡ä»¶è·¯å¾„
            task_name: ä»»åŠ¡åç§° ('move_object', 'drop_object', 'cover_object')ï¼ŒNoneè¡¨ç¤ºæ‰€æœ‰ä»»åŠ¡
            transform: æ•°æ®å˜æ¢
        """
        self.metadata = pd.read_csv(metadata_file)
        
        # å¦‚æœæŒ‡å®šäº†ä»»åŠ¡åç§°ï¼Œç­›é€‰æ•°æ®
        if task_name and task_name != 'all':
            original_count = len(self.metadata)
            self.metadata = self.metadata[self.metadata['category'] == task_name]
            print(f"   ç­›é€‰ä»»åŠ¡ '{task_name}': {len(self.metadata)}/{original_count} ä¸ªæ ·æœ¬")
        
        self.transform = transform
        self.task_name = task_name
        
        # æ‰“å°æ•°æ®é›†ä¿¡æ¯
        task_display = task_name if task_name else 'æ‰€æœ‰ä»»åŠ¡'
        print(f"âœ… åŠ è½½æ•°æ®é›† ({task_display}): {len(self.metadata)} ä¸ªæ ·æœ¬")
        print(f"   å¸§è®¾ç½®: è¾“å…¥å‰20å¸§ â†’ é¢„æµ‹ç¬¬25å¸§ï¼ˆè·³è¿‡4å¸§ï¼‰")
        
        # æ˜¾ç¤ºç±»åˆ«åˆ†å¸ƒ
        category_counts = self.metadata['category'].value_counts()
        for category, count in category_counts.items():
            print(f"   {category}: {count} ä¸ªæ ·æœ¬")
    
    def __len__(self):
        return len(self.metadata)
    
    def __getitem__(self, idx):
        sample = self.metadata.iloc[idx]
        
        try:
            # åŠ è½½è¾“å…¥å¸§ï¼ˆå‰20å¸§ï¼‰
            input_frames = np.load(sample['input_frames_path'])  # (20, 96, 96, 3)
            # åŠ è½½ç›®æ ‡å¸§ï¼ˆç¬¬25å¸§ï¼‰
            target_frame = np.load(sample['target_frame_path'])  # (96, 96, 3)
            
            # è½¬æ¢ä¸ºPyTorchå¼ é‡
            input_frames = torch.from_numpy(input_frames).float()
            target_frame = torch.from_numpy(target_frame).float()
            
            # è°ƒæ•´ç»´åº¦é¡ºåº: (T, H, W, C) -> (T, C, H, W)
            input_frames = input_frames.permute(0, 3, 1, 2)  # (20, 3, 96, 96)
            target_frame = target_frame.permute(2, 0, 1)     # (3, 96, 96)
            
            # å½’ä¸€åŒ–åˆ° [0, 1] (å¦‚æœæ•°æ®åœ¨0-255èŒƒå›´å†…)
            input_frames = input_frames / 255.0
            target_frame = target_frame / 255.0
            
            # åº”ç”¨æ•°æ®å˜æ¢ï¼ˆå¦‚æœæœ‰ï¼‰
            if self.transform:
                input_frames = self.transform(input_frames)
                target_frame = self.transform(target_frame)
            
            return {
                'input_frames': input_frames,  # (20, 3, 96, 96)
                'target_frame': target_frame,  # (3, 96, 96)
                'category': sample['category'],
                'video_id': sample['video_id'],
                'template': sample['template'],
                'label_text': sample['label']
            }
            
        except Exception as e:
            print(f"âŒ åŠ è½½æ ·æœ¬å¤±è´¥ {sample['video_id']}: {e}")
            return None
    
    def get_category_indices(self, category):
        """è·å–ç‰¹å®šç±»åˆ«çš„æ‰€æœ‰ç´¢å¼•"""
        return self.metadata[self.metadata['category'] == category].index.tolist()

def create_task_specific_loaders(task_name='all', batch_size=8, data_path="processed_data", num_workers=4):
    """
    åˆ›å»ºä»»åŠ¡ç‰¹å®šçš„è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•æ•°æ®åŠ è½½å™¨
    
    Args:
        task_name: ä»»åŠ¡åç§° ('move_object', 'drop_object', 'cover_object', 'all')
        batch_size: æ‰¹æ¬¡å¤§å°
        data_path: å¤„ç†æ•°æ®çš„è·¯å¾„
        num_workers: æ•°æ®åŠ è½½çš„è¿›ç¨‹æ•°
    """
    
    # æ•°æ®å˜æ¢
    transform = transforms.Compose([
        # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æ•°æ®å¢å¼º
    ])
    
    # åˆ›å»ºä»»åŠ¡ç‰¹å®šæ•°æ®é›†
    train_dataset = TaskSpecificDataset(
        f"{data_path}/metadata/train_samples.csv", 
        task_name=task_name,
        transform=transform
    )
    val_dataset = TaskSpecificDataset(
        f"{data_path}/metadata/val_samples.csv",
        task_name=task_name
    )
    test_dataset = TaskSpecificDataset(
        f"{data_path}/metadata/test_samples.csv",
        task_name=task_name
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset, 
        batch_size=batch_size, 
        shuffle=True, 
        num_workers=num_workers,
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset, 
        batch_size=batch_size, 
        shuffle=False, 
        num_workers=num_workers,
        pin_memory=True
    )
    
    test_loader = DataLoader(
        test_dataset, 
        batch_size=batch_size, 
        shuffle=False, 
        num_workers=num_workers,
        pin_memory=True
    )
    
    task_display = task_name if task_name != 'all' else 'æ‰€æœ‰ä»»åŠ¡'
    print(f"âœ… åˆ›å»ºä»»åŠ¡ '{task_display}' æ•°æ®åŠ è½½å™¨å®Œæˆ")
    print(f"   è®­ç»ƒé›†: {len(train_dataset)} ä¸ªæ ·æœ¬, {len(train_loader)} ä¸ªæ‰¹æ¬¡")
    print(f"   éªŒè¯é›†: {len(val_dataset)} ä¸ªæ ·æœ¬, {len(val_loader)} ä¸ªæ‰¹æ¬¡")
    print(f"   æµ‹è¯•é›†: {len(test_dataset)} ä¸ªæ ·æœ¬, {len(test_loader)} ä¸ªæ‰¹æ¬¡")
    print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
    
    return train_loader, val_loader, test_loader

# ä¿ç•™åŸæ¥çš„å‡½æ•°ç”¨äºå…¼å®¹æ€§
def create_data_loaders(batch_size=8, data_path="processed_data", num_workers=4):
    """åˆ›å»ºæ‰€æœ‰ä»»åŠ¡æ··åˆçš„æ•°æ®åŠ è½½å™¨"""
    return create_task_specific_loaders('all', batch_size, data_path, num_workers)

def test_task_specific_loader():
    """æµ‹è¯•ä»»åŠ¡ç‰¹å®šæ•°æ®åŠ è½½å™¨"""
    print("æµ‹è¯•ä»»åŠ¡ç‰¹å®šæ•°æ®åŠ è½½å™¨...")
    
    tasks = ['move_object', 'drop_object', 'cover_object', 'all']
    
    for task in tasks:
        print(f"\n=== æµ‹è¯•ä»»åŠ¡: {task} ===")
        train_loader, val_loader, test_loader = create_task_specific_loaders(
            task_name=task, 
            batch_size=2
        )
        
        # æµ‹è¯•ä¸€ä¸ªè®­ç»ƒæ‰¹æ¬¡
        for batch_idx, batch in enumerate(train_loader):
            if batch_idx >= 1:  # åªæµ‹è¯•ç¬¬ä¸€ä¸ªæ‰¹æ¬¡
                break
                
            print(f"æ‰¹æ¬¡ {batch_idx}:")
            print(f"  è¾“å…¥å¸§å½¢çŠ¶: {batch['input_frames'].shape}")
            print(f"  ç›®æ ‡å¸§å½¢çŠ¶: {batch['target_frame'].shape}")
            print(f"  ç±»åˆ«åˆ†å¸ƒ: {pd.Series(batch['category']).value_counts().to_dict()}")
    
    print("\nğŸ‰ ä»»åŠ¡ç‰¹å®šæ•°æ®åŠ è½½å™¨æµ‹è¯•é€šè¿‡!")

if __name__ == "__main__":
    test_task_specific_loader()