#!/usr/bin/env python3
import sys
import os
import gc
import psutil

# æ·»åŠ  CosyVoice åˆ° Python è·¯å¾„ - ä½¿ç”¨ä¸Žutt2ç›¸åŒçš„è®¾ç½®
current_dir = os.path.dirname(os.path.abspath(__file__))
cosyvoice_path = os.path.join(current_dir, 'CosyVoice')

# æ·»åŠ  CosyVoice ä¸»ç›®å½•
if cosyvoice_path not in sys.path:
    sys.path.insert(0, cosyvoice_path)

# æ·»åŠ  CosyVoice çš„ç¬¬ä¸‰æ–¹ä¾èµ–è·¯å¾„
matcha_path = os.path.join(cosyvoice_path, 'third_party', 'Matcha-TTS')
if os.path.exists(matcha_path) and matcha_path not in sys.path:
    sys.path.insert(0, matcha_path)

print(f"æ·»åŠ  CosyVoice è·¯å¾„: {cosyvoice_path}")

# --- huggingface_hub compatibility patch (for CosyVoice) ---
try:
    import huggingface_hub as _hfh
    if not hasattr(_hfh, "cached_download"):
        from huggingface_hub import hf_hub_download as _hf_hub_download

        def cached_download(*args, **kwargs):
            return _hf_hub_download(*args, **kwargs)

        _hfh.cached_download = cached_download
except Exception:
    pass

# çŽ°åœ¨å¯¼å…¥å…¶ä»–æ¨¡å—
import math
import random
from pathlib import Path

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

# å°è¯•å¯¼å…¥ CosyVoice
try:
    from cosyvoice.cli.cosyvoice import CosyVoice
    print("âœ… CosyVoice å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ CosyVoice å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿ CosyVoice ç›®å½•å­˜åœ¨ä¸”åŒ…å«å¿…è¦çš„æ–‡ä»¶")
    sys.exit(1)


# ======================
#  Config - æ›´æ–°ä¸ºä½ çš„å®žé™…è·¯å¾„
# ======================

# è®­ç»ƒé›†ç‰¹å¾æ–‡ä»¶
TRAIN_UTT2_S3_PATH = "s3_output/train_utt2s3.pt"  
TRAIN_UTT2_TEXT_EMB_PATH = "utt2_output/train_text_emb.pt"  
TRAIN_UTT2_WHISPER_PATH = "utt2_output/train_whisper_feats.pt"    

# æµ‹è¯•é›†ç‰¹å¾æ–‡ä»¶  
TEST_UTT2_S3_PATH = "s3_output/test_utt2s3.pt"  
TEST_UTT2_TEXT_EMB_PATH = "utt2_output/test_text_emb.pt"  
TEST_UTT2_WHISPER_PATH = "utt2_output/test_whisper_feats.pt"    

COSYVOICE_MODEL_DIR = "CosyVoice-300M"

S3_PAD_ID = -1
S3_VOCAB_SIZE = 4096
BATCH_SIZE = 4
LR = 1e-4
WEIGHT_DECAY = 1e-3
NUM_EPOCHS = 10
GRAD_CLIP = 1.0
TRAIN_RATIO = 0.95
IGNORE_ID = -100


def print_memory_usage(device, prefix=""):
    """æ‰“å°å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    if device.type == 'cuda':
        allocated = torch.cuda.memory_allocated(device) / 1024**3
        reserved = torch.cuda.memory_reserved(device) / 1024**3
        print(f"{prefix} GPUå†…å­˜: å·²åˆ†é… {allocated:.2f}GB, ä¿ç•™ {reserved:.2f}GB")
    else:
        memory = psutil.virtual_memory()
        print(f"{prefix} CPUå†…å­˜: ä½¿ç”¨çŽ‡ {memory.percent}% ({memory.used//1024//1024}MB / {memory.total//1024//1024}MB)")


# ======================
#  CosyVoice LLM wrapper
# ======================

def load_cosyvoice_llm(device):
    """åŠ è½½CosyVoice LLMæ¨¡åž‹"""
    print(f"æ­£åœ¨åŠ è½½CosyVoiceæ¨¡åž‹ä»Ž: {COSYVOICE_MODEL_DIR}")
    cosy = CosyVoice(COSYVOICE_MODEL_DIR)
    # è¿”å›žæ•´ä¸ª wrapper (TransformerLM æˆ– Qwen2LM)ï¼Œä»¥ä¾¿è®¿é—® embedding
    llm_wrapper = cosy.model.llm
    print(f"âœ… CosyVoice LLMåŠ è½½æˆåŠŸ")
    return llm_wrapper


class SimpleTextSpeechAggregator(nn.Module):
    """
    æ–‡æœ¬-è¯­éŸ³äº¤å‰æ³¨æ„åŠ›èšåˆå™¨
    Q = text_emb         : (B, T_text, D_text)
    K = speech_last      : (B, T_speech, D_last)
    V = speech_mid       : (B, T_speech, D_mid)

    Output:
        z   : (B, T_text, hidden_dim) - å¯¹é½åŽçš„è¯­éŸ³è¡¨ç¤º
        att : (B, T_text, T_speech)   - æ³¨æ„åŠ›æƒé‡
    """
    def __init__(self, text_dim, speech_last_dim, speech_mid_dim, hidden_dim):
        super().__init__()
        # ä¸‰ä¸ªçº¿æ€§æŠ•å½±å±‚ï¼Œå°†ä¸åŒæ¨¡æ€çš„ç‰¹å¾æ˜ å°„åˆ°ç›¸åŒçš„éšè—ç»´åº¦
        self.q_proj = nn.Linear(text_dim, hidden_dim)        # æ–‡æœ¬æŸ¥è¯¢æŠ•å½±
        self.k_proj = nn.Linear(speech_last_dim, hidden_dim) # è¯­éŸ³é”®æŠ•å½±ï¼ˆä½¿ç”¨æ·±å±‚ç‰¹å¾ï¼‰
        self.v_proj = nn.Linear(speech_mid_dim, hidden_dim)  # è¯­éŸ³å€¼æŠ•å½±ï¼ˆä½¿ç”¨ä¸­å±‚ç‰¹å¾ï¼‰
        
        print(f"âœ… èšåˆå™¨åˆå§‹åŒ–å®Œæˆ: text_dim={text_dim}, speech_last_dim={speech_last_dim}, "
              f"speech_mid_dim={speech_mid_dim}, hidden_dim={hidden_dim}")

    def forward(self, text_emb, speech_last, speech_mid, speech_mask=None):
        """
        å‰å‘ä¼ æ’­ï¼šæ‰§è¡Œæ–‡æœ¬åˆ°è¯­éŸ³çš„äº¤å‰æ³¨æ„åŠ›
        
        Args:
            text_emb: (B, T_text, D_text) æ–‡æœ¬åµŒå…¥
            speech_last: (B, T_speech, D_last) æ·±å±‚è¯­éŸ³ç‰¹å¾ï¼ˆç”¨äºŽå¯¹é½ï¼‰
            speech_mid: (B, T_speech, D_mid) ä¸­å±‚è¯­éŸ³ç‰¹å¾ï¼ˆç”¨äºŽå†…å®¹é‡å»ºï¼‰
            speech_mask: (B, T_speech) è¯­éŸ³æŽ©ç ï¼ŒTrueè¡¨ç¤ºæœ‰æ•ˆä½ç½®
            
        Returns:
            z: (B, T_text, hidden_dim) å¯¹é½åŽçš„è¯­éŸ³è¡¨ç¤º
            att: (B, T_text, T_speech) æ³¨æ„åŠ›æƒé‡
        """
        # 1) æŠ•å½±è¾“å…¥åˆ°ç›¸åŒçš„éšè—ç»´åº¦
        Q = self.q_proj(text_emb)      # (B, T_text, hidden_dim)
        K = self.k_proj(speech_last)   # (B, T_speech, hidden_dim) 
        V = self.v_proj(speech_mid)    # (B, T_speech, hidden_dim)
        
        # 2) è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°: Q * K^T / sqrt(d_k)
        d_k = Q.size(-1)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)  # (B, T_text, T_speech)
        
        # 3) åº”ç”¨è¯­éŸ³æŽ©ç ï¼ˆå¦‚æžœæä¾›ï¼‰
        if speech_mask is not None:
            # å°†æŽ©ç æ‰©å±•åˆ°ä¸Žåˆ†æ•°ç›¸åŒçš„ç»´åº¦
            mask = speech_mask.unsqueeze(1).expand(-1, Q.size(1), -1)  # (B, T_text, T_speech)
            # å°†å¡«å……ä½ç½®çš„å€¼è®¾ä¸ºè´Ÿæ— ç©·ï¼Œè¿™æ ·softmaxåŽä¼šæŽ¥è¿‘0
            scores = scores.masked_fill(~mask, -1e9)
        
        # 4) åº”ç”¨softmaxèŽ·å–æ³¨æ„åŠ›æƒé‡
        att = F.softmax(scores, dim=-1)  # (B, T_text, T_speech)
        
        # 5) è®¡ç®—åŠ æƒçš„å€¼ï¼ˆå¯¹é½åŽçš„è¯­éŸ³è¡¨ç¤ºï¼‰
        z = torch.matmul(att, V)  # (B, T_text, hidden_dim)
        
        return z, att


class CosyVoiceS3Model(nn.Module):
    """
    CosyVoice LLM + èšåˆå™¨çš„å®Œæ•´æ¨¡åž‹
    
    Inputs:
        text_emb    : (B, T_text, D_text)
        speech_last : (B, T_speech, D_last)
        speech_mid  : (B, T_speech, D_mid)
        speech_mask : (B, T_speech) bool
        s3_targets  : (B, T_s3) long
        
    Outputs:
        loss        : scalar 
        logits      : (B, T_text, S3_VOCAB_SIZE)
        attn        : (B, T_text, T_speech)
    """
    def __init__(
        self,
        llm_wrapper,
        text_dim,
        speech_last_dim,
        speech_mid_dim,
        hidden_dim,
        s3_vocab_size,
        s3_pad_id=0,
        freeze_llm=True,
    ):
        super().__init__()
        self.llm_wrapper = llm_wrapper
        self.llm = llm_wrapper.llm  # å†…éƒ¨çš„ Transformer backbone
        
        # å¤ç”¨é¢„è®­ç»ƒçš„åµŒå…¥å±‚
        self.llm_embedding = llm_wrapper.llm_embedding      # [SOS/EOS, TASK]
        self.speech_embedding = llm_wrapper.speech_embedding # S3 tokens
        
        # èŽ·å– LLM ç»´åº¦
        llm_input_dim = self.speech_embedding.embedding_dim
        try:
            llm_output_dim = self.llm.output_size()
        except AttributeError:
            # å¦‚æžœæ²¡æœ‰ output_size æ–¹æ³•ï¼Œå‡è®¾è¾“å…¥è¾“å‡ºç»´åº¦ç›¸åŒ (å¯¹äºŽå¤§å¤šæ•° Transformer)
            llm_output_dim = llm_input_dim
            
        print(f"âœ… ä½¿ç”¨é¢„è®­ç»ƒåµŒå…¥: speech_emb_dim={llm_input_dim}, llm_out_dim={llm_output_dim}")
        print(f"âœ… é¢„è®­ç»ƒè¯­éŸ³è¯è¡¨å¤§å°: {self.speech_embedding.num_embeddings}")

        self.aggregator = SimpleTextSpeechAggregator(
            text_dim=text_dim,
            speech_last_dim=speech_last_dim,
            speech_mid_dim=speech_mid_dim,
            hidden_dim=hidden_dim,
        )
        self.s3_pad_id = s3_pad_id
        self.s3_vocab_size = s3_vocab_size
        self.s3_vocab_size_with_eos = s3_vocab_size + 1  # é¢å¤–çš„EOSæ ‡è®°
        
        # æŠ•å½±å±‚
        self.input_proj = nn.Linear(text_dim, llm_input_dim)
        # å¤ç”¨é¢„è®­ç»ƒçš„è¾“å‡ºæŠ•å½±å±‚ (Decoder Head)
        self.proj = llm_wrapper.llm_decoder
        
        # èžåˆï¼šæ·»åŠ å½’ä¸€åŒ–
        self.ln_text = nn.LayerNorm(text_dim)
        self.ln_z = nn.LayerNorm(hidden_dim)
        # self.fuse_alpha = nn.Parameter(torch.tensor(0.0)) # ç§»é™¤é—¨æŽ§å‚æ•°ï¼Œä½¿ç”¨ç›´æŽ¥ç›¸åŠ 

        # å†»ç»“LLMå‚æ•°
        if freeze_llm:
            for p in self.llm.parameters():
                p.requires_grad = False
            for p in self.llm_embedding.parameters():
                p.requires_grad = False
            for p in self.speech_embedding.parameters():
                p.requires_grad = False
            # ç¡®ä¿ llm_decoder ä¹Ÿè¢«å†»ç»“ (å¦‚æžœå®ƒåŒ…å«åœ¨ llm_wrapper ä¸­)
            for p in self.proj.parameters():
                p.requires_grad = False
            print("âœ… LLMåŠåµŒå…¥å±‚å‚æ•°å·²å†»ç»“")

        print(f"âœ… CosyVoiceS3Modelåˆå§‹åŒ–å®Œæˆ")

    def forward(
        self,
        text_emb,
        speech_last,
        speech_mid,
        speech_mask=None,
        text_mask=None,
        s3_targets=None,
        s3_lens=None,
    ):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            text_emb: (B, T_text, D_text) æ–‡æœ¬åµŒå…¥
            speech_last: (B, T_speech, D_last) æ·±å±‚è¯­éŸ³ç‰¹å¾
            speech_mid: (B, T_speech, D_mid) ä¸­å±‚è¯­éŸ³ç‰¹å¾
            speech_mask: (B, T_speech) è¯­éŸ³æŽ©ç 
            text_mask: (B, T_text) æ–‡æœ¬æŽ©ç 
            s3_targets: (B, T_s3) S3ç›®æ ‡æ ‡è®°
            s3_lens: (B,) S3åºåˆ—é•¿åº¦
            
        Returns:
            loss: æ ‡é‡æŸå¤±
            logits: (B, L, V+1) é¢„æµ‹logits
            attn: (B, T_text, T_speech) æ³¨æ„åŠ›æƒé‡
        """
        device = text_emb.device
        B = text_emb.size(0)
        
        # ========== æ­¥éª¤1: èšåˆ + èžåˆ ==========
        
        # 1) è°ƒç”¨èšåˆå™¨è¿›è¡Œæ–‡æœ¬-è¯­éŸ³å¯¹é½
        z, attn = self.aggregator(text_emb, speech_last, speech_mid, speech_mask)
        
        # 2) èžåˆæ–‡æœ¬åµŒå…¥å’Œå¯¹é½åŽçš„è¯­éŸ³è¡¨ç¤º
        # é¢˜ç›®è¦æ±‚: e_combined = v + z
        # æˆ‘ä»¬ä¿ç•™LayerNormä»¥ç¡®ä¿æ•°å€¼ç¨³å®šæ€§ï¼Œç›´æŽ¥ç›¸åŠ 
        fused = self.ln_text(text_emb) + self.ln_z(z)
        # fusedå½¢çŠ¶: (B, T_text, text_dim)
        
        # ========== æ–‡æœ¬é•¿åº¦å’ŒLLMè¾“å…¥æž„å»º ==========

        if text_mask is not None:
            text_lens = text_mask.sum(dim=1).to(dtype=torch.int32, device=device)
        else:
            text_lens = torch.full(
                (B,),
                fused.size(1),
                dtype=torch.int32,
                device=device,
            )

        # å°†èžåˆç‰¹å¾æŠ•å½±åˆ°LLMè¾“å…¥ç©ºé—´
        fused_llm = self.input_proj(fused)  # (B, T_text, D_llm_in)

        # å‡†å¤‡å‰ç¼€åµŒå…¥
        sos_eos_emb = self.llm_embedding.weight[0].reshape(1, 1, -1).expand(B, 1, -1)
        task_id_emb = self.llm_embedding.weight[1].reshape(1, 1, -1).expand(B, 1, -1)

        # å¤„ç†è¯­éŸ³ç›®æ ‡æ ‡è®°
        speech_ids = s3_targets.clamp(min=0, max=self.s3_vocab_size - 1)  # (B, T_s3)
        speech_embeds = self.speech_embedding(speech_ids)  # (B, T_s3, D_llm_in)

        # è®¡ç®—S3åºåˆ—é•¿åº¦
        if s3_lens is None:
             # Fallback if not provided (though it should be)
             s3_lens = (s3_targets != self.s3_pad_id).sum(dim=1).to(dtype=torch.int32, device=device)
        else:
             s3_lens = s3_lens.to(dtype=torch.int32, device=device)

        # æž„å»ºLLMè¾“å…¥åºåˆ—: [SOS] + èžåˆç‰¹å¾ + [TASK] + è¯­éŸ³åµŒå…¥
        lm_input = torch.cat([sos_eos_emb, fused_llm, task_id_emb, speech_embeds], dim=1)  # (B, L, D)
        lm_input_len = (1 + text_lens + 1 + s3_lens).to(dtype=torch.int32, device=device)  # (B,)

        # é€šè¿‡LLM
        hidden, _ = self.llm(lm_input, lm_input_len)  # (B, L, H)
        logits = self.proj(hidden)                    # (B, L, V+1)

        # ========== ç›®æ ‡æž„å»ºå’ŒæŸå¤±è®¡ç®— ==========
        
        # æž„å»ºæ•™å¸ˆå¼ºåˆ¶ç›®æ ‡
        L = lm_input.size(1)
        lm_target = torch.full((B, L), IGNORE_ID, dtype=torch.long, device=device)
        
        for i in range(B):
            prefix_len = 2 + text_lens[i]  # [SOS] + fused_len + [TASK]
            slen = s3_lens[i]
            
            if slen > 0:
                # ä¿®å¤: ç›®æ ‡åº”è¯¥å·¦ç§»ä¸€ä½ (Next Token Prediction)
                # è¾“å…¥ [TASK] -> ç›®æ ‡ S0
                # è¾“å…¥ S0     -> ç›®æ ‡ S1
                # ...
                # è¾“å…¥ S_last -> ç›®æ ‡ EOS
                
                # 1. å¡«å…¥ S0 åˆ° S_last (ä½œä¸º [TASK] åˆ° S_last-1 çš„ç›®æ ‡)
                lm_target[i, prefix_len - 1 : prefix_len + slen - 1] = s3_targets[i, :slen]
                
                # 2. å¡«å…¥ EOS (ä½œä¸º S_last çš„ç›®æ ‡)
                lm_target[i, prefix_len + slen - 1] = self.s3_vocab_size
        
        # è®¡ç®—äº¤å‰ç†µæŸå¤±
        loss = F.cross_entropy(
            logits.view(-1, self.s3_vocab_size_with_eos),
            lm_target.view(-1),
            ignore_index=IGNORE_ID
        )
        
        # è®¡ç®—å‡†ç¡®çŽ‡
        with torch.no_grad():
            preds = logits.argmax(dim=-1)
            mask = lm_target != IGNORE_ID
            correct = (preds[mask] == lm_target[mask]).sum()
            total = mask.sum()
            acc = correct.float() / total.float() if total > 0 else torch.tensor(0.0, device=device)
        
        return loss, logits, attn, acc


# ======================
#  Dataset / DataLoader
# ======================

class S3Dataset(Dataset):
    """S3æ•°æ®é›†ç±»"""
    def __init__(self, samples):
        self.samples = samples
        print(f"âœ… æ•°æ®é›†åˆå§‹åŒ–: {len(samples)} ä¸ªæ ·æœ¬")

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        return self.samples[idx]


def collate_fn(batch):
    """æ‰¹å¤„ç†å‡½æ•°"""
    B = len(batch)
    
    # è®¡ç®—å„åºåˆ—é•¿åº¦
    text_lens = [b["text_emb"].size(0) for b in batch]
    speech_lens = [b["speech_mid"].size(0) for b in batch]
    s3_lens = []
    for b in batch:
        tokens = b["s3_tokens"]
        if torch.is_tensor(tokens):
            s3_lens.append(int(tokens.numel()))
        else:
            s3_lens.append(len(tokens))

    # æ‰¾åˆ°æœ€å¤§é•¿åº¦ç”¨äºŽå¡«å……
    max_T_text = max(text_lens)
    max_T_speech = max(speech_lens)
    max_T_s3 = max(s3_lens)

    # èŽ·å–ç‰¹å¾ç»´åº¦
    text_dim = batch[0]["text_emb"].size(-1)
    d_last = batch[0]["speech_last"].size(-1)
    d_mid = batch[0]["speech_mid"].size(-1)

    # åˆå§‹åŒ–å¡«å……å¼ é‡
    text_emb = torch.zeros(B, max_T_text, text_dim)
    speech_last = torch.zeros(B, max_T_speech, d_last)
    speech_mid = torch.zeros(B, max_T_speech, d_mid)
    speech_mask = torch.zeros(B, max_T_speech, dtype=torch.bool)
    s3_targets = torch.full((B, max_T_s3), S3_PAD_ID, dtype=torch.long)
    text_mask = torch.zeros(B, max_T_text, dtype=torch.bool)

    # å¡«å……æ•°æ®
    for i, b in enumerate(batch):
        tt = text_lens[i]
        ts = speech_lens[i]
        ts3 = s3_lens[i]

        text_emb[i, :tt] = b["text_emb"]
        speech_last[i, :ts] = b["speech_last"]
        speech_mid[i, :ts] = b["speech_mid"]
        speech_mask[i, :ts] = True
        
        tokens = b["s3_tokens"]
        if not torch.is_tensor(tokens):
            tokens = torch.as_tensor(tokens, dtype=torch.long)
        else:
            tokens = tokens.to(dtype=torch.long)
        s3_targets[i, :ts3] = tokens[:ts3]
        text_mask[i, :tt] = True

    return {
        "text_emb": text_emb,
        "speech_last": speech_last,
        "speech_mid": speech_mid,
        "speech_mask": speech_mask,
        "s3_targets": s3_targets,
        "s3_lens": torch.tensor(s3_lens, dtype=torch.long),
        "text_mask": text_mask,
    }


def load_samples(utt2s3_path, utt2text_path, utt2whisper_path, dataset_name="è®­ç»ƒé›†"):
    """åŠ è½½æ ·æœ¬æ•°æ®"""
    print(f"æ­£åœ¨åŠ è½½{dataset_name}æ ·æœ¬æ•°æ®...")
    
    # åŠ è½½ä¸‰ä¸ªç‰¹å¾æ–‡ä»¶
    utt2s3 = torch.load(utt2s3_path, map_location="cpu")
    utt2text = torch.load(utt2text_path, map_location="cpu")
    utt2whisper = torch.load(utt2whisper_path, map_location="cpu")

    # æ³¨æ„: ä¸åŒæ–‡ä»¶ä¸­utt idçš„æ ¼å¼å¯èƒ½ä¸ä¸€è‡´ï¼Œå¸¸è§æƒ…å†µæ˜¯
    # - s3 å­—å…¸ä½¿ç”¨çŸ­å½¢å¼ idï¼Œå¦‚ '3830-12531-0003'
    # - text_emb / whisper ä½¿ç”¨æ–‡ä»¶è·¯å¾„å½¢å¼ï¼Œå¦‚ 'LibriSpeech/.../3830-12531-0003.flac'
    # ä¸ºä¿è¯åŒ¹é…ï¼Œæˆ‘ä»¬å»ºç«‹ä»Žè§„èŒƒåŒ–id->åŽŸå§‹keyçš„æ˜ å°„ï¼ˆè§„èŒƒåŒ–ä¸ºbasenameåŽ»æŽ‰æ‰©å±•åï¼‰
    def _norm_key(k):
        try:
            if isinstance(k, str) and ('/' in k or '\\' in k):
                return os.path.splitext(os.path.basename(k))[0]
            return k
        except Exception:
            return k

    map_s3 = { _norm_key(k): k for k in utt2s3.keys() }
    map_text = { _norm_key(k): k for k in utt2text.keys() }
    # whisper å­˜åœ¨åµŒå¥— dictï¼Œåˆ†åˆ«æ˜ å°„ mid/final çš„ keys
    whisper_mid = utt2whisper.get('mid', {})
    whisper_final = utt2whisper.get('final', {})
    map_whisper_mid = { _norm_key(k): k for k in whisper_mid.keys() }
    map_whisper_final = { _norm_key(k): k for k in whisper_final.keys() }

    # å–äº¤é›†
    common_keys = sorted(set(map_s3.keys()) & set(map_text.keys()) & set(map_whisper_mid.keys()) & set(map_whisper_final.keys()))

    samples = []
    skipped_count = 0

    for nk in common_keys:
        # ä½¿ç”¨æ˜ å°„å–å¾—åŽŸå§‹å­—å…¸ä¸­çš„æ•°æ®
        s3_tokens = utt2s3.get(map_s3[nk])
        text_emb = utt2text.get(map_text[nk])
        speech_mid = whisper_mid.get(map_whisper_mid[nk])
        speech_last = whisper_final.get(map_whisper_final[nk])

        # è·³è¿‡æ— æ•ˆæ•°æ®
        if (s3_tokens is None) or (text_emb is None) or (speech_mid is None) or (speech_last is None):
            skipped_count += 1
            continue
        if (getattr(text_emb, "numel", lambda: 0)() == 0) or (getattr(speech_mid, "numel", lambda: 0)() == 0) or (getattr(speech_last, "numel", lambda: 0)() == 0):
            skipped_count += 1
            continue

        samples.append({
            "utt_id": nk,
            "text_emb": text_emb,
            "speech_mid": speech_mid,
            "speech_last": speech_last,
            "s3_tokens": s3_tokens,
        })

    print(f"âœ… {dataset_name}åŠ è½½å®Œæˆ: {len(samples)} ä¸ªæœ‰æ•ˆæ ·æœ¬, {skipped_count} ä¸ªè¢«è·³è¿‡")
    return samples


# ======================
#  Train / Eval / Predict
# ======================

def train_one_epoch(model, dataloader, optimizer, device):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    total_loss = 0.0
    total_acc = 0.0
    total_tokens = 0
    num_batches = len(dataloader)
    
    print(f"å¼€å§‹è®­ç»ƒepochï¼Œå…±{num_batches}ä¸ªbatch...")
    
    for batch_idx, batch in enumerate(dataloader):
        # ç›‘æŽ§å†…å­˜
        if batch_idx % 10 == 0:
            print_memory_usage(device, f"è®­ç»ƒBatch {batch_idx}/{num_batches}")
        
        # å°†æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
        batch_on_device = {}
        for k, v in batch.items():
            if torch.is_tensor(v):
                batch_on_device[k] = v.to(device)
            else:
                batch_on_device[k] = v
        
        # å‰å‘ä¼ æ’­
        loss, logits, attn, acc = model(**batch_on_device)
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)
        
        # ä¼˜åŒ–å™¨æ­¥è¿›
        optimizer.step()
        
        # ç»Ÿè®¡æœ‰æ•ˆtokenæ•°é‡ï¼ˆå¿½ç•¥paddingï¼‰
        mask = (batch_on_device["s3_targets"] != S3_PAD_ID)
        batch_tokens = mask.sum().item()
        total_loss += loss.item() * batch_tokens
        total_tokens += batch_tokens
        total_acc += acc.item() * batch_tokens
        
        # æ¯10ä¸ªbatchæ‰“å°ä¸€æ¬¡è¿›åº¦
        if batch_idx % 10 == 0:
            avg_loss = loss.item()
            print(f"  Batch {batch_idx}/{num_batches} | Loss: {avg_loss:.4f} | Acc: {acc.item():.2%} | Tokens: {batch_tokens}")
        
        # æ¸…ç†ä»¥é‡Šæ”¾å†…å­˜
        del batch_on_device, loss, logits, attn, acc
        
        # å®šæœŸåžƒåœ¾å›žæ”¶
        if batch_idx % 20 == 0:
            gc.collect()
            if device.type == 'cuda':
                torch.cuda.empty_cache()
    
    # è®¡ç®—å¹³å‡æ¯ä¸ªtokençš„æŸå¤±
    avg_loss_per_token = total_loss / total_tokens if total_tokens > 0 else 0.0
    avg_acc = total_acc / total_tokens if total_tokens > 0 else 0.0
    print(f"âœ… è®­ç»ƒå®Œæˆ: å¹³å‡æŸå¤±/Token = {avg_loss_per_token:.4f}, å¹³å‡å‡†ç¡®çŽ‡ = {avg_acc:.2%}")
    
    return avg_loss_per_token, avg_acc


@torch.no_grad()
def eval_one_epoch(model, dataloader, device):
    """è¯„ä¼°ä¸€ä¸ªepoch"""
    model.eval()
    total_loss = 0.0
    total_acc = 0.0
    total_tokens = 0
    num_batches = len(dataloader)
    
    print(f"å¼€å§‹è¯„ä¼°ï¼Œå…±{num_batches}ä¸ªbatch...")
    
    for batch_idx, batch in enumerate(dataloader):
        # å°†æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
        batch_on_device = {}
        for k, v in batch.items():
            if torch.is_tensor(v):
                batch_on_device[k] = v.to(device)
            else:
                batch_on_device[k] = v
        
        # å‰å‘ä¼ æ’­
        loss, logits, attn, acc = model(**batch_on_device)
        
        # ç»Ÿè®¡æœ‰æ•ˆtokenæ•°é‡
        mask = (batch_on_device["s3_targets"] != S3_PAD_ID)
        batch_tokens = mask.sum().item()
        total_loss += loss.item() * batch_tokens
        total_tokens += batch_tokens
        total_acc += acc.item() * batch_tokens
        
        # æ¯10ä¸ªbatchæ‰“å°ä¸€æ¬¡è¿›åº¦
        if batch_idx % 10 == 0:
            print(f"  è¯„ä¼°Batch {batch_idx}/{num_batches} | Loss: {loss.item():.4f} | Acc: {acc.item():.2%}")
        
        # æ¸…ç†
        del batch_on_device, loss, logits, attn, acc
    
    # è®¡ç®—å¹³å‡æŸå¤±
    avg_loss_per_token = total_loss / total_tokens if total_tokens > 0 else 0.0
    avg_acc = total_acc / total_tokens if total_tokens > 0 else 0.0
    print(f"âœ… è¯„ä¼°å®Œæˆ: å¹³å‡æŸå¤±/Token = {avg_loss_per_token:.4f}, å¹³å‡å‡†ç¡®çŽ‡ = {avg_acc:.2%}")
    
    return avg_loss_per_token, avg_acc


@torch.no_grad()
def predict_s3(model, text_emb, speech_last, speech_mid, device, max_steps=200):
    """
    è‡ªå›žå½’è§£ç ç”ŸæˆS3 tokens
    
    Args:
        text_emb: (T_text, D_text) æ–‡æœ¬åµŒå…¥
        speech_last: (T_speech, D_last) æ·±å±‚è¯­éŸ³ç‰¹å¾
        speech_mid: (T_speech, D_mid) ä¸­å±‚è¯­éŸ³ç‰¹å¾
        device: è®¡ç®—è®¾å¤‡
        max_steps: æœ€å¤§è§£ç æ­¥æ•°
        
    Returns:
        pred_s3: (L,) ç”Ÿæˆçš„S3 tokens
    """
    model.eval()
    
    # 1) æ·»åŠ æ‰¹æ¬¡ç»´åº¦å¹¶ç§»åŠ¨åˆ°è®¾å¤‡
    text_emb = text_emb.unsqueeze(0).to(device)        # (1, T_text, D_text)
    speech_last = speech_last.unsqueeze(0).to(device)  # (1, T_speech, D_last)
    speech_mid = speech_mid.unsqueeze(0).to(device)    # (1, T_speech, D_mid)
    
    # 2) åˆ›å»ºå…¨Trueçš„è¯­éŸ³æŽ©ç ï¼ˆæŽ¨ç†æ—¶æ— å¡«å……ï¼‰
    speech_mask = torch.ones(1, speech_last.size(1), dtype=torch.bool, device=device)
    
    # 3) ä½¿ç”¨èšåˆå™¨+èžåˆèŽ·å–å¯¹é½ç‰¹å¾
    z, _ = model.aggregator(text_emb, speech_last, speech_mid, speech_mask)
    # w = torch.sigmoid(model.fuse_alpha)
    # fused = w * model.ln_text(text_emb) + (1 - w) * model.ln_z(z)
    fused = model.ln_text(text_emb) + model.ln_z(z)
    fused_llm = model.input_proj(fused)  # (1, T_text, D_llm_in)
    
    # 4) æž„å»ºåˆå§‹åºåˆ—: [SOS] + èžåˆç‰¹å¾ + [TASK]
    sos_eos_emb = model.llm_embedding.weight[0].reshape(1, 1, -1)  # (1, 1, D_llm_in)
    task_id_emb = model.llm_embedding.weight[1].reshape(1, 1, -1)  # (1, 1, D_llm_in)
    
    seq = torch.cat([sos_eos_emb, fused_llm, task_id_emb], dim=1)  # (1, 2+T_text, D_llm_in)
    seq_len = torch.tensor([seq.size(1)], dtype=torch.int32, device=device)
    
    # 5) è‡ªå›žå½’è§£ç 
    generated_ids = []
    T_text = text_emb.size(1)
    
    for step in range(max_steps):
        # è¿è¡ŒLLM
        hidden, _ = model.llm(seq, seq_len)  # (1, current_len, H)
        
        # èŽ·å–æœ€åŽä¸€æ­¥çš„logits
        last_logits = model.proj(hidden[:, -1:])  # (1, 1, V+1)
        
        # é€‰æ‹©æ¦‚çŽ‡æœ€é«˜çš„token
        next_id = last_logits.argmax(dim=-1).squeeze(-1)  # (1,)
        
        # æ£€æŸ¥æ˜¯å¦ç”ŸæˆEOS
        if next_id.item() == model.s3_vocab_size:  # EOSæ ‡è®°
            break
        
        # é™åˆ¶IDåœ¨æœ‰æ•ˆèŒƒå›´å†…å¹¶åµŒå…¥
        next_id_clamped = next_id.clamp(min=0, max=model.s3_vocab_size - 1)
        next_embed = model.speech_embedding(next_id_clamped).unsqueeze(1)  # (1, 1, D_llm_in)
        
        # æ·»åŠ åˆ°åºåˆ—
        seq = torch.cat([seq, next_embed], dim=1)
        seq_len = torch.tensor([seq.size(1)], dtype=torch.int32, device=device)
        generated_ids.append(next_id_clamped.item())
        
        # å¦‚æžœç”Ÿæˆäº†å¤ªå¤štokenï¼ˆè¶…è¿‡æ–‡æœ¬é•¿åº¦çš„4å€ï¼‰ï¼Œæå‰åœæ­¢
        if len(generated_ids) >= 4 * T_text:
            break
    
    pred_s3 = torch.tensor(generated_ids, dtype=torch.long)
    print(f"âœ… ç”Ÿæˆå®Œæˆ: {len(pred_s3)} ä¸ªS3 tokens")
    
    return pred_s3


# ======================
#  Main
# ======================

def main():
    # è®¾ç½®è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆå§‹å†…å­˜ç›‘æŽ§
    print_memory_usage(device, "ç¨‹åºå¼€å§‹")
    
    try:
        # åˆ†åˆ«åŠ è½½è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ ·æœ¬
        print("æ­£åœ¨åŠ è½½æ•°æ®é›†...")
        train_samples = load_samples(
            TRAIN_UTT2_S3_PATH, 
            TRAIN_UTT2_TEXT_EMB_PATH, 
            TRAIN_UTT2_WHISPER_PATH,
            "è®­ç»ƒé›†"
        )
        
        test_samples = load_samples(
            TEST_UTT2_S3_PATH, 
            TEST_UTT2_TEXT_EMB_PATH, 
            TEST_UTT2_WHISPER_PATH,
            "æµ‹è¯•é›†"
        )
        
        if len(train_samples) == 0:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°è®­ç»ƒæ ·æœ¬ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„")
            return
        
        if len(test_samples) == 0:
            print("âš  æ²¡æœ‰æ‰¾åˆ°æµ‹è¯•æ ·æœ¬ï¼Œå°†ä»Žè®­ç»ƒé›†åˆ†å‰²éªŒè¯é›†")
            # å¦‚æžœæ²¡æœ‰æµ‹è¯•é›†ï¼Œä»Žè®­ç»ƒé›†åˆ†å‰²
            random.shuffle(train_samples)
            n_train = int(len(train_samples) * TRAIN_RATIO)
            train_samples, test_samples = train_samples[:n_train], train_samples[n_train:]
        
        print(f"æ•°æ®é›†: è®­ç»ƒé›† {len(train_samples)} æ ·æœ¬, æµ‹è¯•é›† {len(test_samples)} æ ·æœ¬")
        
        # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
        train_ds = S3Dataset(train_samples)
        test_ds = S3Dataset(test_samples)
        
        train_loader = DataLoader(
            train_ds,
            batch_size=BATCH_SIZE,
            shuffle=True,
            collate_fn=collate_fn,
        )
        test_loader = DataLoader(
            test_ds,
            batch_size=BATCH_SIZE,
            shuffle=False,
            collate_fn=collate_fn,
        )
        
        # èŽ·å–ç‰¹å¾ç»´åº¦
        example = train_samples[0]
        text_dim = example["text_emb"].size(-1)
        d_last = example["speech_last"].size(-1)
        d_mid = example["speech_mid"].size(-1)
        
        print(f"ç‰¹å¾ç»´åº¦: text_dim={text_dim}, speech_last_dim={d_last}, speech_mid_dim={d_mid}")
        
        # åŠ è½½LLMå’Œåˆ›å»ºæ¨¡åž‹
        llm_wrapper = load_cosyvoice_llm(device)
        
        model = CosyVoiceS3Model(
            llm_wrapper=llm_wrapper,
            text_dim=text_dim,
            speech_last_dim=d_last,
            speech_mid_dim=d_mid,
            hidden_dim=text_dim,  # ä½¿ç”¨æ–‡æœ¬ç»´åº¦ä½œä¸ºéšè—ç»´åº¦
            s3_vocab_size=S3_VOCAB_SIZE,
            s3_pad_id=S3_PAD_ID,
            freeze_llm=True,         
        ).to(device)
        
        # æ‰“å°æ¨¡åž‹å‚æ•°ç»Ÿè®¡
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"æ¨¡åž‹å‚æ•°: æ€»è®¡ {total_params:,}ï¼Œå¯è®­ç»ƒ {trainable_params:,}")
        
        # éªŒè¯ input_proj æ˜¯å¦å¯è®­ç»ƒ
        print(f"input_proj requires_grad: {model.input_proj.weight.requires_grad}")
        print(f"aggregator requires_grad: {model.aggregator.q_proj.weight.requires_grad}")
        
        # åˆ›å»ºä¼˜åŒ–å™¨ï¼ˆåªä¼˜åŒ–å¯è®­ç»ƒå‚æ•°ï¼‰
        trainable_params = [p for p in model.parameters() if p.requires_grad]
        optimizer = torch.optim.AdamW(trainable_params, lr=LR, weight_decay=WEIGHT_DECAY)
        
        # è®­ç»ƒå¾ªçŽ¯
        train_losses = []
        test_losses = []
        
        for epoch in range(1, NUM_EPOCHS + 1):
            print(f"\n{'='*50}")
            print(f"Epoch {epoch:02d}/{NUM_EPOCHS}")
            print(f"{'='*50}")
            
            # è®­ç»ƒ
            train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, device)
            train_losses.append(train_loss)
            
            # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
            test_loss, test_acc = eval_one_epoch(model, test_loader, device)
            test_losses.append(test_loss)
            
            print(f"Epoch {epoch:02d} | è®­ç»ƒæŸå¤±: {train_loss:.4f} | è®­ç»ƒå‡†ç¡®çŽ‡: {train_acc:.2%} | æµ‹è¯•æŸå¤±: {test_loss:.4f} | æµ‹è¯•å‡†ç¡®çŽ‡: {test_acc:.2%}")
            
            # æ¯3ä¸ªepochè¿›è¡Œä¸€æ¬¡å®Œæ•´çš„å†…å­˜æ¸…ç†
            if epoch % 3 == 0:
                gc.collect()
                if device.type == 'cuda':
                    torch.cuda.empty_cache()
                print_memory_usage(device, f"Epoch {epoch} åŽ")
        
        # ç»˜åˆ¶è®­ç»ƒæŸå¤±æ›²çº¿
        try:
            import matplotlib.pyplot as plt
            plt.figure(figsize=(10, 6))
            plt.plot(range(1, NUM_EPOCHS + 1), train_losses, label='Train Loss')
            plt.plot(range(1, NUM_EPOCHS + 1), test_losses, label='Test Loss')
            plt.xlabel('Epoch')
            plt.ylabel('Loss')
            plt.title('Training and Validation Loss')
            plt.legend()
            plt.grid(True)
            plt.savefig('loss_curve.png')
            print("âœ… æŸå¤±æ›²çº¿å·²ä¿å­˜ä¸º loss_curve.png")
        except ImportError:
            print("âš  æœªå®‰è£… matplotlibï¼Œè·³è¿‡ç»˜åˆ¶æŸå¤±æ›²çº¿")
        except Exception as e:
            print(f"âš  ç»˜åˆ¶æŸå¤±æ›²çº¿å¤±è´¥: {e}")

        # è®­ç»ƒå®Œæˆï¼Œè¿›è¡ŒæŽ¨ç†ç¤ºä¾‹
        print(f"\n{'='*50}")
        print("è®­ç»ƒå®Œæˆï¼Œå¼€å§‹æŽ¨ç†ç¤ºä¾‹...")
        print(f"{'='*50}")
        
        if len(test_samples) > 0:
            ex = test_samples[0]
            print(f"ä½¿ç”¨æµ‹è¯•æ ·æœ¬è¿›è¡ŒæŽ¨ç†: {ex['utt_id']}")
            
            pred_s3 = predict_s3(
                model,
                ex["text_emb"],
                ex["speech_last"],
                ex["speech_mid"],
                device,
            )
            
            # å¯¹æ¯”çœŸå®žå’Œé¢„æµ‹çš„S3 tokens
            true_s3 = ex["s3_tokens"]
            if torch.is_tensor(true_s3):
                true_s3 = true_s3.tolist()
            
            print(f"çœŸå®žS3 tokens (å‰10ä¸ª): {true_s3[:10]}... (å…±{len(true_s3)}ä¸ª)")
            print(f"é¢„æµ‹S3 tokens (å‰10ä¸ª): {pred_s3[:10].tolist()}... (å…±{len(pred_s3)}ä¸ª)")
            
            # è®¡ç®—å‡†ç¡®çŽ‡ï¼ˆå¦‚æžœé•¿åº¦åŒ¹é…ï¼‰
            if len(pred_s3) >= min(10, len(true_s3)):
                match_count = sum(1 for i in range(min(10, len(true_s3))) 
                               if i < len(pred_s3) and pred_s3[i].item() == true_s3[i])
                accuracy = match_count / min(10, len(true_s3))
                print(f"å‰10ä¸ªtokenå‡†ç¡®çŽ‡: {accuracy:.2%}")
        
        print("\nðŸŽ‰ ç¨‹åºæ‰§è¡Œå®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()